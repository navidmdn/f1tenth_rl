{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1bd4eb",
   "metadata": {},
   "source": [
    "# F1tenth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b781558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f07c65",
   "metadata": {},
   "source": [
    "## Environment playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e382dbcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# racecar_env = gym.make(\n",
    "#     'f110_gym:f110-v0',\n",
    "#     map='./f1tenth_gym/gym/f110_gym/envs/maps/vegas',\n",
    "#     map_ext='.png'\n",
    "# )\n",
    "\n",
    "racecar_env = gym.make(\n",
    "    'f110_gym:f110-v0',\n",
    "    map='./f1tenth_gym/examples/example_map',\n",
    "    map_ext='.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acba6632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ego_idx': 0,\n",
       " 'scans': [array([29.98576175, 30.01263728, 29.99129338, ..., 29.99864406,\n",
       "         29.9889291 , 30.01278365]),\n",
       "  array([29.98576175, 30.01263728, 29.99129338, ..., 29.99864406,\n",
       "         29.9889291 , 30.01278365])],\n",
       " 'poses_x': [0.0, 2.0],\n",
       " 'poses_y': [0.0, 0.0],\n",
       " 'poses_theta': [0.0, 0.0],\n",
       " 'linear_vels_x': [0.0, 0.0],\n",
       " 'linear_vels_y': [0.0, 0.0],\n",
       " 'ang_vels_z': [0.0, 0.0],\n",
       " 'collisions': array([0., 0.]),\n",
       " 'lap_times': array([0.01, 0.01]),\n",
       " 'lap_counts': array([0., 0.])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, step_reward, done, info = racecar_env.reset(\n",
    "    poses=np.array([[0., 0., 0.], # pose of ego\n",
    "             [2., 0., 0.]])  # pose of 2nd agent\n",
    ") \n",
    "\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2df1290a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1082,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = np.concatenate([\n",
    "    obs['scans'][0],\n",
    "    np.array(obs['linear_vels_x'][:1]),\n",
    "    np.array(obs['linear_vels_y'][:1]),\n",
    "])\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54349ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0596855 ,  0.44670051],\n",
       "       [-0.05675966,  1.8354974 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeds = np.random.rand(2, 1)*2\n",
    "pi_4 = 3.1415/8\n",
    "pi_2 = 3.1415/4\n",
    "angles = np.random.rand(2, 1)*pi_2-pi_4\n",
    "actions = np.concatenate([angles, speeds], axis=1)\n",
    "\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "133a6cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## action consists of ndarray(num_agent, 2) 0: steering angle 1: velocity\n",
    "## the reward function is only for the first agent\n",
    "\n",
    "import time\n",
    "import gym \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "racecar_env = gym.make(\n",
    "    'f110_gym:f110-v0',\n",
    "    map='./f1tenth_gym/examples/example_map',\n",
    "    map_ext='.png',\n",
    "    num_agents=1\n",
    ")\n",
    "steps = 0\n",
    "\n",
    "obs, step_reward, done, info = racecar_env.reset(\n",
    "    poses=np.array([[0., 0., 1.5]]) \n",
    ") \n",
    "\n",
    "rewards = []\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    speeds = np.random.rand(2, 1)*20\n",
    "    pi_4 = 3.1415/8\n",
    "    pi_2 = 3.1415/4\n",
    "    speeds[1][0] = 0.1\n",
    "    angles = np.random.rand(2, 1)*pi_2-pi_4\n",
    "    actions = np.concatenate([angles, speeds], axis=1)\n",
    "\n",
    "    obs, step_reward, done, info = racecar_env.step(actions)\n",
    "    rewards.append(step_reward)\n",
    "    \n",
    "    racecar_env.render()\n",
    "    steps += 1\n",
    "    \n",
    "    if steps > 500:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa1c0a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa25f6c8460>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab60a5e8",
   "metadata": {},
   "source": [
    "## Define environment wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8052fa",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### waypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a6250",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import yaml\n",
    "import gym\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "\n",
    "from numba import njit\n",
    "\n",
    "from pyglet.gl import GL_POINTS\n",
    "\n",
    "\n",
    "# @njit(fastmath=False, cache=True)\n",
    "def nearest_point_on_trajectory(point, trajectory):\n",
    "    \"\"\"\n",
    "    Return the nearest point along the given piecewise linear trajectory.\n",
    "\n",
    "    Same as nearest_point_on_line_segment, but vectorized. This method is quite fast, time constraints should\n",
    "    not be an issue so long as trajectories are not insanely long.\n",
    "\n",
    "        Order of magnitude: trajectory length: 1000 --> 0.0002 second computation (5000fps)\n",
    "\n",
    "    point: size 2 numpy array\n",
    "    trajectory: Nx2 matrix of (x,y) trajectory waypoints\n",
    "        - these must be unique. If they are not unique, a divide by 0 error will destroy the world\n",
    "    \"\"\"\n",
    "    diffs = trajectory[1:,:] - trajectory[:-1,:]\n",
    "    l2s   = diffs[:,0]**2 + diffs[:,1]**2\n",
    "    # this is equivalent to the elementwise dot product\n",
    "    # dots = np.sum((point - trajectory[:-1,:]) * diffs[:,:], axis=1)\n",
    "    dots = np.empty((trajectory.shape[0]-1, ))\n",
    "    for i in range(dots.shape[0]):\n",
    "        dots[i] = np.dot((point - trajectory[i, :]), diffs[i, :])\n",
    "    t = dots / l2s\n",
    "    t[t<0.0] = 0.0\n",
    "    t[t>1.0] = 1.0\n",
    "    # t = np.clip(dots / l2s, 0.0, 1.0)\n",
    "    projections = trajectory[:-1,:] + (t*diffs.T).T\n",
    "    # dists = np.linalg.norm(point - projections, axis=1)\n",
    "    dists = np.empty((projections.shape[0],))\n",
    "    for i in range(dists.shape[0]):\n",
    "        temp = point - projections[i]\n",
    "        dists[i] = np.sqrt(np.sum(temp*temp))\n",
    "    min_dist_segment = np.argmin(dists)\n",
    "    return projections[min_dist_segment], dists[min_dist_segment], t[min_dist_segment], min_dist_segment\n",
    "\n",
    "# @njit(fastmath=False, cache=True)\n",
    "def first_point_on_trajectory_intersecting_circle(point, radius, trajectory, t=0.0, wrap=False):\n",
    "    \"\"\"\n",
    "    starts at beginning of trajectory, and find the first point one radius away from the given point along the trajectory.\n",
    "\n",
    "    Assumes that the first segment passes within a single radius of the point\n",
    "\n",
    "    http://codereview.stackexchange.com/questions/86421/line-segment-to-circle-collision-algorithm\n",
    "    \"\"\"\n",
    "    start_i = int(t)\n",
    "    start_t = t % 1.0\n",
    "    first_t = None\n",
    "    first_i = None\n",
    "    first_p = None\n",
    "    trajectory = np.ascontiguousarray(trajectory)\n",
    "    for i in range(start_i, trajectory.shape[0]-1):\n",
    "        start = trajectory[i,:]\n",
    "        end = trajectory[i+1,:]+1e-6\n",
    "        V = np.ascontiguousarray(end - start)\n",
    "\n",
    "        a = np.dot(V,V)\n",
    "        b = 2.0*np.dot(V, start - point)\n",
    "        c = np.dot(start, start) + np.dot(point,point) - 2.0*np.dot(start, point) - radius*radius\n",
    "        discriminant = b*b-4*a*c\n",
    "\n",
    "        if discriminant < 0:\n",
    "            continue\n",
    "        #   print \"NO INTERSECTION\"\n",
    "        # else:\n",
    "        # if discriminant >= 0.0:\n",
    "        discriminant = np.sqrt(discriminant)\n",
    "        t1 = (-b - discriminant) / (2.0*a)\n",
    "        t2 = (-b + discriminant) / (2.0*a)\n",
    "        if i == start_i:\n",
    "            if t1 >= 0.0 and t1 <= 1.0 and t1 >= start_t:\n",
    "                first_t = t1\n",
    "                first_i = i\n",
    "                first_p = start + t1 * V\n",
    "                break\n",
    "            if t2 >= 0.0 and t2 <= 1.0 and t2 >= start_t:\n",
    "                first_t = t2\n",
    "                first_i = i\n",
    "                first_p = start + t2 * V\n",
    "                break\n",
    "        elif t1 >= 0.0 and t1 <= 1.0:\n",
    "            first_t = t1\n",
    "            first_i = i\n",
    "            first_p = start + t1 * V\n",
    "            break\n",
    "        elif t2 >= 0.0 and t2 <= 1.0:\n",
    "            first_t = t2\n",
    "            first_i = i\n",
    "            first_p = start + t2 * V\n",
    "            break\n",
    "    # wrap around to the beginning of the trajectory if no intersection is found1\n",
    "    if wrap and first_p is None:\n",
    "        for i in range(-1, start_i):\n",
    "            start = trajectory[i % trajectory.shape[0],:]\n",
    "            end = trajectory[(i+1) % trajectory.shape[0],:]+1e-6\n",
    "            V = end - start\n",
    "\n",
    "            a = np.dot(V,V)\n",
    "            b = 2.0*np.dot(V, start - point)\n",
    "            c = np.dot(start, start) + np.dot(point,point) - 2.0*np.dot(start, point) - radius*radius\n",
    "            discriminant = b*b-4*a*c\n",
    "\n",
    "            if discriminant < 0:\n",
    "                continue\n",
    "            discriminant = np.sqrt(discriminant)\n",
    "            t1 = (-b - discriminant) / (2.0*a)\n",
    "            t2 = (-b + discriminant) / (2.0*a)\n",
    "            if t1 >= 0.0 and t1 <= 1.0:\n",
    "                first_t = t1\n",
    "                first_i = i\n",
    "                first_p = start + t1 * V\n",
    "                break\n",
    "            elif t2 >= 0.0 and t2 <= 1.0:\n",
    "                first_t = t2\n",
    "                first_i = i\n",
    "                first_p = start + t2 * V\n",
    "                break\n",
    "\n",
    "    return first_p, first_i, first_t\n",
    "\n",
    "# @njit(fastmath=False, cache=True)\n",
    "def get_actuation(pose_theta, lookahead_point, position, lookahead_distance, wheelbase):\n",
    "    \"\"\"\n",
    "    Returns actuation\n",
    "    \"\"\"\n",
    "    waypoint_y = np.dot(np.array([np.sin(-pose_theta), np.cos(-pose_theta)]), lookahead_point[0:2]-position)\n",
    "    speed = lookahead_point[2]\n",
    "    if np.abs(waypoint_y) < 1e-6:\n",
    "        return speed, 0.\n",
    "    radius = 1/(2.0*waypoint_y/lookahead_distance**2)\n",
    "    steering_angle = np.arctan(wheelbase/radius)\n",
    "    return speed, steering_angle\n",
    "\n",
    "class PurePursuitPlanner:\n",
    "    \"\"\"\n",
    "    Example Planner\n",
    "    \"\"\"\n",
    "    def __init__(self, conf, wb):\n",
    "        self.wheelbase = wb\n",
    "        self.conf = conf\n",
    "        self.load_waypoints(conf)\n",
    "        self.max_reacquire = 20.\n",
    "\n",
    "        self.drawn_waypoints = []\n",
    "\n",
    "    def load_waypoints(self, conf):\n",
    "        \"\"\"\n",
    "        loads waypoints\n",
    "        \"\"\"\n",
    "        self.waypoints = np.loadtxt(conf.wpt_path, delimiter=conf.wpt_delim, skiprows=conf.wpt_rowskip)\n",
    "\n",
    "    def render_waypoints(self, e):\n",
    "        \"\"\"\n",
    "        update waypoints being drawn by EnvRenderer\n",
    "        \"\"\"\n",
    "\n",
    "        #points = self.waypoints\n",
    "\n",
    "        points = np.vstack((self.waypoints[:, self.conf.wpt_xind], self.waypoints[:, self.conf.wpt_yind])).T\n",
    "        \n",
    "        scaled_points = 50.*points\n",
    "\n",
    "        for i in range(points.shape[0]):\n",
    "            if len(self.drawn_waypoints) < points.shape[0]:\n",
    "                b = e.batch.add(1, GL_POINTS, None, ('v3f/stream', [scaled_points[i, 0], scaled_points[i, 1], 0.]),\n",
    "                                ('c3B/stream', [183, 193, 222]))\n",
    "                self.drawn_waypoints.append(b)\n",
    "            else:\n",
    "                self.drawn_waypoints[i].vertices = [scaled_points[i, 0], scaled_points[i, 1], 0.]\n",
    "        \n",
    "    def _get_current_waypoint(self, waypoints, lookahead_distance, position, theta):\n",
    "        \"\"\"\n",
    "        gets the current waypoint to follow\n",
    "        \"\"\"\n",
    "        wpts = np.vstack((self.waypoints[:, self.conf.wpt_xind], self.waypoints[:, self.conf.wpt_yind])).T\n",
    "        nearest_point, nearest_dist, t, i = nearest_point_on_trajectory(position, wpts)\n",
    "        if nearest_dist < lookahead_distance:\n",
    "            lookahead_point, i2, t2 = first_point_on_trajectory_intersecting_circle(position, lookahead_distance, wpts, i+t, wrap=True)\n",
    "            if i2 == None:\n",
    "                return None\n",
    "            current_waypoint = np.empty((3, ))\n",
    "            # x, y\n",
    "            current_waypoint[0:2] = wpts[i2, :]\n",
    "            # speed\n",
    "            current_waypoint[2] = waypoints[i, self.conf.wpt_vind]\n",
    "            return current_waypoint\n",
    "        elif nearest_dist < self.max_reacquire:\n",
    "            return np.append(wpts[i, :], waypoints[i, self.conf.wpt_vind])\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def plan(self, pose_x, pose_y, pose_theta, lookahead_distance, vgain):\n",
    "        \"\"\"\n",
    "        gives actuation given observation\n",
    "        \"\"\"\n",
    "        position = np.array([pose_x, pose_y])\n",
    "        lookahead_point = self._get_current_waypoint(self.waypoints, lookahead_distance, position, pose_theta)\n",
    "\n",
    "        if lookahead_point is None:\n",
    "            return 4.0, 0.0\n",
    "\n",
    "        speed, steering_angle = get_actuation(pose_theta, lookahead_point, position, lookahead_distance, self.wheelbase)\n",
    "        speed = vgain * speed\n",
    "\n",
    "        return speed, steering_angle\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    main entry point\n",
    "    \"\"\"\n",
    "\n",
    "    work = {'mass': 3.463388126201571, 'lf': 0.15597534362552312, 'tlad': 1.82461887897713965, 'vgain': 0.90338203837889}\n",
    "    \n",
    "    with open('./f1tenth_gym/examples/config_example_map.yaml') as file:\n",
    "        conf_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    conf = Namespace(**conf_dict)\n",
    "\n",
    "    planner = PurePursuitPlanner(conf, 0.17145+0.15875)\n",
    "\n",
    "    def render_callback(env_renderer):\n",
    "        # custom extra drawing function\n",
    "\n",
    "        e = env_renderer\n",
    "\n",
    "        # update camera to follow car\n",
    "        x = e.cars[0].vertices[::2]\n",
    "        y = e.cars[0].vertices[1::2]\n",
    "        top, bottom, left, right = max(y), min(y), min(x), max(x)\n",
    "        e.score_label.x = left\n",
    "        e.score_label.y = top - 700\n",
    "        e.left = left - 800\n",
    "        e.right = right + 800\n",
    "        e.top = top + 800\n",
    "        e.bottom = bottom - 800\n",
    "\n",
    "        planner.render_waypoints(env_renderer)\n",
    "\n",
    "    env = gym.make('f110_gym:f110-v0', map=conf.map_path, map_ext=conf.map_ext, num_agents=1)\n",
    "    env.add_render_callback(render_callback)\n",
    "    \n",
    "    obs, step_reward, done, info = env.reset(np.array([[conf.sx, conf.sy, conf.stheta]]))\n",
    "    env.render()\n",
    "\n",
    "    laptime = 0.0\n",
    "    start = time.time()\n",
    "\n",
    "    while not done:\n",
    "        speed, steer = planner.plan(obs['poses_x'][0], obs['poses_y'][0], obs['poses_theta'][0], work['tlad'], work['vgain'])\n",
    "        obs, step_reward, done, info = env.step(np.array([[steer, speed]]))\n",
    "        laptime += step_reward\n",
    "        env.render(mode='human')\n",
    "        \n",
    "    print('Sim elapsed time:', laptime, 'Real elapsed time:', time.time()-start)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717804a9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### waypoint handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78ecc7a6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import yaml\n",
    "import gym\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "\n",
    "# @njit(fastmath=False, cache=True)\n",
    "def nearest_point_on_trajectory(point, trajectory):\n",
    "    \"\"\"\n",
    "    Return the nearest point along the given piecewise linear trajectory.\n",
    "\n",
    "    Same as nearest_point_on_line_segment, but vectorized. This method is quite fast, time constraints should\n",
    "    not be an issue so long as trajectories are not insanely long.\n",
    "\n",
    "        Order of magnitude: trajectory length: 1000 --> 0.0002 second computation (5000fps)\n",
    "\n",
    "    point: size 2 numpy array\n",
    "    trajectory: Nx2 matrix of (x,y) trajectory waypoints\n",
    "        - these must be unique. If they are not unique, a divide by 0 error will destroy the world\n",
    "    \"\"\"\n",
    "    diffs = trajectory[1:,:] - trajectory[:-1,:]\n",
    "    l2s   = diffs[:,0]**2 + diffs[:,1]**2\n",
    "    # this is equivalent to the elementwise dot product\n",
    "    # dots = np.sum((point - trajectory[:-1,:]) * diffs[:,:], axis=1)\n",
    "    dots = np.empty((trajectory.shape[0]-1, ))\n",
    "    for i in range(dots.shape[0]):\n",
    "        dots[i] = np.dot((point - trajectory[i, :]), diffs[i, :])\n",
    "    t = dots / l2s\n",
    "    t[t<0.0] = 0.0\n",
    "    t[t>1.0] = 1.0\n",
    "    # t = np.clip(dots / l2s, 0.0, 1.0)\n",
    "    projections = trajectory[:-1,:] + (t*diffs.T).T\n",
    "    # dists = np.linalg.norm(point - projections, axis=1)\n",
    "    dists = np.empty((projections.shape[0],))\n",
    "    for i in range(dists.shape[0]):\n",
    "        temp = point - projections[i]\n",
    "        dists[i] = np.sqrt(np.sum(temp*temp))\n",
    "    min_dist_segment = np.argmin(dists)\n",
    "    return projections[min_dist_segment], dists[min_dist_segment], t[min_dist_segment], min_dist_segment\n",
    "\n",
    "# @njit(fastmath=False, cache=True)\n",
    "def first_point_on_trajectory_intersecting_circle(point, radius, trajectory, t=0.0, wrap=False):\n",
    "    \"\"\"\n",
    "    starts at beginning of trajectory, and find the first point one radius away from the given point along the trajectory.\n",
    "\n",
    "    Assumes that the first segment passes within a single radius of the point\n",
    "\n",
    "    http://codereview.stackexchange.com/questions/86421/line-segment-to-circle-collision-algorithm\n",
    "    \"\"\"\n",
    "    start_i = int(t)\n",
    "    start_t = t % 1.0\n",
    "    first_t = None\n",
    "    first_i = None\n",
    "    first_p = None\n",
    "    trajectory = np.ascontiguousarray(trajectory)\n",
    "    for i in range(start_i, trajectory.shape[0]-1):\n",
    "        start = trajectory[i,:]\n",
    "        end = trajectory[i+1,:]+1e-6\n",
    "        V = np.ascontiguousarray(end - start)\n",
    "\n",
    "        a = np.dot(V,V)\n",
    "        b = 2.0*np.dot(V, start - point)\n",
    "        c = np.dot(start, start) + np.dot(point,point) - 2.0*np.dot(start, point) - radius*radius\n",
    "        discriminant = b*b-4*a*c\n",
    "\n",
    "        if discriminant < 0:\n",
    "            continue\n",
    "        #   print \"NO INTERSECTION\"\n",
    "        # else:\n",
    "        # if discriminant >= 0.0:\n",
    "        discriminant = np.sqrt(discriminant)\n",
    "        t1 = (-b - discriminant) / (2.0*a)\n",
    "        t2 = (-b + discriminant) / (2.0*a)\n",
    "        if i == start_i:\n",
    "            if t1 >= 0.0 and t1 <= 1.0 and t1 >= start_t:\n",
    "                first_t = t1\n",
    "                first_i = i\n",
    "                first_p = start + t1 * V\n",
    "                break\n",
    "            if t2 >= 0.0 and t2 <= 1.0 and t2 >= start_t:\n",
    "                first_t = t2\n",
    "                first_i = i\n",
    "                first_p = start + t2 * V\n",
    "                break\n",
    "        elif t1 >= 0.0 and t1 <= 1.0:\n",
    "            first_t = t1\n",
    "            first_i = i\n",
    "            first_p = start + t1 * V\n",
    "            break\n",
    "        elif t2 >= 0.0 and t2 <= 1.0:\n",
    "            first_t = t2\n",
    "            first_i = i\n",
    "            first_p = start + t2 * V\n",
    "            break\n",
    "    # wrap around to the beginning of the trajectory if no intersection is found1\n",
    "    if wrap and first_p is None:\n",
    "        for i in range(-1, start_i):\n",
    "            start = trajectory[i % trajectory.shape[0],:]\n",
    "            end = trajectory[(i+1) % trajectory.shape[0],:]+1e-6\n",
    "            V = end - start\n",
    "\n",
    "            a = np.dot(V,V)\n",
    "            b = 2.0*np.dot(V, start - point)\n",
    "            c = np.dot(start, start) + np.dot(point,point) - 2.0*np.dot(start, point) - radius*radius\n",
    "            discriminant = b*b-4*a*c\n",
    "\n",
    "            if discriminant < 0:\n",
    "                continue\n",
    "            discriminant = np.sqrt(discriminant)\n",
    "            t1 = (-b - discriminant) / (2.0*a)\n",
    "            t2 = (-b + discriminant) / (2.0*a)\n",
    "            if t1 >= 0.0 and t1 <= 1.0:\n",
    "                first_t = t1\n",
    "                first_i = i\n",
    "                first_p = start + t1 * V\n",
    "                break\n",
    "            elif t2 >= 0.0 and t2 <= 1.0:\n",
    "                first_t = t2\n",
    "                first_i = i\n",
    "                first_p = start + t2 * V\n",
    "                break\n",
    "\n",
    "    return first_p, first_i, first_t\n",
    "    \n",
    "    \n",
    "CAPTURE_TIME = 100\n",
    "class F110Env(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(2,))\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(217,), dtype=np.float32)\n",
    "\n",
    "        \n",
    "        work = {'mass': 3.463388126201571, 'lf': 0.15597534362552312, 'tlad': 1.82461887897713965, 'vgain': 0.90338203837889}\n",
    "\n",
    "        with open('./f1tenth_gym/examples/config_example_map.yaml') as file:\n",
    "            conf_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        conf = Namespace(**conf_dict)\n",
    "        self.conf = conf\n",
    "        self.load_waypoints(conf)\n",
    "        self.max_reacquire = 20.\n",
    "\n",
    "        def render_callback(env_renderer):\n",
    "            # custom extra drawing function\n",
    "\n",
    "            e = env_renderer\n",
    "\n",
    "            # update camera to follow car\n",
    "            x = e.cars[0].vertices[::2]\n",
    "            y = e.cars[0].vertices[1::2]\n",
    "            top, bottom, left, right = max(y), min(y), min(x), max(x)\n",
    "            e.score_label.x = left\n",
    "            e.score_label.y = top - 700\n",
    "            e.left = left - 800\n",
    "            e.right = right + 800\n",
    "            e.top = top + 800\n",
    "            e.bottom = bottom - 800\n",
    "\n",
    "\n",
    "        self.env = gym.make('f110_gym:f110-v0', map=conf.map_path, map_ext=conf.map_ext, num_agents=1)\n",
    "        self.env.add_render_callback(render_callback)\n",
    "        self.prev_capture_coord = None\n",
    "        self.reset()\n",
    "        \n",
    "    def load_waypoints(self, conf):\n",
    "        \"\"\"\n",
    "        loads waypoints\n",
    "        \"\"\"\n",
    "        self.waypoints = np.loadtxt(conf.wpt_path, delimiter=conf.wpt_delim, skiprows=conf.wpt_rowskip)\n",
    "\n",
    "        \n",
    "    def _get_current_waypoint(self, waypoints, lookahead_distance, position, theta):\n",
    "        \"\"\"\n",
    "        gets the current waypoint to follow\n",
    "        \"\"\"\n",
    "        wpts = np.vstack((self.waypoints[:, self.conf.wpt_xind], self.waypoints[:, self.conf.wpt_yind])).T\n",
    "        nearest_point, nearest_dist, t, i = nearest_point_on_trajectory(position, wpts)\n",
    "        if nearest_dist < lookahead_distance:\n",
    "            lookahead_point, i2, t2 = first_point_on_trajectory_intersecting_circle(position, lookahead_distance, wpts, i+t, wrap=True)\n",
    "            if i2 == None:\n",
    "                return None\n",
    "            current_waypoint = np.empty((3, ))\n",
    "            # x, y\n",
    "            current_waypoint[0:2] = wpts[i2, :]\n",
    "            # speed\n",
    "            current_waypoint[2] = waypoints[i, self.conf.wpt_vind]\n",
    "            return current_waypoint\n",
    "        elif nearest_dist < self.max_reacquire:\n",
    "            return np.append(wpts[i, :], waypoints[i, self.conf.wpt_vind])\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def reset(self):\n",
    "        obs, step_reward, done, info = self.env.reset(np.array([[self.conf.sx, self.conf.sy, self.conf.stheta]]))\n",
    "        self.prev_capture_coord = [obs['poses_x'][0], obs['poses_y'][0]]\n",
    "        self.time_to_capture = CAPTURE_TIME\n",
    "        self.init_x = 0\n",
    "        return self.to_vector_state(obs)\n",
    "    \n",
    "    def to_vector_state(self, obs):\n",
    "\n",
    "        scanner = np.zeros(1080//5,)\n",
    "        for i in range(1080//5):\n",
    "            scanner[i] = np.clip(np.mean(obs['scans'][0][i*5: i*5+5]), 0, 10)\n",
    "\n",
    "        scanner /= 10\n",
    "        state = np.concatenate([\n",
    "            scanner,\n",
    "            np.array(obs['linear_vels_x'][:1])/5,\n",
    "        ])\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "\n",
    "        action[0] = action[0]*np.pi/4\n",
    "        action[1] = action[1]*2.5 + 2.5\n",
    "        action = action.reshape(1, 2)\n",
    "        action = np.repeat(action, repeats=2, axis=0)\n",
    "        action[1][1] = 0\n",
    "        # print(action)\n",
    "        \n",
    "\n",
    "        obs, step_reward, done, info = self.env.step(action)\n",
    "        \n",
    "        pose_x = obs['poses_x'][0]\n",
    "        pose_y = obs['poses_y'][0]\n",
    "        \n",
    "        position = np.array([pose_x, pose_y])\n",
    "        lookahead_point = self._get_current_waypoint(self.waypoints, 1.8, position, 0.9033)\n",
    "        \n",
    "        print(position, lookahead_point)\n",
    "        \n",
    "        reward = 0\n",
    "        if obs['collisions'][0] == 1.0:\n",
    "            reward = -10\n",
    "        \n",
    "\n",
    "        next_state = self.to_vector_state(obs)\n",
    "        reward += (step_reward + obs['linear_vels_x'][0]*0.01)\n",
    "\n",
    "        self.time_to_capture -= 1\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08e1bb2",
   "metadata": {},
   "source": [
    "### checkpoint handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99d8936d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.F110Env at 0x2abd9b02ff10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import yaml\n",
    "import gym\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "\n",
    "    \n",
    "class F110Env(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        self.action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(1,))\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(362,), dtype=np.float32)\n",
    "        self.min_cp_dist = 2.0\n",
    "        self.cp_reward = 1.0\n",
    "\n",
    "        with open('./f1tenth_gym/examples/config_example_map.yaml') as file:\n",
    "            conf_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        conf = Namespace(**conf_dict)\n",
    "        self.conf = conf\n",
    "        wps = np.loadtxt(conf.wpt_path, delimiter=conf.wpt_delim, skiprows=conf.wpt_rowskip)[:, 1:3]\n",
    "        idxs = [i%10 == 0 for i in range(len(wps))]\n",
    "        self.checkpoints = wps[idxs][1:]\n",
    "        self.t = 0\n",
    "        \n",
    "        \n",
    "        def render_callback(env_renderer):\n",
    "            # custom extra drawing function\n",
    "\n",
    "            e = env_renderer\n",
    "\n",
    "            # update camera to follow car\n",
    "            x = e.cars[0].vertices[::2]\n",
    "            y = e.cars[0].vertices[1::2]\n",
    "            top, bottom, left, right = max(y), min(y), min(x), max(x)\n",
    "            e.score_label.x = left\n",
    "            e.score_label.y = top - 700\n",
    "            e.left = left - 800\n",
    "            e.right = right + 800\n",
    "            e.top = top + 800\n",
    "            e.bottom = bottom - 800\n",
    "\n",
    "\n",
    "        self.env = gym.make('f110_gym:f110-v0', map=conf.map_path, map_ext=conf.map_ext, num_agents=1)\n",
    "        self.env.add_render_callback(render_callback)\n",
    "        self.prev_capture_coord = None\n",
    "        self.reset()\n",
    "        \n",
    "  \n",
    "    def reset(self):\n",
    "        obs, step_reward, done, info = self.env.reset(np.array([[self.conf.sx, self.conf.sy, self.conf.stheta]]))\n",
    "        self.next_cp_idx = 0\n",
    "        self.t = 0\n",
    "        return self.to_vector_state(obs)\n",
    "    \n",
    "    def to_vector_state(self, obs):\n",
    "        buck = 3\n",
    "        scanner = np.zeros(1080//buck,)\n",
    "        for i in range(1080//buck):\n",
    "            scanner[i] = np.clip(np.mean(obs['scans'][0][i*buck: i*buck+buck]), 0, 10)\n",
    "        scanner /= 10\n",
    "        \n",
    "        state = np.concatenate([\n",
    "            scanner,\n",
    "            np.array(obs['linear_vels_x'])/5,\n",
    "            np.array(obs['ang_vels_z'])\n",
    "        ])\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def checkpoint(self, position):\n",
    "        dist = np.linalg.norm(position - self.checkpoints[self.next_cp_idx])\n",
    "        reward = 0\n",
    "        if dist < self.min_cp_dist:\n",
    "#             print(f\"Got to CP {self.next_cp_idx}\")\n",
    "            reward = self.cp_reward\n",
    "    \n",
    "            self.next_cp_idx = (self.next_cp_idx + 1)%len(self.checkpoints)\n",
    "        return reward\n",
    "        \n",
    "    def step(self, action):\n",
    "\n",
    "        angle = action[0]*(np.pi/4)\n",
    "#         action[1] = action[1]*5\n",
    "        speed = 5\n",
    "        act = np.array([[angle, speed]])\n",
    "#         action = np.repeat(action, repeats=2, axis=0)\n",
    "#         action[1][1] = 0\n",
    "        # print(action)\n",
    "        \n",
    "\n",
    "        obs, step_reward, done, info = self.env.step(act)\n",
    "        pose_x = obs['poses_x'][0]\n",
    "        pose_y = obs['poses_y'][0]\n",
    "        \n",
    "        position = np.array([pose_x, pose_y])\n",
    "#         print(action, position)\n",
    "        \n",
    "        \n",
    "        reward = 0\n",
    "        if obs['collisions'][0] == 1.0:\n",
    "            reward = -1\n",
    "            \n",
    "#         if int(self.t+1) % 100 == 0:\n",
    "#             print(self.t+1, position, action[0], angle)\n",
    "        \n",
    "        cp_reward = self.checkpoint(position)\n",
    "        next_state = self.to_vector_state(obs)\n",
    "        reward += cp_reward\n",
    "        self.t += 1\n",
    "\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "F110Env({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fbd2cb",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Env simple wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbf8e2c7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "CAPTURE_TIME = 100\n",
    "class F110Env(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(2,))\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(217,), dtype=np.float32)\n",
    "        \n",
    "        self.env = gym.make(\n",
    "            'f110_gym:f110-v0',\n",
    "            map='./f1tenth_gym/examples/example_map',\n",
    "            map_ext='.png'\n",
    "        )\n",
    "        \n",
    "        self.reset()\n",
    "        self.prev_capture_coord = None\n",
    "        \n",
    "    def reset(self):\n",
    "        obs, step_reward, done, info = self.env.reset(\n",
    "            poses=np.array([[0., 0., 0.], \n",
    "                     [-1., -1., 0.]]) \n",
    "        )\n",
    "        self.prev_capture_coord = [obs['poses_x'][0], obs['poses_y'][0]]\n",
    "        self.time_to_capture = CAPTURE_TIME\n",
    "        self.init_x = 0\n",
    "        return self.to_vector_state(obs)\n",
    "    \n",
    "    def to_vector_state(self, obs):\n",
    "\n",
    "        scanner = np.zeros(1080//5,)\n",
    "        for i in range(1080//5):\n",
    "            scanner[i] = np.clip(np.mean(obs['scans'][0][i*5: i*5+5]), 0, 10)\n",
    "\n",
    "        scanner /= 10\n",
    "        state = np.concatenate([\n",
    "            scanner,\n",
    "            np.array(obs['linear_vels_x'][:1])/5,\n",
    "        ])\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "\n",
    "        action[0] = action[0]*np.pi/4\n",
    "        action[1] = action[1]*2.5 + 2.5\n",
    "        action = action.reshape(1, 2)\n",
    "        action = np.repeat(action, repeats=2, axis=0)\n",
    "        action[1][1] = 0\n",
    "        # print(action)\n",
    "        obs, step_reward, done, info = self.env.step(action)\n",
    "        reward = 0\n",
    "        if obs['collisions'][0] == 1.0:\n",
    "            reward = -10\n",
    "        \n",
    "        moving_forward_rew = 0\n",
    "\n",
    "        if self.time_to_capture == 0:\n",
    "            current_coord = [obs['poses_x'][0], obs['poses_y'][0]]\n",
    "            dist = abs(current_coord[0] - self.prev_capture_coord[0]) + abs(current_coord[1] - self.prev_capture_coord[1])\n",
    "            # print(f\"prev coord:{self.prev_capture_coord}, current_coord:{current_coord}, dist:{dist}\")\n",
    "            \n",
    "            self.prev_capture_coord = current_coord\n",
    "            if dist < 2:\n",
    "                # print(\"Neg reward\")\n",
    "                moving_forward_rew = -10\n",
    "\n",
    "            self.time_to_capture = CAPTURE_TIME + 1\n",
    "\n",
    "        next_state = self.to_vector_state(obs)\n",
    "        # ang_vel = obs['ang_vels_z'][0]\n",
    "        # print(obs['ang_vels_z'][0]*0.1)\n",
    "        reward += (step_reward + obs['linear_vels_x'][0]*0.01)\n",
    "\n",
    "        self.time_to_capture -= 1\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc32abc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde1bf5b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "done = False\n",
    "env = F110Env({'explore':False})\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = np.array([1.0, 0.0])\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311e9da4",
   "metadata": {},
   "source": [
    "# RAY algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1526925b",
   "metadata": {},
   "source": [
    "## ppo continuous actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7731806",
   "metadata": {},
   "source": [
    "### continious env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74a6c5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of waypoints:79\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.F110Env at 0x2ae697ecc8b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import yaml\n",
    "import gym\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "\n",
    "    \n",
    "class F110Env(gym.Env):\n",
    "    def __init__(self, env_config, deterministic=False, max_v=10, map_no=4):\n",
    "        \"\"\"\n",
    "        break: [0., 0.]\n",
    "        fast forward: [0., 5.]\n",
    "        fast left: [-pi/4, 5.]\n",
    "        fast right: [pi/4, 5]\n",
    "        slow left: [-pi/4, 2] #later\n",
    "        slow right: [pi/4, 2] \n",
    "        \n",
    "        \"\"\"\n",
    "        self.deterministic = deterministic\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(362,), dtype=np.float32)\n",
    "        self.min_cp_dist = 2.0\n",
    "        self.cp_reward = 1.0\n",
    "        self.max_v = max_v\n",
    "        self.map_no = map_no\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "        \n",
    "\n",
    "        with open('./f1tenth_gym/examples/config_example_map.yaml') as file:\n",
    "            conf_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        conf = Namespace(**conf_dict)\n",
    "        self.conf = conf\n",
    "        wps = np.loadtxt(conf.wpt_path, delimiter=conf.wpt_delim, skiprows=0)[:, 1:4]\n",
    "        idxs = [i%10 == 0 for i in range(len(wps))]\n",
    "        \n",
    "        self.min_x, self.max_x = np.min(wps[:,0]), np.max(wps[:, 0])\n",
    "        self.min_y, self.max_y = np.min(wps[:,1]), np.max(wps[:, 1])\n",
    "\n",
    "        self.checkpoints = wps[idxs]\n",
    "        print(f\"number of waypoints:{len(self.checkpoints)}\")        \n",
    "        \n",
    "#         self.low_theta = 1.1\n",
    "#         self.high_theta = 2.1\n",
    "#         print(f\"initializing theta between: {self.low_theta} and {self.high_theta}\")\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "  \n",
    "    def reset(self):\n",
    "        def render_callback(env_renderer):\n",
    "            # custom extra drawing function\n",
    "            \n",
    "            e = env_renderer\n",
    "\n",
    "            # update camera to follow car\n",
    "            x = e.cars[0].vertices[::2]\n",
    "            y = e.cars[0].vertices[1::2]\n",
    "            top, bottom, left, right = max(y), min(y), min(x), max(x)\n",
    "            e.score_label.x = left\n",
    "            e.score_label.y = top - 700\n",
    "            e.left = left - 800\n",
    "            e.right = right + 800\n",
    "            e.top = top + 800\n",
    "            e.bottom = bottom - 800\n",
    "            \n",
    "        map_no = np.random.randint(1, 5) if self.map_no is None else self.map_no\n",
    "        map_path = f\"./f1tenth_gym/examples/{map_no}\"\n",
    "        \n",
    "        if self.deterministic:\n",
    "            print(f\"using map:{map_path}\")\n",
    "            \n",
    "        self.env = gym.make('f110_gym:f110-v0', map=map_path, map_ext=self.conf.map_ext, num_agents=1)\n",
    "        self.env.add_render_callback(render_callback)\n",
    "        \n",
    "        #theta = np.random.rand() * (self.high_theta - self.low_theta) + self.low_theta\n",
    "        \n",
    "        random_idx = np.random.randint(0, len(self.checkpoints)-1)\n",
    "        current_pos = self.checkpoints[random_idx][:2]\n",
    "        theta = self.checkpoints[random_idx][2] + np.pi/2\n",
    "        obs, step_reward, done, info = self.env.reset(np.array([[current_pos[0], current_pos[1], theta]]))\n",
    "        #obs, step_reward, done, info = self.env.reset(np.array([[self.conf.sx, self.conf.sy, theta]]))\n",
    " \n",
    "        self.next_cp_idx = random_idx + 1\n",
    "        #self.next_cp_idx = 1\n",
    "    \n",
    "        self.t = 0\n",
    "        return self.to_vector_state(obs)\n",
    "    \n",
    "    def to_vector_state(self, obs):\n",
    "        scanner = obs['scans'][0]\n",
    "        \n",
    "        buck = 3\n",
    "        size = 1080//buck\n",
    "        scanner = np.zeros(size,)\n",
    "        for i in range(size):\n",
    "            scanner[i] = np.clip(np.mean(obs['scans'][0][i*buck: i*buck+buck]), 0, 10)\n",
    "        \n",
    "        scanner /= 10.0\n",
    "        \n",
    "        state = np.concatenate([\n",
    "            scanner,\n",
    "            np.array(obs['linear_vels_x'][:1])/self.max_v,\n",
    "            np.array(obs['ang_vels_z'][:1])/2,\n",
    "        ])\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def checkpoint(self, position):\n",
    "        dist = np.linalg.norm(position - self.checkpoints[self.next_cp_idx][:2])\n",
    "        reward = 0\n",
    "        if dist < self.min_cp_dist:\n",
    "            reward = self.cp_reward\n",
    "    \n",
    "            self.next_cp_idx = (self.next_cp_idx + 1)%len(self.checkpoints)\n",
    "        return reward\n",
    "        \n",
    "    def step(self, action):\n",
    "        angle = action[0]*np.pi/4.\n",
    "        speed = action[1]*5.0 + 5.0\n",
    "        act = np.array([[angle, speed]])\n",
    "        \n",
    "#         if not self.deterministic:\n",
    "#             act[0] += np.random.normal(0, 0.05, size=(2,))\n",
    "        \n",
    "        obs, step_reward, done, info = self.env.step(act)\n",
    "        pose_x = obs['poses_x'][0]\n",
    "        pose_y = obs['poses_y'][0]\n",
    "        \n",
    "        reward = 0\n",
    "#         if obs['lap_counts'][0] == 1.0:\n",
    "#             done = True\n",
    "#             reward = 1.0\n",
    "        \n",
    "        position = np.array([pose_x, pose_y])\n",
    "        \n",
    "        if obs['collisions'][0] == 1.0:\n",
    "            done = True\n",
    "            reward = -1\n",
    "            \n",
    "        cp_reward = self.checkpoint(position)\n",
    "        next_state = self.to_vector_state(obs)\n",
    "        reward += cp_reward\n",
    "        \n",
    "        #padding from wall:\n",
    "        scans = obs['scans'][0]\n",
    "        if min(scans) < 0.1:\n",
    "            reward -= 0.1\n",
    "            \n",
    "        self.t += 1\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "F110Env({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c85938c",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe30888f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-08 17:45:09,459\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-03-08 17:45:09,460\tINFO trainer.py:790 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=39527)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39527)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39508)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39508)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39504)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39504)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39528)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39528)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39521)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39521)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39520)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39520)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39517)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39517)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39494)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39494)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39490)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39490)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39487)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39487)\u001b[0m initializing theta between: 1.1 and 2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=39527)\u001b[0m 2022-03-08 17:45:23,337\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39528)\u001b[0m 2022-03-08 17:45:23,679\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39487)\u001b[0m 2022-03-08 17:45:23,629\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39508)\u001b[0m 2022-03-08 17:45:23,749\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39521)\u001b[0m 2022-03-08 17:45:23,784\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39520)\u001b[0m 2022-03-08 17:45:23,787\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39517)\u001b[0m 2022-03-08 17:45:23,778\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39494)\u001b[0m 2022-03-08 17:45:23,740\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39490)\u001b[0m 2022-03-08 17:45:23,812\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39504)\u001b[0m 2022-03-08 17:45:23,861\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "2022-03-08 17:45:29,093\tINFO trainable.py:125 -- Trainable.setup took 19.635 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-03-08 17:45:29,095\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_workers': 10, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'complete_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 10000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [512, 512], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'F110Env', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 10, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'complete_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 10000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [512, 512], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'F110Env', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1.0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.1, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.agents.ppo.ppo_torch_policy.PPOTorchPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.01, 'entropy_coeff_schedule': None, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1.0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.1, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.agents.ppo.ppo_torch_policy.PPOTorchPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.01, 'entropy_coeff_schedule': None, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-08 17:46:06,028\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward:1.6736842105263159\n",
      "checkpoint saved at ./checkpoints/ppo_cont/checkpoint_000001/checkpoint-1\n",
      "episode: 1 reward:1.97\n",
      "episode: 2 reward:1.98\n",
      "episode: 3 reward:2.25\n",
      "episode: 4 reward:3.02\n",
      "episode: 5 reward:3.6\n",
      "episode: 6 reward:5.19\n",
      "episode: 7 reward:3.64\n",
      "episode: 8 reward:4.02\n",
      "episode: 9 reward:6.18\n",
      "episode: 10 reward:7.08\n",
      "checkpoint saved at ./checkpoints/ppo_cont/checkpoint_000011/checkpoint-11\n",
      "episode: 11 reward:8.21\n",
      "episode: 12 reward:8.45\n",
      "episode: 13 reward:10.19\n",
      "episode: 14 reward:9.47\n",
      "episode: 15 reward:9.17\n",
      "episode: 16 reward:10.27\n",
      "episode: 17 reward:11.219000000000001\n",
      "episode: 18 reward:11.509\n",
      "episode: 19 reward:12.109000000000002\n",
      "episode: 20 reward:13.01\n",
      "checkpoint saved at ./checkpoints/ppo_cont/checkpoint_000021/checkpoint-21\n",
      "episode: 21 reward:13.21\n",
      "episode: 22 reward:12.94\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.agents import ppo, sac, ddpg, dqn\n",
    "from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "\n",
    "DEFAULT_CONFIG['framework'] = 'torch'\n",
    "DEFAULT_CONFIG['num_workers'] = 10\n",
    "DEFAULT_CONFIG['num_gpus'] = 1.0\n",
    "DEFAULT_CONFIG['num_gpus_per_worker'] = 1/10\n",
    "DEFAULT_CONFIG['model']['fcnet_hiddens'] = [300, 300, 300]\n",
    "DEFAULT_CONFIG['entropy_coeff'] = 0.0\n",
    "DEFAULT_CONFIG['clip_param'] = 0.2\n",
    "DEFAULT_CONFIG['train_batch_size'] = 5000\n",
    "DEFAULT_CONFIG['batch_mode'] = 'truncate_episodes'\n",
    "# DEFAULT_CONFIG['model']['use_lstm'] = True\n",
    "# DEFAULT_CONFIG['model']['lstm_use_prev_action'] = True\n",
    "# DEFAULT_CONFIG['model']['max_seq_len'] = 10\n",
    "# DEFAULT_CONFIG['lr'] = 5e-5\n",
    "\n",
    "trainer = ppo.PPOTrainer(env=F110Env, config=DEFAULT_CONFIG)\n",
    "print(trainer.config)\n",
    "rewards = []\n",
    "\n",
    "import pickle\n",
    "\n",
    "for i in range(10000):\n",
    "    result = trainer.train()\n",
    "    print(f\"episode: {i} reward:{result['episode_reward_mean']}\")\n",
    "    rewards.append(result['episode_reward_mean'])\n",
    "    if i%50 == 0 or (i<50 and i%10==0):\n",
    "        with open('./checkpoints/ppo_cont_r', 'wb') as f:\n",
    "            pickle.dump(rewards, f)\n",
    "        cp = trainer.save(\"./checkpoints/ppo_cont\")\n",
    "        print(\"checkpoint saved at\", cp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1f18b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b66d1174d60>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7ea0126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./ppo_cp_2/checkpoint_000043/checkpoint-43'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save(\"./ppo_cp_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64aa25d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of waypoints:79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 12:33:40,729\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-03-14 12:33:40,742\tINFO trainable.py:472 -- Restored on 128.205.39.153 from checkpoint: ./checkpoints/ppo_cont/checkpoint_000751/checkpoint-751\n",
      "2022-03-14 12:33:40,743\tINFO trainable.py:480 -- Current state after restoring: {'_iteration': 751, '_timesteps_total': 3755000, '_time_total': 12026.480172395706, '_episodes_total': 2561}\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.agents import ppo, sac, ddpg, dqn\n",
    "from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG\n",
    "\n",
    "\n",
    "DEFAULT_CONFIG['framework'] = 'torch'\n",
    "DEFAULT_CONFIG['num_workers'] = 0\n",
    "DEFAULT_CONFIG['model']['fcnet_hiddens'] = [300, 300, 300]\n",
    "DEFAULT_CONFIG['entropy_coeff'] = 0.0\n",
    "DEFAULT_CONFIG['clip_param'] = 0.2\n",
    "DEFAULT_CONFIG['train_batch_size'] = 5000\n",
    "DEFAULT_CONFIG['batch_mode'] = 'truncate_episodes'\n",
    "DEFAULT_CONFIG[\"evaluation_config\"][\"explore\"] = False\n",
    "# DEFAULT_CONFIG['model']['use_lstm'] = True\n",
    "# DEFAULT_CONFIG['model']['lstm_use_prev_action'] = True\n",
    "# DEFAULT_CONFIG['model']['max_seq_len'] = 10\n",
    "# DEFAULT_CONFIG['lr'] = 5e-5\n",
    "\n",
    "trainer = ppo.PPOTrainer(env=F110Env, config=DEFAULT_CONFIG)\n",
    "trainer.restore('./checkpoints/ppo_cont/checkpoint_000751/checkpoint-751')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de268ad3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mreset()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "238037f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9726   , 0.9282523], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.compute_single_action(observation=obs, explore=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e6cdb1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of waypoints:79\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "env = F110Env({'explore':False})\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = trainer.compute_single_action(observation=obs, explore=False)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fbaea3",
   "metadata": {},
   "source": [
    "## discrete action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5da93e",
   "metadata": {},
   "source": [
    "### define environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1eef26d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "number of waypoints:79\n",
      "using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +6m3s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +6m38s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import yaml\n",
    "import gym\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "\n",
    "    \n",
    "class F110Env(gym.Env):\n",
    "    def __init__(self, env_config, deterministic=False):\n",
    "        \"\"\"\n",
    "        break: [0., 0.]\n",
    "        fast forward: [0., 5.]\n",
    "        fast left: [-pi/4, 5.]\n",
    "        fast right: [pi/4, 5]\n",
    "        slow left: [-pi/4, 2] #later\n",
    "        slow right: [pi/4, 2] \n",
    "        \n",
    "        \"\"\"\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(364,), dtype=np.float32)\n",
    "        self.min_cp_dist = 2.0\n",
    "        self.cp_reward = 1.0\n",
    "        self.max_v = 5.0\n",
    "        self.deterministic = deterministic\n",
    "        \n",
    "        self.action_map = {\n",
    "            0: [0., 0.],\n",
    "            1: [0., self.max_v],\n",
    "            2: [-np.pi/4, self.max_v],\n",
    "            3: [np.pi/4, self.max_v],\n",
    "            4: [-np.pi/6, self.max_v],\n",
    "            5: [np.pi/6, self.max_v],\n",
    "            6: [-np.pi/4, self.max_v/2], #later\n",
    "            7: [np.pi/4, self.max_v/2] #later\n",
    "        }\n",
    "        self.action_space = gym.spaces.Discrete(len(self.action_map),)\n",
    "        \n",
    "\n",
    "        with open('./f1tenth_gym/examples/config_example_map.yaml') as file:\n",
    "            conf_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        conf = Namespace(**conf_dict)\n",
    "        self.conf = conf\n",
    "        wps = np.loadtxt(conf.wpt_path, delimiter=conf.wpt_delim, skiprows=0)[:, 1:3]\n",
    "        idxs = [i%10 == 0 for i in range(len(wps))]\n",
    "        \n",
    "        self.min_x, self.max_x = np.min(wps[:,0]), np.max(wps[:, 0])\n",
    "        self.min_y, self.max_y = np.min(wps[:,1]), np.max(wps[:, 1])\n",
    "\n",
    "        self.checkpoints = wps[idxs]\n",
    "        self.t = 0\n",
    "        \n",
    "        print(f\"x and y range: {self.min_x}to{self.max_x} and {self.min_y}to{self.max_y}\")\n",
    "        print(f\"number of waypoints:{len(self.checkpoints)}\")        \n",
    "        \n",
    "        def render_callback(env_renderer):\n",
    "            # custom extra drawing function\n",
    "\n",
    "            e = env_renderer\n",
    "\n",
    "            # update camera to follow car\n",
    "            x = e.cars[0].vertices[::2]\n",
    "            y = e.cars[0].vertices[1::2]\n",
    "            top, bottom, left, right = max(y), min(y), min(x), max(x)\n",
    "            e.score_label.x = left\n",
    "            e.score_label.y = top - 700\n",
    "            e.left = left - 800\n",
    "            e.right = right + 800\n",
    "            e.top = top + 800\n",
    "            e.bottom = bottom - 800\n",
    "\n",
    "        print(f\"using map:{conf.map_path}\")\n",
    "        self.env = gym.make('f110_gym:f110-v0', map=conf.map_path, map_ext=conf.map_ext, num_agents=1)\n",
    "        self.env.add_render_callback(render_callback)\n",
    "        self.prev_capture_coord = None\n",
    "\n",
    "        self.reset()\n",
    "        \n",
    "  \n",
    "    def reset(self):\n",
    "        obs, step_reward, done, info = self.env.reset(np.array([[self.conf.sx, self.conf.sy, self.conf.stheta]]))\n",
    "        \n",
    "        self.next_cp_idx = 1\n",
    "        self.t = 0\n",
    "        return self.to_vector_state(obs)\n",
    "    \n",
    "    def to_vector_state(self, obs):\n",
    "        scanner = obs['scans'][0]\n",
    "        \n",
    "        buck = 3\n",
    "        size = 1080//buck\n",
    "        scanner = np.zeros(size,)\n",
    "        for i in range(size):\n",
    "            scanner[i] = np.clip(np.mean(obs['scans'][0][i*buck: i*buck+buck]), 0, 10)\n",
    "        \n",
    "        scanner /= 10.0\n",
    "        \n",
    "        state = np.concatenate([\n",
    "            scanner,\n",
    "            np.array(obs['linear_vels_x'][:1])/self.max_v,\n",
    "            np.array(obs['ang_vels_z'][:1])/2,\n",
    "            np.array(obs['poses_x'][:1]/(self.max_x-self.min_x)),\n",
    "            np.array(obs['poses_y'][:1]/(self.max_y-self.min_y))\n",
    "        ])\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def checkpoint(self, position):\n",
    "        dist = np.linalg.norm(position - self.checkpoints[self.next_cp_idx])\n",
    "        reward = 0\n",
    "        if dist < self.min_cp_dist:\n",
    "#             print(f\"Got to CP {self.next_cp_idx}\")\n",
    "            reward = self.cp_reward\n",
    "    \n",
    "            self.next_cp_idx = (self.next_cp_idx + 1)%len(self.checkpoints)\n",
    "        return reward\n",
    "        \n",
    "    def step(self, action):\n",
    "\n",
    "        act = np.array([self.action_map[action]])\n",
    "        \n",
    "        obs, step_reward, done, info = self.env.step(act)\n",
    "        pose_x = obs['poses_x'][0]\n",
    "        pose_y = obs['poses_y'][0]\n",
    "        \n",
    "        position = np.array([pose_x, pose_y])\n",
    "        \n",
    "        reward = 0\n",
    "        if obs['collisions'][0] == 1.0:\n",
    "            reward = -1\n",
    "            \n",
    "#         if int(self.t+1) % 100 == 0:\n",
    "#             print(action)\n",
    "        \n",
    "        cp_reward = self.checkpoint(position)\n",
    "        next_state = self.to_vector_state(obs)\n",
    "        reward += cp_reward\n",
    "        self.t += 1\n",
    "\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "F110Env({}).render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a5dd80",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c50c87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.agents import ppo, sac, ddpg, dqn\n",
    "from ray.rllib.agents.dqn.apex import APEX_DEFAULT_CONFIG\n",
    "from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "\n",
    "DEFAULT_CONFIG['framework'] = 'torch'\n",
    "DEFAULT_CONFIG['num_workers'] = 30\n",
    "DEFAULT_CONFIG['num_gpus'] = 2\n",
    "DEFAULT_CONFIG['model']['fcnet_hiddens'] = [1024, 1024]\n",
    "\n",
    "\n",
    "APEX_DEFAULT_CONFIG['framework'] = 'torch'\n",
    "APEX_DEFAULT_CONFIG['num_gpus'] = 1\n",
    "APEX_DEFAULT_CONFIG['exploration_config']['epsilon_timesteps'] = int(1e8)\n",
    "APEX_DEFAULT_CONFIG['model']['fcnet_hiddens'] = [1024, 1024]\n",
    "APEX_DEFAULT_CONFIG['final_epsilon'] = 0.1\n",
    "APEX_DEFAULT_CONFIG['buffer_size'] =  2000000\n",
    "\n",
    "# trainer = dqn.ApexTrainer(env=F110Env, config=APEX_DEFAULT_CONFIG)\n",
    "trainer = ppo.PPOTrainer(env=F110Env, config=DEFAULT_CONFIG)\n",
    "rewards = []\n",
    "\n",
    "import pickle\n",
    "\n",
    "for i in range(10000):\n",
    "    result = trainer.train()\n",
    "    print(f\"episode: {i} reward:{result['episode_reward_mean']}\")\n",
    "    rewards.append(result['episode_reward_mean'])\n",
    "    if i%50 == 0:\n",
    "        with open('./checkpoints/ppo_disc_r2', 'wb') as f:\n",
    "            pickle.dump(rewards, f)\n",
    "        cp = trainer.save(\"./checkpoints/ppo_disc_hard_cp2\")\n",
    "        print(\"checkpoint saved at\", cp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "568a5022",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_workers': 32,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'create_env_on_driver': False,\n",
       " 'rollout_fragment_length': 50,\n",
       " 'batch_mode': 'truncate_episodes',\n",
       " 'gamma': 0.99,\n",
       " 'lr': 0.0005,\n",
       " 'train_batch_size': 512,\n",
       " 'model': {'_use_default_native_models': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  'fcnet_hiddens': [256, 256],\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'post_fcnet_hiddens': [],\n",
       "  'post_fcnet_activation': 'relu',\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': True,\n",
       "  'use_lstm': False,\n",
       "  'max_seq_len': 20,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action': False,\n",
       "  'lstm_use_prev_reward': False,\n",
       "  '_time_major': False,\n",
       "  'use_attention': False,\n",
       "  'attention_num_transformer_units': 1,\n",
       "  'attention_dim': 64,\n",
       "  'attention_num_heads': 1,\n",
       "  'attention_head_dim': 32,\n",
       "  'attention_memory_inference': 50,\n",
       "  'attention_memory_training': 50,\n",
       "  'attention_position_wise_mlp_dim': 32,\n",
       "  'attention_init_gru_gate_bias': 2.0,\n",
       "  'attention_use_n_prev_actions': 0,\n",
       "  'attention_use_n_prev_rewards': 0,\n",
       "  'framestack': True,\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None,\n",
       "  'lstm_use_prev_action_reward': -1},\n",
       " 'optimizer': {'max_weight_sync_delay': 400,\n",
       "  'num_replay_buffer_shards': 4,\n",
       "  'debug': False},\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'env': None,\n",
       " 'observation_space': None,\n",
       " 'action_space': None,\n",
       " 'env_config': {},\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'env_task_fn': None,\n",
       " 'render_env': False,\n",
       " 'record_env': False,\n",
       " 'clip_rewards': None,\n",
       " 'normalize_actions': True,\n",
       " 'clip_actions': False,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'log_level': 'WARN',\n",
       " 'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       " 'ignore_worker_failures': False,\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'framework': 'torch',\n",
       " 'eager_tracing': False,\n",
       " 'eager_max_retraces': 20,\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'PerWorkerEpsilonGreedy',\n",
       "  'initial_epsilon': 1.0,\n",
       "  'final_epsilon': 0.02,\n",
       "  'epsilon_timesteps': 10000},\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_duration': 10,\n",
       " 'evaluation_duration_unit': 'episodes',\n",
       " 'evaluation_parallel_to_training': False,\n",
       " 'in_evaluation': False,\n",
       " 'evaluation_config': {'explore': False},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'custom_eval_function': None,\n",
       " 'always_attach_evaluation_results': False,\n",
       " 'sample_async': False,\n",
       " 'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'compress_observations': False,\n",
       " 'metrics_episode_collection_timeout_s': 180,\n",
       " 'metrics_num_episodes_for_smoothing': 100,\n",
       " 'min_time_s_per_reporting': None,\n",
       " 'min_train_timesteps_per_reporting': None,\n",
       " 'min_sample_timesteps_per_reporting': None,\n",
       " 'seed': None,\n",
       " 'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_gpus': 2,\n",
       " '_fake_gpus': False,\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'num_cpus_for_driver': 1,\n",
       " 'placement_strategy': 'PACK',\n",
       " 'input': 'sampler',\n",
       " 'input_config': {},\n",
       " 'actions_in_input_normalized': False,\n",
       " 'input_evaluation': ['is', 'wis'],\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'multiagent': {'policies': {},\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_map_cache': None,\n",
       "  'policy_mapping_fn': None,\n",
       "  'policies_to_train': None,\n",
       "  'observation_fn': None,\n",
       "  'replay_mode': 'independent',\n",
       "  'count_steps_by': 'env_steps'},\n",
       " 'logger_config': None,\n",
       " '_tf_policy_handles_more_than_one_loss': False,\n",
       " '_disable_preprocessor_api': False,\n",
       " '_disable_action_flattening': False,\n",
       " '_disable_execution_plan_api': False,\n",
       " 'simple_optimizer': -1,\n",
       " 'monitor': -1,\n",
       " 'evaluation_num_episodes': -1,\n",
       " 'metrics_smoothing_episodes': -1,\n",
       " 'timesteps_per_iteration': 25000,\n",
       " 'min_iter_time_s': 30,\n",
       " 'collect_metrics_timeout': -1,\n",
       " 'target_network_update_freq': 500000,\n",
       " 'buffer_size': 2000000,\n",
       " 'replay_buffer_config': None,\n",
       " 'store_buffer_in_checkpoints': False,\n",
       " 'replay_sequence_length': 1,\n",
       " 'lr_schedule': None,\n",
       " 'adam_epsilon': 1e-08,\n",
       " 'grad_clip': 40,\n",
       " 'learning_starts': 50000,\n",
       " 'num_atoms': 1,\n",
       " 'v_min': -10.0,\n",
       " 'v_max': 10.0,\n",
       " 'noisy': False,\n",
       " 'sigma0': 0.5,\n",
       " 'dueling': True,\n",
       " 'hiddens': [256],\n",
       " 'double_q': True,\n",
       " 'n_step': 3,\n",
       " 'prioritized_replay': True,\n",
       " 'prioritized_replay_alpha': 0.6,\n",
       " 'prioritized_replay_beta': 0.4,\n",
       " 'final_prioritized_replay_beta': 0.4,\n",
       " 'prioritized_replay_beta_annealing_timesteps': 20000,\n",
       " 'prioritized_replay_eps': 1e-06,\n",
       " 'before_learn_on_batch': None,\n",
       " 'training_intensity': None,\n",
       " 'worker_side_prioritization': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=1477)\u001b[0m 2022-02-19 17:34:53,395\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1486)\u001b[0m 2022-02-19 17:34:53,556\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1506)\u001b[0m 2022-02-19 17:34:53,728\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1480)\u001b[0m 2022-02-19 17:34:53,666\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1455)\u001b[0m 2022-02-19 17:34:53,676\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1495)\u001b[0m 2022-02-19 17:34:53,740\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1501)\u001b[0m 2022-02-19 17:34:53,727\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1465)\u001b[0m 2022-02-19 17:34:53,738\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1491)\u001b[0m 2022-02-19 17:34:53,672\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1457)\u001b[0m 2022-02-19 17:34:53,724\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1494)\u001b[0m 2022-02-19 17:34:53,727\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1462)\u001b[0m 2022-02-19 17:34:53,737\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1468)\u001b[0m 2022-02-19 17:34:53,809\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1509)\u001b[0m 2022-02-19 17:34:53,797\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1502)\u001b[0m 2022-02-19 17:34:53,811\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1493)\u001b[0m 2022-02-19 17:34:53,758\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1479)\u001b[0m 2022-02-19 17:34:53,818\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1454)\u001b[0m 2022-02-19 17:34:53,777\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1461)\u001b[0m 2022-02-19 17:34:53,802\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1472)\u001b[0m 2022-02-19 17:34:53,755\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1487)\u001b[0m 2022-02-19 17:34:53,763\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1481)\u001b[0m 2022-02-19 17:34:53,789\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1453)\u001b[0m 2022-02-19 17:34:53,803\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1508)\u001b[0m 2022-02-19 17:34:53,765\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1464)\u001b[0m 2022-02-19 17:34:53,845\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1466)\u001b[0m 2022-02-19 17:34:53,789\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1460)\u001b[0m 2022-02-19 17:34:53,846\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1463)\u001b[0m 2022-02-19 17:34:53,821\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1467)\u001b[0m 2022-02-19 17:34:53,871\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1458)\u001b[0m 2022-02-19 17:34:53,881\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1482)\u001b[0m 2022-02-19 17:34:53,867\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1483)\u001b[0m 2022-02-19 17:34:53,869\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "APEX_DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "739c2894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b668dd2c370>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('checkpoints/ppo_disc_r2', 'rb') as f:\n",
    "    l = pickle.load(f)\n",
    "    \n",
    "plt.plot(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e25b554a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 14:56:20,179\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-02-24 14:56:20,180\tINFO trainer.py:790 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "number of waypoints:79\n",
      "using map:./f1tenth_gym/examples/hard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 14:56:20,566\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "2022-02-24 14:56:20,568\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-02-24 14:56:20,618\tINFO trainable.py:472 -- Restored on 128.205.39.153 from checkpoint: ./checkpoints/ppo_disc_hard_cp2/checkpoint_002701/checkpoint-2701\n",
      "2022-02-24 14:56:20,618\tINFO trainable.py:480 -- Current state after restoring: {'_iteration': 2701, '_timesteps_total': 21553980, '_time_total': 58813.86238908768, '_episodes_total': 9190}\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.agents import ppo, sac, ddpg, dqn\n",
    "from ray.rllib.agents.dqn.apex import APEX_DEFAULT_CONFIG\n",
    "from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG\n",
    "\n",
    "DEFAULT_CONFIG['framework'] = 'torch'\n",
    "DEFAULT_CONFIG['num_workers'] = 0\n",
    "DEFAULT_CONFIG['num_gpus'] = 0\n",
    "DEFAULT_CONFIG['model']['fcnet_hiddens'] = [1024, 1024]\n",
    "\n",
    "\n",
    "APEX_DEFAULT_CONFIG['framework'] = 'torch'\n",
    "APEX_DEFAULT_CONFIG['num_gpus'] = 0\n",
    "APEX_DEFAULT_CONFIG['num_workers'] = 0\n",
    "APEX_DEFAULT_CONFIG['exploration_config']['epsilon_timesteps'] = int(1e8)\n",
    "APEX_DEFAULT_CONFIG['model']['fcnet_hiddens'] = [1024, 1024]\n",
    "APEX_DEFAULT_CONFIG['final_epsilon'] = 0.1\n",
    "\n",
    "\n",
    "trainer = ppo.PPOTrainer(env=F110Env, config=DEFAULT_CONFIG)\n",
    "trainer.restore('./checkpoints/ppo_disc_hard_cp2/checkpoint_002701/checkpoint-2701')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bc61f0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "number of waypoints:79\n",
      "using map:./f1tenth_gym/examples/hard\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './f1tenth_gym/examples/hard.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mF110Env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexplore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mF110Env.__init__\u001b[0;34m(self, env_config)\u001b[0m\n\u001b[1;32m     69\u001b[0m     e\u001b[38;5;241m.\u001b[39mbottom \u001b[38;5;241m=\u001b[39m bottom \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m800\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing map:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconf\u001b[38;5;241m.\u001b[39mmap_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mf110_gym:f110-v0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_ext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_ext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_agents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39madd_render_callback(render_callback)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_capture_coord \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/genv/lib/python3.8/site-packages/gym/envs/registration.py:184\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake\u001b[39m(\u001b[38;5;28mid\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/genv/lib/python3.8/site-packages/gym/envs/registration.py:106\u001b[0m, in \u001b[0;36mEnvRegistry.make\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaking new env: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, path)\n\u001b[1;32m    105\u001b[0m spec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspec(path)\n\u001b[0;32m--> 106\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# We used to have people override _reset/_step rather than\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# reset/step. Set _gym_disable_underscore_compat = True on\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# your environment if you use these methods and don't want\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# compatibility code to be invoked.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_step\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_gym_disable_underscore_compat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    115\u001b[0m ):\n",
      "File \u001b[0;32m~/venvs/genv/lib/python3.8/site-packages/gym/envs/registration.py:76\u001b[0m, in \u001b[0;36mEnvSpec.make\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentry_point)\n\u001b[0;32m---> 76\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Make the environment aware of which spec it came from.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m spec \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/f1tenth_rl/f1tenth_gym/gym/f110_gym/envs/f110_env.py:179\u001b[0m, in \u001b[0;36mF110Env.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# initiate stuff\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim \u001b[38;5;241m=\u001b[39m Simulator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_agents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_map\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_ext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# stateful observations for rendering\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/f1tenth_rl/f1tenth_gym/gym/f110_gym/envs/base_classes.py:402\u001b[0m, in \u001b[0;36mSimulator.set_map\u001b[0;34m(self, map_path, map_ext)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03mSets the map of the environment and sets the map for scan simulator of each agent\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m    None\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents:\n\u001b[0;32m--> 402\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_ext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/f1tenth_rl/f1tenth_gym/gym/f110_gym/envs/base_classes.py:169\u001b[0m, in \u001b[0;36mRaceCar.set_map\u001b[0;34m(self, map_path, map_ext)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_map\u001b[39m(\u001b[38;5;28mself\u001b[39m, map_path, map_ext):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    Sets the map for scan simulator\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m        map_ext (str): extension of the map image file\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m     \u001b[43mRaceCar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_simulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_ext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/f1tenth_rl/f1tenth_gym/gym/f110_gym/envs/laser_models.py:398\u001b[0m, in \u001b[0;36mScanSimulator2D.set_map\u001b[0;34m(self, map_path, map_ext)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# TODO: do we open the option to flip the images, and turn rgb into grayscale? or specify the exact requirements in documentation.\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# TODO: throw error if image specification isn't met\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# load map image\u001b[39;00m\n\u001b[1;32m    397\u001b[0m map_img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(map_path)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m map_ext\n\u001b[0;32m--> 398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap_img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap_img_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(Image\u001b[38;5;241m.\u001b[39mFLIP_TOP_BOTTOM))\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap_img\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# grayscale -> binary\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/genv/lib/python3.8/site-packages/PIL/Image.py:2953\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2950\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   2952\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 2953\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2954\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2956\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './f1tenth_gym/examples/hard.png'"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "env = F110Env({'explore':False})\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = trainer.compute_action(obs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a46ef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "number of waypoints:79\n",
      "using map:./f1tenth_gym/examples/hard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0224 14:46:47.268370986   13634 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 14:46:47.304396184   13634 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 14:46:47.368199447   13634 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 14:46:47.400014867   13634 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 14:46:47.421832661   13634 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 14:46:47.446290439   13634 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 14:46:47.623234275   13634 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 14:46:47.658036917   13634 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 14:46:47.689152184   13634 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 14:46:47.715443869   13634 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +4m31s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "env = F110Env({'explore':False})\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action, _, info = trainer.get_policy().compute_single_action(obs)\n",
    "    best_action = info['action_dist_inputs'].argmax()\n",
    "    obs, reward, done, _ = env.step(best_action)\n",
    "    env.render()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8d9a41",
   "metadata": {},
   "source": [
    "## discrete with generalization over maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b080f0",
   "metadata": {},
   "source": [
    "### environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f3bffd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of waypoints:79\n",
      "initializing theta between: 1.1 and 2.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.F110Env at 0x7f3f2dc1a5e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import yaml\n",
    "import gym\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "\n",
    "    \n",
    "class F110Env(gym.Env):\n",
    "    def __init__(self, env_config, deterministic=False, max_v=10.0, map_no=4):\n",
    "        \"\"\"\n",
    "        break: [0., 0.]\n",
    "        fast forward: [0., 5.]\n",
    "        fast left: [-pi/4, 5.]\n",
    "        fast right: [pi/4, 5]\n",
    "        slow left: [-pi/4, 2] #later\n",
    "        slow right: [pi/4, 2] \n",
    "        \n",
    "        \"\"\"\n",
    "        self.deterministic = deterministic\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(362,), dtype=np.float32)\n",
    "        self.min_cp_dist = 2.0\n",
    "        self.cp_reward = 1.0\n",
    "        self.max_v = max_v\n",
    "        self.map_no = map_no\n",
    "        \n",
    "        self.action_map = {\n",
    "            0: [0., 0.],\n",
    "            1: [0.0, self.max_v],\n",
    "            2: [np.pi/4, self.max_v],\n",
    "            3: [-np.pi/4, self.max_v],           \n",
    "        }\n",
    "        self.action_space = gym.spaces.Discrete(len(self.action_map),)\n",
    "        \n",
    "\n",
    "        with open('./f1tenth_gym/examples/config_example_map.yaml') as file:\n",
    "            conf_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        conf = Namespace(**conf_dict)\n",
    "        self.conf = conf\n",
    "        wps = np.loadtxt(conf.wpt_path, delimiter=conf.wpt_delim, skiprows=0)[:, 1:4]\n",
    "        idxs = [i%10 == 0 for i in range(len(wps))]\n",
    "        \n",
    "        self.min_x, self.max_x = np.min(wps[:,0]), np.max(wps[:, 0])\n",
    "        self.min_y, self.max_y = np.min(wps[:,1]), np.max(wps[:, 1])\n",
    "\n",
    "        self.checkpoints = wps[idxs]\n",
    "        print(f\"number of waypoints:{len(self.checkpoints)}\")        \n",
    "        \n",
    "        self.low_theta = 1.1\n",
    "        self.high_theta = 2.1\n",
    "        print(f\"initializing theta between: {self.low_theta} and {self.high_theta}\")\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "  \n",
    "    def reset(self):\n",
    "        def render_callback(env_renderer):\n",
    "            # custom extra drawing function\n",
    "            \n",
    "            e = env_renderer\n",
    "\n",
    "            # update camera to follow car\n",
    "            x = e.cars[0].vertices[::2]\n",
    "            y = e.cars[0].vertices[1::2]\n",
    "            top, bottom, left, right = max(y), min(y), min(x), max(x)\n",
    "            e.score_label.x = left\n",
    "            e.score_label.y = top - 700\n",
    "            e.left = left - 800\n",
    "            e.right = right + 800\n",
    "            e.top = top + 800\n",
    "            e.bottom = bottom - 800\n",
    "            \n",
    "        map_no = np.random.randint(1, 5) if self.map_no is None else self.map_no\n",
    "        map_path = f\"./f1tenth_gym/examples/{map_no}\"\n",
    "        \n",
    "        if self.deterministic:\n",
    "            print(f\"using map:{map_path}\")\n",
    "            \n",
    "        self.env = gym.make('f110_gym:f110-v0', map=map_path, map_ext=self.conf.map_ext, num_agents=1)\n",
    "        self.env.add_render_callback(render_callback)\n",
    "        \n",
    "        #theta = np.random.rand() * (self.high_theta - self.low_theta) + self.low_theta\n",
    "        \n",
    "        random_idx = np.random.randint(0, len(self.checkpoints)-1)\n",
    "        current_pos = self.checkpoints[random_idx][:2]\n",
    "        theta = self.checkpoints[random_idx][2] + np.pi/2\n",
    "        obs, step_reward, done, info = self.env.reset(np.array([[current_pos[0], current_pos[1], theta]]))\n",
    "        #obs, step_reward, done, info = self.env.reset(np.array([[self.conf.sx, self.conf.sy, theta]]))\n",
    " \n",
    "        self.next_cp_idx = random_idx + 1\n",
    "        #self.next_cp_idx = 1\n",
    "    \n",
    "        self.t = 0\n",
    "        return self.to_vector_state(obs)\n",
    "    \n",
    "    def to_vector_state(self, obs):\n",
    "        scanner = obs['scans'][0]\n",
    "        \n",
    "        buck = 3\n",
    "        size = 1080//buck\n",
    "        scanner = np.zeros(size,)\n",
    "        for i in range(size):\n",
    "            scanner[i] = np.clip(np.mean(obs['scans'][0][i*buck: i*buck+buck]), 0, 10)\n",
    "        \n",
    "        scanner /= 10.0\n",
    "        \n",
    "        state = np.concatenate([\n",
    "            scanner,\n",
    "            np.array(obs['linear_vels_x'][:1])/self.max_v,\n",
    "            np.array(obs['ang_vels_z'][:1])/2,\n",
    "        ])\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def checkpoint(self, position):\n",
    "        dist = np.linalg.norm(position - self.checkpoints[self.next_cp_idx][:2])\n",
    "        reward = 0\n",
    "        if dist < self.min_cp_dist:\n",
    "            reward = self.cp_reward\n",
    "    \n",
    "            self.next_cp_idx = (self.next_cp_idx + 1)%len(self.checkpoints)\n",
    "        return reward\n",
    "        \n",
    "    def step(self, action):\n",
    "\n",
    "        act = np.array([self.action_map[action]])\n",
    "        \n",
    "#         if not self.deterministic:\n",
    "#             act[0][0] += np.random.normal(0, 0.05)\n",
    "#             act[0][1] += np.random.normal(0, 2.0)\n",
    "        \n",
    "        obs, step_reward, done, info = self.env.step(act)\n",
    "        pose_x = obs['poses_x'][0]\n",
    "        pose_y = obs['poses_y'][0]\n",
    "\n",
    "        reward = 0\n",
    "        \n",
    "#         if obs['lap_counts'][0] == 1.0:\n",
    "#             reward = 1.0\n",
    "#             done = True\n",
    "        \n",
    "        position = np.array([pose_x, pose_y])\n",
    "        \n",
    "        if obs['collisions'][0] == 1.0:\n",
    "            reward = -1\n",
    "            done = True\n",
    "            \n",
    "        scans = obs['scans'][0]\n",
    "        if min(scans) < 0.1:\n",
    "            reward -= 0.1    \n",
    "        \n",
    "        cp_reward = self.checkpoint(position)\n",
    "        next_state = self.to_vector_state(obs)\n",
    "        reward += cp_reward\n",
    "        self.t += 1\n",
    "\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "F110Env({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f42bc18",
   "metadata": {},
   "source": [
    "### PPO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce56b46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-11 13:32:06,153\tWARNING ppo.py:223 -- `train_batch_size` (5000) cannot be achieved with your other settings (num_workers=10 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 500.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=506)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=506)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=531)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=531)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=516)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=516)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=519)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=519)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=525)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=525)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=535)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=535)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=533)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=533)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=529)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=529)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=530)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=530)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=518)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=518)\u001b[0m initializing theta between: 1.1 and 2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=533)\u001b[0m 2022-03-11 13:32:20,007\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=519)\u001b[0m 2022-03-11 13:32:20,139\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=506)\u001b[0m 2022-03-11 13:32:20,428\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=525)\u001b[0m 2022-03-11 13:32:20,528\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=516)\u001b[0m 2022-03-11 13:32:20,561\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=518)\u001b[0m 2022-03-11 13:32:20,632\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "2022-03-11 13:32:20,739\tINFO trainable.py:125 -- Trainable.setup took 14.587 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-03-11 13:32:20,740\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=531)\u001b[0m 2022-03-11 13:32:20,637\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=535)\u001b[0m 2022-03-11 13:32:20,641\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=529)\u001b[0m 2022-03-11 13:32:20,649\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=530)\u001b[0m 2022-03-11 13:32:20,684\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "2022-03-11 13:32:20,762\tINFO trainable.py:472 -- Restored on 10.64.91.46 from checkpoint: ./checkpoints/safe_ppo/checkpoint_000101/checkpoint-101\n",
      "2022-03-11 13:32:20,763\tINFO trainable.py:480 -- Current state after restoring: {'_iteration': 101, '_timesteps_total': 1980662, '_time_total': 7113.529857873917, '_episodes_total': 1311}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINE TUNING\n",
      "{'num_workers': 10, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 500, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 5000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [200, 200], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'F110Env', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 10, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 500, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 5000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [200, 200], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'F110Env', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1.0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.1, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.agents.ppo.ppo_torch_policy.PPOTorchPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1.0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.1, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.agents.ppo.ppo_torch_policy.PPOTorchPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward:10.166666666666666\n",
      "checkpoint saved at ./checkpoints/safe_ppo_ft/checkpoint_000102/checkpoint-102\n",
      "episode: 1 reward:15.785714285714286\n",
      "episode: 2 reward:18.333333333333332\n",
      "episode: 3 reward:21.161290322580644\n",
      "episode: 4 reward:22.105263157894736\n",
      "episode: 5 reward:22.52173913043478\n",
      "episode: 6 reward:23.346153846153847\n",
      "episode: 7 reward:23.625\n",
      "episode: 8 reward:24.9672131147541\n",
      "episode: 9 reward:26.159420289855074\n",
      "episode: 10 reward:25.613333333333333\n",
      "checkpoint saved at ./checkpoints/safe_ppo_ft/checkpoint_000112/checkpoint-112\n",
      "episode: 11 reward:26.759493670886076\n",
      "episode: 12 reward:26.41176470588235\n",
      "episode: 13 reward:27.747252747252748\n",
      "episode: 14 reward:27.96842105263158\n",
      "episode: 15 reward:28.53\n",
      "episode: 16 reward:29.87\n",
      "episode: 17 reward:30.46\n",
      "episode: 18 reward:31.49\n",
      "episode: 19 reward:31.81\n",
      "episode: 20 reward:31.8\n",
      "checkpoint saved at ./checkpoints/safe_ppo_ft/checkpoint_000122/checkpoint-122\n",
      "episode: 21 reward:32.39\n",
      "episode: 22 reward:33.15\n",
      "episode: 23 reward:33.3\n",
      "episode: 24 reward:34.72\n",
      "episode: 25 reward:35.57\n",
      "episode: 26 reward:35.07\n",
      "episode: 27 reward:35.33\n",
      "episode: 28 reward:35.17\n",
      "episode: 29 reward:35.51\n",
      "episode: 30 reward:35.99\n",
      "checkpoint saved at ./checkpoints/safe_ppo_ft/checkpoint_000132/checkpoint-132\n",
      "episode: 31 reward:35.94\n",
      "episode: 32 reward:36.09\n",
      "episode: 33 reward:35.32\n",
      "episode: 34 reward:36.03\n",
      "episode: 35 reward:35.87\n",
      "episode: 36 reward:36.41\n",
      "episode: 37 reward:36.15\n",
      "episode: 38 reward:35.8\n",
      "episode: 39 reward:33.65\n",
      "episode: 40 reward:31.84\n",
      "checkpoint saved at ./checkpoints/safe_ppo_ft/checkpoint_000142/checkpoint-142\n",
      "episode: 41 reward:32.18\n",
      "episode: 42 reward:31.32\n",
      "episode: 43 reward:31.81\n",
      "episode: 44 reward:31.8\n",
      "episode: 45 reward:30.23\n",
      "episode: 46 reward:29.14\n",
      "episode: 47 reward:29.16\n",
      "episode: 48 reward:28.19\n",
      "episode: 49 reward:27.47\n",
      "episode: 50 reward:26.84\n",
      "checkpoint saved at ./checkpoints/safe_ppo_ft/checkpoint_000152/checkpoint-152\n",
      "episode: 51 reward:26.62\n",
      "episode: 52 reward:27.15\n",
      "episode: 53 reward:27.54\n",
      "episode: 54 reward:27.78\n",
      "episode: 55 reward:27.3\n",
      "episode: 56 reward:27.43\n",
      "episode: 57 reward:27.59\n",
      "episode: 58 reward:28.15\n",
      "episode: 59 reward:27.71\n",
      "episode: 60 reward:28.5\n",
      "episode: 61 reward:27.87\n",
      "episode: 62 reward:28.15\n",
      "episode: 63 reward:27.85\n",
      "episode: 64 reward:28.32\n",
      "episode: 65 reward:26.78\n",
      "episode: 66 reward:26.78\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.agents import ppo, sac, ddpg, dqn\n",
    "from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "\n",
    "DEFAULT_CONFIG['framework'] = 'torch'\n",
    "DEFAULT_CONFIG['num_workers'] = 10\n",
    "DEFAULT_CONFIG['num_gpus'] = 1.0\n",
    "DEFAULT_CONFIG['num_gpus_per_worker'] = 1/10\n",
    "DEFAULT_CONFIG['model']['fcnet_hiddens'] = [200, 200]\n",
    "DEFAULT_CONFIG['entropy_coeff'] = 0.0\n",
    "DEFAULT_CONFIG['clip_param'] = 0.2\n",
    "DEFAULT_CONFIG['train_batch_size'] = 5000\n",
    "DEFAULT_CONFIG['batch_mode'] = 'truncate_episodes'\n",
    "# DEFAULT_CONFIG['model']['use_lstm'] = True\n",
    "# DEFAULT_CONFIG['model']['lstm_use_prev_action'] = True\n",
    "# DEFAULT_CONFIG['model']['max_seq_len'] = 10\n",
    "# DEFAULT_CONFIG['lr'] = 5e-5\n",
    "\n",
    "trainer = ppo.PPOTrainer(env=F110Env, config=DEFAULT_CONFIG)\n",
    "print(\"FINE TUNING\")\n",
    "trainer.restore('./checkpoints/safe_ppo/checkpoint_000101/checkpoint-101')\n",
    "\n",
    "print(trainer.config)\n",
    "rewards = []\n",
    "\n",
    "import pickle\n",
    "\n",
    "for i in range(1000):\n",
    "    result = trainer.train()\n",
    "    print(f\"episode: {i} reward:{result['episode_reward_mean']}\")\n",
    "    rewards.append(result['episode_reward_mean'])\n",
    "    if i%50 == 0 or (i<50 and i%10==0):\n",
    "        with open('./checkpoints/safe_ppo_ft_r', 'wb') as f:\n",
    "            pickle.dump(rewards, f)\n",
    "        cp = trainer.save(\"./checkpoints/safe_ppo_ft\")\n",
    "        print(\"checkpoint saved at\", cp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72d30681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./checkpoints/ppo_gen3/checkpoint_000040/checkpoint-40'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entropy can help I guess!\n",
    "# try attention too\n",
    "# padding punishment\n",
    "\n",
    "#trainer.save('./checkpoints/ppo_gen3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8695a32",
   "metadata": {},
   "source": [
    "### rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f95d578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f249ff4e7c0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "with open('checkpoints/ppo_gen1_r', 'rb') as f:\n",
    "    rewards = pickle.load(f)\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7970a583",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ppo, sac, ddpg, dqn\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mppo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mppo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_CONFIG\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ray'"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.agents import ppo, sac, ddpg, dqn\n",
    "from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG\n",
    "\n",
    "DEFAULT_CONFIG['framework'] = 'torch'\n",
    "DEFAULT_CONFIG['num_workers'] = 0\n",
    "DEFAULT_CONFIG['num_gpus'] = 0\n",
    "DEFAULT_CONFIG['model']['fcnet_hiddens'] = [200, 200]\n",
    "DEFAULT_CONFIG[\"evaluation_num_workers\"] = 0\n",
    "DEFAULT_CONFIG[\"evaluation_config\"][\"render_env\"] = False\n",
    "DEFAULT_CONFIG[\"evaluation_config\"][\"explore\"] = False\n",
    "DEFAULT_CONFIG['explore'] = False\n",
    "\n",
    "trainer = ppo.PPOTrainer(env=F110Env, config=DEFAULT_CONFIG)\n",
    "trainer.restore('./checkpoints/safe_ppo_ft/checkpoint_000302/checkpoint-302')\n",
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc74a078",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of waypoints:79\n",
      "initializing theta between: 1.1 and 2.1\n",
      "using map:./f1tenth_gym/examples/4\n",
      "using map:./f1tenth_gym/examples/4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "done = False\n",
    "env = F110Env({'explore':False}, deterministic=True, map_no=4, max_v=7)\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = trainer.compute_single_action(obs, explore=False)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c9d97f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f26074cd",
   "metadata": {},
   "source": [
    "### deterministic action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0d09dcb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of waypoints:79\n",
      "initializing theta between: 1.1 and 2.1\n",
      "using map:./f1tenth_gym/examples/4\n",
      "using map:./f1tenth_gym/examples/4\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "env = F110Env({'explore':False}, deterministic=True, map_no=4, max_v=10)\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action, _, info = trainer.get_policy().compute_single_action(obs)\n",
    "    best_action = info['action_dist_inputs'].argmax()\n",
    "    obs, reward, done, _ = env.step(best_action)\n",
    "    env.render()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c1263",
   "metadata": {},
   "source": [
    "### APEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ad184",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 14:39:06,824\tINFO simple_q.py:153 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n",
      "2022-02-25 14:39:06,825\tINFO trainer.py:790 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=13839)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13839)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13853)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13853)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13827)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13827)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13824)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13824)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13856)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13856)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13834)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13834)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13844)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13844)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13847)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13847)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13832)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13832)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13871)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13871)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13874)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13874)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13823)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13823)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13820)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13820)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13870)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13870)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13840)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13840)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13855)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13855)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13858)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13858)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13875)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13875)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13819)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13819)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13838)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13838)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13846)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13846)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13860)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13860)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13817)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13817)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13845)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13845)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13851)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13851)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13852)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13852)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13828)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13828)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13857)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13857)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13843)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13843)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13854)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13854)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13863)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13863)\u001b[0m initializing theta between: 1.1 and 2.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13862)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13862)\u001b[0m initializing theta between: 1.1 and 2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=13839)\u001b[0m 2022-02-25 14:39:19,608\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13824)\u001b[0m 2022-02-25 14:39:19,959\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13827)\u001b[0m 2022-02-25 14:39:21,249\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13853)\u001b[0m 2022-02-25 14:39:21,551\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13844)\u001b[0m 2022-02-25 14:39:21,594\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13834)\u001b[0m 2022-02-25 14:39:21,547\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13856)\u001b[0m 2022-02-25 14:39:21,865\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13847)\u001b[0m 2022-02-25 14:39:21,913\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13832)\u001b[0m 2022-02-25 14:39:21,987\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13871)\u001b[0m 2022-02-25 14:39:21,979\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13874)\u001b[0m 2022-02-25 14:39:21,938\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13823)\u001b[0m 2022-02-25 14:39:22,019\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13820)\u001b[0m 2022-02-25 14:39:21,984\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13870)\u001b[0m 2022-02-25 14:39:22,050\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13840)\u001b[0m 2022-02-25 14:39:22,096\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13855)\u001b[0m 2022-02-25 14:39:22,118\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13875)\u001b[0m 2022-02-25 14:39:22,031\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13858)\u001b[0m 2022-02-25 14:39:22,143\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13838)\u001b[0m 2022-02-25 14:39:22,178\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13846)\u001b[0m 2022-02-25 14:39:22,232\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13819)\u001b[0m 2022-02-25 14:39:22,273\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13817)\u001b[0m 2022-02-25 14:39:22,250\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13860)\u001b[0m 2022-02-25 14:39:22,566\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13854)\u001b[0m 2022-02-25 14:39:22,567\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13845)\u001b[0m 2022-02-25 14:39:22,569\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13851)\u001b[0m 2022-02-25 14:39:22,569\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13828)\u001b[0m 2022-02-25 14:39:22,570\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13852)\u001b[0m 2022-02-25 14:39:22,566\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13857)\u001b[0m 2022-02-25 14:39:22,565\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13863)\u001b[0m 2022-02-25 14:39:22,568\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13843)\u001b[0m 2022-02-25 14:39:22,575\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13862)\u001b[0m 2022-02-25 14:39:22,572\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 14:39:32,467\tINFO trainable.py:125 -- Trainable.setup took 25.645 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-02-25 14:39:32,470\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(MultiAgentReplayBuffer pid=13842)\u001b[0m 2022-02-25 14:39:34,250\tINFO replay_buffer.py:41 -- Estimated max memory usage for replay buffer is 1.4705 GB (500000.0 batches of size 1, 2941 bytes each), available system memory is 540.16866304 GB\n",
      "\u001b[2m\u001b[36m(MultiAgentReplayBuffer pid=13848)\u001b[0m 2022-02-25 14:39:34,353\tINFO replay_buffer.py:41 -- Estimated max memory usage for replay buffer is 1.4705 GB (500000.0 batches of size 1, 2941 bytes each), available system memory is 540.16866304 GB\n",
      "\u001b[2m\u001b[36m(MultiAgentReplayBuffer pid=13849)\u001b[0m 2022-02-25 14:39:34,807\tINFO replay_buffer.py:41 -- Estimated max memory usage for replay buffer is 1.4705 GB (500000.0 batches of size 1, 2941 bytes each), available system memory is 540.16866304 GB\n",
      "\u001b[2m\u001b[36m(MultiAgentReplayBuffer pid=13876)\u001b[0m 2022-02-25 14:39:50,264\tINFO replay_buffer.py:41 -- Estimated max memory usage for replay buffer is 1.4705 GB (500000.0 batches of size 1, 2941 bytes each), available system memory is 540.16866304 GB\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.agents import ppo, sac, ddpg, dqn\n",
    "from ray.rllib.agents.dqn.apex import APEX_DEFAULT_CONFIG\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "\n",
    "APEX_DEFAULT_CONFIG['framework'] = 'torch'\n",
    "APEX_DEFAULT_CONFIG['num_gpus'] = 1\n",
    "APEX_DEFAULT_CONFIG['num_workers'] = 32\n",
    "APEX_DEFAULT_CONFIG['rollout_fragment_length'] = 100\n",
    "APEX_DEFAULT_CONFIG['exploration_config']['epsilon_timesteps'] = int(1e7)\n",
    "APEX_DEFAULT_CONFIG['model']['fcnet_hiddens'] = [512, 512]\n",
    "# APEX_DEFAULT_CONFIG['num_gpus_per_worker'] = 1/15\n",
    "# APEX_DEFAULT_CONFIG['final_epsilon'] = 0.05\n",
    "APEX_DEFAULT_CONFIG['target_network_update_freq'] = 10000\n",
    "APEX_DEFAULT_CONFIG['train_batch_size'] = 256\n",
    "APEX_DEFAULT_CONFIG['batch_mode'] = 'complete_episodes'\n",
    "\n",
    "trainer = dqn.ApexTrainer(env=F110Env, config=APEX_DEFAULT_CONFIG)\n",
    "rewards = []\n",
    "\n",
    "import pickle\n",
    "\n",
    "for i in range(10000):\n",
    "    result = trainer.train()\n",
    "    print(f\"episode: {i} reward:{result['episode_reward_mean']}\")\n",
    "    rewards.append(result['episode_reward_mean'])\n",
    "    if i%50 == 0 or (i<50 and i%10==0):\n",
    "        with open('./checkpoints/apex_gen2_r', 'wb') as f:\n",
    "            pickle.dump(rewards, f)\n",
    "        cp = trainer.save(\"./checkpoints/apex_gen2\")\n",
    "        print(\"checkpoint saved at\", cp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8af4029",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_workers': 32,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'create_env_on_driver': False,\n",
       " 'rollout_fragment_length': 50,\n",
       " 'batch_mode': 'truncate_episodes',\n",
       " 'gamma': 0.99,\n",
       " 'lr': 0.0005,\n",
       " 'train_batch_size': 512,\n",
       " 'model': {'_use_default_native_models': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  'fcnet_hiddens': [512, 512],\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'post_fcnet_hiddens': [],\n",
       "  'post_fcnet_activation': 'relu',\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': True,\n",
       "  'use_lstm': False,\n",
       "  'max_seq_len': 20,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action': False,\n",
       "  'lstm_use_prev_reward': False,\n",
       "  '_time_major': False,\n",
       "  'use_attention': False,\n",
       "  'attention_num_transformer_units': 1,\n",
       "  'attention_dim': 64,\n",
       "  'attention_num_heads': 1,\n",
       "  'attention_head_dim': 32,\n",
       "  'attention_memory_inference': 50,\n",
       "  'attention_memory_training': 50,\n",
       "  'attention_position_wise_mlp_dim': 32,\n",
       "  'attention_init_gru_gate_bias': 2.0,\n",
       "  'attention_use_n_prev_actions': 0,\n",
       "  'attention_use_n_prev_rewards': 0,\n",
       "  'framestack': True,\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None,\n",
       "  'lstm_use_prev_action_reward': -1},\n",
       " 'optimizer': {'max_weight_sync_delay': 400,\n",
       "  'num_replay_buffer_shards': 4,\n",
       "  'debug': False},\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'env': None,\n",
       " 'observation_space': None,\n",
       " 'action_space': None,\n",
       " 'env_config': {},\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'env_task_fn': None,\n",
       " 'render_env': False,\n",
       " 'record_env': False,\n",
       " 'clip_rewards': None,\n",
       " 'normalize_actions': True,\n",
       " 'clip_actions': False,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'log_level': 'WARN',\n",
       " 'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       " 'ignore_worker_failures': False,\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'framework': 'torch',\n",
       " 'eager_tracing': False,\n",
       " 'eager_max_retraces': 20,\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'PerWorkerEpsilonGreedy',\n",
       "  'initial_epsilon': 1.0,\n",
       "  'final_epsilon': 0.02,\n",
       "  'epsilon_timesteps': 10000},\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_duration': 10,\n",
       " 'evaluation_duration_unit': 'episodes',\n",
       " 'evaluation_parallel_to_training': False,\n",
       " 'in_evaluation': False,\n",
       " 'evaluation_config': {'explore': False},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'custom_eval_function': None,\n",
       " 'always_attach_evaluation_results': False,\n",
       " 'sample_async': False,\n",
       " 'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'compress_observations': False,\n",
       " 'metrics_episode_collection_timeout_s': 180,\n",
       " 'metrics_num_episodes_for_smoothing': 100,\n",
       " 'min_time_s_per_reporting': None,\n",
       " 'min_train_timesteps_per_reporting': None,\n",
       " 'min_sample_timesteps_per_reporting': None,\n",
       " 'seed': None,\n",
       " 'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_gpus': 1,\n",
       " '_fake_gpus': False,\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'num_cpus_for_driver': 1,\n",
       " 'placement_strategy': 'PACK',\n",
       " 'input': 'sampler',\n",
       " 'input_config': {},\n",
       " 'actions_in_input_normalized': False,\n",
       " 'input_evaluation': ['is', 'wis'],\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'multiagent': {'policies': {},\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_map_cache': None,\n",
       "  'policy_mapping_fn': None,\n",
       "  'policies_to_train': None,\n",
       "  'observation_fn': None,\n",
       "  'replay_mode': 'independent',\n",
       "  'count_steps_by': 'env_steps'},\n",
       " 'logger_config': None,\n",
       " '_tf_policy_handles_more_than_one_loss': False,\n",
       " '_disable_preprocessor_api': False,\n",
       " '_disable_action_flattening': False,\n",
       " '_disable_execution_plan_api': False,\n",
       " 'simple_optimizer': -1,\n",
       " 'monitor': -1,\n",
       " 'evaluation_num_episodes': -1,\n",
       " 'metrics_smoothing_episodes': -1,\n",
       " 'timesteps_per_iteration': 25000,\n",
       " 'min_iter_time_s': 30,\n",
       " 'collect_metrics_timeout': -1,\n",
       " 'target_network_update_freq': 500000,\n",
       " 'buffer_size': 2000000,\n",
       " 'replay_buffer_config': None,\n",
       " 'store_buffer_in_checkpoints': False,\n",
       " 'replay_sequence_length': 1,\n",
       " 'lr_schedule': None,\n",
       " 'adam_epsilon': 1e-08,\n",
       " 'grad_clip': 40,\n",
       " 'learning_starts': 50000,\n",
       " 'num_atoms': 1,\n",
       " 'v_min': -10.0,\n",
       " 'v_max': 10.0,\n",
       " 'noisy': False,\n",
       " 'sigma0': 0.5,\n",
       " 'dueling': True,\n",
       " 'hiddens': [256],\n",
       " 'double_q': True,\n",
       " 'n_step': 3,\n",
       " 'prioritized_replay': True,\n",
       " 'prioritized_replay_alpha': 0.6,\n",
       " 'prioritized_replay_beta': 0.4,\n",
       " 'final_prioritized_replay_beta': 0.4,\n",
       " 'prioritized_replay_beta_annealing_timesteps': 20000,\n",
       " 'prioritized_replay_eps': 1e-06,\n",
       " 'before_learn_on_batch': None,\n",
       " 'training_intensity': None,\n",
       " 'worker_side_prioritization': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APEX_DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311cca35",
   "metadata": {},
   "source": [
    "### rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e19c349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of waypoints:79\n",
      "initializing theta between: 1.1 and 2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 17:31:41,328\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "Install gputil for GPU system monitoring.\n",
      "E0224 17:31:42.566541816   23712 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 17:31:42.587183151   23712 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 17:31:42.603914247   23712 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "2022-02-24 17:31:43,939\tINFO trainable.py:472 -- Restored on 128.205.39.153 from checkpoint: ./checkpoints/ppo_gen1/checkpoint_000101/checkpoint-101\n",
      "2022-02-24 17:31:43,940\tINFO trainable.py:480 -- Current state after restoring: {'_iteration': 101, '_timesteps_total': 404000, '_time_total': 1193.5172264575958, '_episodes_total': 568}\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.agents import ppo, sac, ddpg, dqn\n",
    "from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG\n",
    "\n",
    "DEFAULT_CONFIG['framework'] = 'torch'\n",
    "DEFAULT_CONFIG['num_workers'] = 0\n",
    "DEFAULT_CONFIG['num_gpus'] = 0\n",
    "DEFAULT_CONFIG['model']['fcnet_hiddens'] = [512, 512]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer = ppo.PPOTrainer(env=F110Env, config=DEFAULT_CONFIG)\n",
    "trainer.restore('./checkpoints/ppo_gen1/checkpoint_000101/checkpoint-101')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde754a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of waypoints:79\n",
      "initializing theta between: 1.1 and 2.1\n",
      "using map:./f1tenth_gym/examples/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 17:28:09,432\tWARNING deprecation.py:45 -- DeprecationWarning: `compute_action` has been deprecated. Use `Trainer.compute_single_action()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using map:./f1tenth_gym/examples/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0224 17:28:09.464016808   23296 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 17:28:09.486268041   23296 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 17:28:09.552270206   23296 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 17:28:09.586315409   23296 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 17:28:09.611037816   23296 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 17:28:09.636488141   23296 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 17:28:09.956462774   23296 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 17:28:09.986253508   23296 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 17:28:10.016623028   23296 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 17:28:10.047656991   23296 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "done = False\n",
    "env = F110Env({'explore':False}, deterministic=True)\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = trainer.compute_action(obs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62905a64",
   "metadata": {},
   "source": [
    "### deterministic action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7508c18a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of waypoints:79\n",
      "initializing theta between: 1.1 and 2.1\n",
      "using map:./f1tenth_gym/examples/4\n",
      "using map:./f1tenth_gym/examples/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0224 17:31:44.364789236   23712 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 17:31:44.387505847   23712 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 17:31:44.458213310   23712 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 17:31:44.492264741   23712 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 17:31:44.517877649   23712 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 17:31:44.541289314   23712 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 17:31:44.732439977   23712 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 17:31:44.764952154   23712 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 17:31:44.799374189   23712 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0224 17:31:44.830534369   23712 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "env = F110Env({'explore':False}, deterministic=True)\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action, _, info = trainer.get_policy().compute_single_action(obs)\n",
    "    best_action = info['action_dist_inputs'].argmax()\n",
    "    obs, reward, done, _ = env.step(best_action)\n",
    "    env.render()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd7fb19",
   "metadata": {},
   "source": [
    "## Racing environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a00ed2",
   "metadata": {},
   "source": [
    "### follow the gap agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2af5032",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGAgent:\n",
    "    def __init__(self, bubble_r=0.5, lookahead_degrees=[90., 270.], speed=2.0):\n",
    "        self.bubble_r = bubble_r\n",
    "        self.lookahead_degrees = lookahead_degrees\n",
    "        self.speed = speed\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        ranges = state[:360]\n",
    "        #print(ranges[10], ranges[90], ranges[180], ranges[270], ranges[350])\n",
    "        beg = int(self.lookahead_degrees[0])\n",
    "        end = int(self.lookahead_degrees[1])\n",
    "\n",
    "        ranges = ranges[beg:end]\n",
    "\n",
    "        ranges = np.clip(ranges, 0, 10.0)\n",
    "        \n",
    "        nearest_point = np.min(ranges)\n",
    "        min_dist = nearest_point + self.bubble_r\n",
    "\n",
    "        ranges = np.where(ranges <= min_dist, 0.0, ranges)\n",
    "\n",
    "        gaps = []\n",
    "        if ranges[0] != 0.0:\n",
    "            gaps.append(0)\n",
    "\n",
    "        i = 0\n",
    "        while i < len(ranges):\n",
    "            if ranges[i] == 0.0:\n",
    "                if i > 0:\n",
    "                    gaps.append(i-1)\n",
    "                while i < len(ranges) and ranges[i] == 0.0:\n",
    "                    i += 1\n",
    "                if i < len(ranges):\n",
    "                    gaps.append(i)\n",
    "                continue\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        if ranges[-1] != 0.0:\n",
    "            gaps.append(len(ranges) - 1)\n",
    "\n",
    "        assert len(gaps) % 2 == 0\n",
    "\n",
    "        max_gap = -1\n",
    "        gap_beg = 0\n",
    "        gap_end = 360\n",
    "\n",
    "        # find max gap\n",
    "        i = 0\n",
    "        while i < len(gaps):\n",
    "            if gaps[i+1]-gaps[i] > max_gap:\n",
    "                max_gap = gaps[i+1]-gaps[i]\n",
    "                gap_beg = gaps[i]+beg\n",
    "                gap_end = gaps[i+1]+beg\n",
    "            i += 2\n",
    "\n",
    "        mid_point = float(gap_end+gap_beg)/2.\n",
    "        angle_deg = mid_point - 180\n",
    "        angle_rad = angle_deg * (np.pi/180.)\n",
    "#         print(f\"midpoint:{mid_point}\")\n",
    "        angle = angle_rad * 1.0\n",
    "        return [angle, self.speed]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840352c2",
   "metadata": {},
   "source": [
    "###  classic control env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e23a40ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of waypoints:79\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VFHEnv at 0x7f0e201ef130>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import yaml\n",
    "import gym\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "\n",
    "    \n",
    "class VFHEnv(gym.Env):\n",
    "    def __init__(self, env_config, deterministic=False, max_v=10, map_no=4, begin_pos=None):\n",
    "        \"\"\"\n",
    "        break: [0., 0.]\n",
    "        fast forward: [0., 5.]\n",
    "        fast left: [-pi/4, 5.]\n",
    "        fast right: [pi/4, 5]\n",
    "        slow left: [-pi/4, 2] #later\n",
    "        slow right: [pi/4, 2] \n",
    "        \n",
    "        \"\"\"\n",
    "        self.deterministic = deterministic\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(362,), dtype=np.float32)\n",
    "        self.min_cp_dist = 2.0\n",
    "        self.cp_reward = 1.0\n",
    "        self.max_v = max_v\n",
    "        self.map_no = map_no\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "        self.begin_pos = begin_pos\n",
    "\n",
    "        with open('./f1tenth_gym/examples/config_example_map.yaml') as file:\n",
    "            conf_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        conf = Namespace(**conf_dict)\n",
    "        self.conf = conf\n",
    "        wps = np.loadtxt(conf.wpt_path, delimiter=conf.wpt_delim, skiprows=0)[:, 1:4]\n",
    "        idxs = [i%10 == 0 for i in range(len(wps))]\n",
    "        \n",
    "        self.min_x, self.max_x = np.min(wps[:,0]), np.max(wps[:, 0])\n",
    "        self.min_y, self.max_y = np.min(wps[:,1]), np.max(wps[:, 1])\n",
    "\n",
    "        self.checkpoints = wps[idxs]\n",
    "        print(f\"number of waypoints:{len(self.checkpoints)}\")        \n",
    "        \n",
    "#         self.low_theta = 1.1\n",
    "#         self.high_theta = 2.1\n",
    "#         print(f\"initializing theta between: {self.low_theta} and {self.high_theta}\")\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "  \n",
    "    def reset(self):\n",
    "        def render_callback(env_renderer):\n",
    "            # custom extra drawing function\n",
    "            \n",
    "            e = env_renderer\n",
    "\n",
    "            # update camera to follow car\n",
    "            x = e.cars[0].vertices[::2]\n",
    "            y = e.cars[0].vertices[1::2]\n",
    "            top, bottom, left, right = max(y), min(y), min(x), max(x)\n",
    "            e.score_label.x = left\n",
    "            e.score_label.y = top - 700\n",
    "            e.left = left - 800\n",
    "            e.right = right + 800\n",
    "            e.top = top + 800\n",
    "            e.bottom = bottom - 800\n",
    "            \n",
    "        map_no = np.random.randint(1, 5) if self.map_no is None else self.map_no\n",
    "        map_path = f\"./f1tenth_gym/examples/{map_no}\"\n",
    "        \n",
    "        if self.deterministic:\n",
    "            print(f\"using map:{map_path}\")\n",
    "            \n",
    "        self.env = gym.make('f110_gym:f110-v0', map=map_path, map_ext=self.conf.map_ext, num_agents=1)\n",
    "        self.env.add_render_callback(render_callback)\n",
    "        \n",
    "        #theta = np.random.rand() * (self.high_theta - self.low_theta) + self.low_theta\n",
    "        \n",
    "        random_idx = np.random.randint(0, len(self.checkpoints)-1)\n",
    "        current_pos = self.checkpoints[random_idx][:2]\n",
    "        theta = self.checkpoints[random_idx][2] + np.pi/2\n",
    "        begin_pos = [0, 0, 0] if self.begin_pos is None else self.begin_pos\n",
    "        obs, step_reward, done, info = self.env.reset(np.array([begin_pos]))\n",
    "        #obs, step_reward, done, info = self.env.reset(np.array([[self.conf.sx, self.conf.sy, theta]]))\n",
    " \n",
    "        self.next_cp_idx = random_idx + 1\n",
    "        #self.next_cp_idx = 1\n",
    "    \n",
    "        self.t = 0\n",
    "        return self.to_vector_state(obs)\n",
    "    \n",
    "    def to_vector_state(self, obs):\n",
    "        \n",
    "        scanner = np.clip(obs['scans'][0], 0, 10)\n",
    "                \n",
    "        state = [\n",
    "            scanner,\n",
    "            np.array(obs['linear_vels_x'][0]),\n",
    "            np.array(obs['ang_vels_z'][0]),\n",
    "            (obs['poses_x'][0], obs['poses_y'][0], obs['poses_theta'][0])\n",
    "        ]\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def checkpoint(self, position):\n",
    "        dist = np.linalg.norm(position - self.checkpoints[self.next_cp_idx][:2])\n",
    "        reward = 0\n",
    "        if dist < self.min_cp_dist:\n",
    "            reward = self.cp_reward\n",
    "    \n",
    "            self.next_cp_idx = (self.next_cp_idx + 1)%len(self.checkpoints)\n",
    "        return reward\n",
    "        \n",
    "    def step(self, action):\n",
    "        act = np.array([action])\n",
    "        \n",
    "        obs, step_reward, done, info = self.env.step(act)\n",
    "        pose_x = obs['poses_x'][0]\n",
    "        pose_y = obs['poses_y'][0]\n",
    "        \n",
    "        reward = 0\n",
    "        \n",
    "        position = np.array([pose_x, pose_y])\n",
    "        \n",
    "        if obs['collisions'][0] == 1.0:\n",
    "            done = True\n",
    "            reward = -1\n",
    "        \n",
    "                \n",
    "        if obs['lap_counts'][0] == 1.0:\n",
    "            reward = 1.0\n",
    "            done = True\n",
    "        \n",
    "        cp_reward = self.checkpoint(position)\n",
    "        next_state = self.to_vector_state(obs)\n",
    "        reward += cp_reward\n",
    "        \n",
    "        #padding from wall:\n",
    "        scans = obs['scans'][0]\n",
    "        if min(scans) < 0.1:\n",
    "            reward -= 0.1\n",
    "            \n",
    "        self.t += 1\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "VFHEnv({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b8ba2c",
   "metadata": {},
   "source": [
    "### follow the gap and vfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ef5d701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of waypoints:79\n"
     ]
    }
   ],
   "source": [
    "from f1tenth.pkg.src.pkg.vfh_gym import VFH\n",
    "\n",
    "done = False\n",
    "env = VFHEnv({}, map_no='SILVERSTONE_TRAIN', max_v=0, begin_pos=[0, 0, np.pi+1])\n",
    "obs = env.reset()\n",
    "vfh = VFH()\n",
    "positions = []\n",
    "ths = []\n",
    "while not done:\n",
    "    positions.append(obs[3])\n",
    "#     velocity = Vector('v')\n",
    "#     velocity.set(obs[1], 0)\n",
    "#     pos = Vector('p')\n",
    "#     pos.set(0, 0)\n",
    "    v, th = vfh.process_observation(obs[0], {'linear_vel_x': obs[1], 'angular_vel_z': obs[2]})\n",
    "#     print(obs[1], v, th)\n",
    "#     ths.append(th)\n",
    "    obs, r, done, _ = env.step([th, v])\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2d02be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4476903384201821 -0.3530283983221573\n"
     ]
    }
   ],
   "source": [
    "print(max(ths), min(ths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15f37af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5235987755982988"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.pi/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "468eacdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./ss-wp.np', 'wb') as f:\n",
    "    np.save(f, np.array(positions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "851f8aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7759, 3)\n"
     ]
    }
   ],
   "source": [
    "with open('./ss-wp.np', 'rb') as f:\n",
    "    x = np.load(f)\n",
    "    \n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b166d9",
   "metadata": {},
   "source": [
    "### render classic control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc428db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "speed = 5.0\n",
    "env = F110Env({}, map_no=4, max_v=speed)\n",
    "obs = env.reset()\n",
    "\n",
    "agent = FGAgent(speed=speed)\n",
    "\n",
    "while not done:\n",
    "    obs = obs*10.0\n",
    "    action = agent.get_action(obs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "#     break\n",
    "    \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c90717",
   "metadata": {},
   "source": [
    "### Race environment single car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b935a060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.F110RaceEnv at 0x7f5da7325820>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import yaml\n",
    "import gym\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "from f1tenth.pkg.src.pkg.vfh_gym import VFH\n",
    "    \n",
    "class F110RaceEnv(gym.Env):\n",
    "    def __init__(self, env_config, deterministic=False, max_v=10.0, map_name='SILVERSTONE_OBS', scan_range=10.0, n_step=1):\n",
    "        self.deterministic = deterministic\n",
    "        self.n_step = n_step\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(110,), dtype=np.float32)\n",
    "        self.max_v = max_v\n",
    "        self.map_name = map_name\n",
    "        self.min_cp_dist = 2.0\n",
    "        self.cp_reward = 0.2\n",
    "        self.scan_range = scan_range\n",
    "        \n",
    "#         self.speed_map = {\n",
    "#             0: self.max_v,\n",
    "#             1: self.max_v*3./4,\n",
    "#             2: self.max_v*1./2,\n",
    "#             3: self.max_v*1./4, \n",
    "#             4: 0.0\n",
    "#         }\n",
    "#         self.action_space = gym.spaces.Discrete(len(self.speed_map),)\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "        \n",
    "        self.vfh = None\n",
    "        self.reset()\n",
    "        \n",
    "  \n",
    "    def reset(self):\n",
    "        self.vfh = VFH()\n",
    "        def render_callback(env_renderer):\n",
    "            # custom extra drawing function\n",
    "            \n",
    "            e = env_renderer\n",
    "\n",
    "            # update camera to follow car\n",
    "            x = e.cars[0].vertices[::2]\n",
    "            y = e.cars[0].vertices[1::2]\n",
    "            top, bottom, left, right = max(y), min(y), min(x), max(x)\n",
    "            e.score_label.x = left\n",
    "            e.score_label.y = top - 700\n",
    "            e.left = left - 800\n",
    "            e.right = right + 800\n",
    "            e.top = top + 800\n",
    "            e.bottom = bottom - 800\n",
    "            \n",
    "        map_path = f\"./f1tenth_gym/examples/{self.map_name}\"\n",
    "        \n",
    "        self.env = gym.make('f110_gym:f110-v0', map=map_path, map_ext='.png', num_agents=1)\n",
    "        self.env.add_render_callback(render_callback)\n",
    "        \n",
    "        with open('./sochi-wp.np', 'rb') as f:\n",
    "            wps = np.load(f)\n",
    "            \n",
    "        idxs = [i%20 == 0 for i in range(len(wps))]\n",
    "        self.checkpoints = wps[idxs]\n",
    "\n",
    "        random_idx = np.random.randint(0, len(self.checkpoints)-1)\n",
    "#         start_point = self.checkpoints[random_idx]\n",
    "        start_point = [0, 0, np.pi+1] \n",
    "    \n",
    "        obs, step_reward, done, info = self.env.reset(\n",
    "            np.array([\n",
    "                start_point\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        self.next_cp_idx = random_idx + 1\n",
    "        self.vfh_state = None\n",
    "        self.t = 0\n",
    "\n",
    "        return self.to_vector_state(obs)\n",
    "    \n",
    "    def to_vector_state(self, obs):\n",
    "        \n",
    "        scanner = np.clip(obs['scans'][0], 0, self.scan_range)\n",
    "        self.vfh_state = [scanner, obs['linear_vels_x'][0]]\n",
    "        \n",
    "        buck = 10\n",
    "        size = 1080//buck\n",
    "        agg_scanner = np.zeros(size,)\n",
    "        for i in range(size):\n",
    "            agg_scanner[i] = np.mean(scanner[i*buck: i*buck+buck])\n",
    "        \n",
    "        agg_scanner /= self.scan_range\n",
    "        state = np.concatenate([\n",
    "            agg_scanner,\n",
    "            np.array(obs['linear_vels_x'][:1])/self.max_v,\n",
    "            np.array(obs['ang_vels_z'][:1])/3.0,\n",
    "        ])\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def checkpoint(self, position):\n",
    "        dist = np.linalg.norm(position - self.checkpoints[self.next_cp_idx][:2])\n",
    "        reward = 0\n",
    "        if dist < self.min_cp_dist:\n",
    "            reward = self.cp_reward\n",
    "    \n",
    "            self.next_cp_idx = (self.next_cp_idx + 1)%len(self.checkpoints)\n",
    "        return reward\n",
    "        \n",
    "        \n",
    "    def _step(self, action):\n",
    "        reward = 0\n",
    "#         vfh_v = Vector('v')\n",
    "#         vfh_v.set(self.vfh_state[1], 0)\n",
    "#         pos = Vector('p')\n",
    "#         pos.set(0, 0)\n",
    "#         v, th = self.vfh.process_lidar(self.vfh_state[0], vfh_v, pos)\n",
    "\n",
    "        act_v = action[0]*(self.max_v/2)+(self.max_v/2)\n",
    "        th = action[1]*np.pi/6\n",
    "#         act_v = self.speed_map[action]\n",
    "        act = np.array([[th, act_v]])\n",
    "        obs, step_reward, done, info = self.env.step(act)\n",
    "        pose_x = obs['poses_x'][0]\n",
    "        pose_y = obs['poses_y'][0]\n",
    "\n",
    "\n",
    "        position = np.array([pose_x, pose_y])\n",
    "\n",
    "        if obs['collisions'][0] == 1.0:\n",
    "            reward = -1\n",
    "            done = True\n",
    "\n",
    "        cp_reward = self.checkpoint(position)\n",
    "        next_state = self.to_vector_state(obs)\n",
    "        reward += cp_reward\n",
    "        self.t += 1\n",
    "        \n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = 0\n",
    "        self.t += 1\n",
    "            \n",
    "        if self.n_step > 1:\n",
    "            while not done and self.t % self.n_step != 0:\n",
    "                next_state, reward, done, info = self._step(action)\n",
    "            return next_state, reward, done, info\n",
    "        else:\n",
    "            return self._step(action)\n",
    "    \n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "F110RaceEnv({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f171cf",
   "metadata": {},
   "source": [
    "### train single car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa059274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219c1369",
   "metadata": {},
   "source": [
    "### ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a4cc6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-23 01:24:34,051\tWARNING ppo.py:223 -- `train_batch_size` (70000) cannot be achieved with your other settings (num_workers=16 num_envs_per_worker=16 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 273.\n",
      "2022-03-23 01:24:34,052\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-03-23 01:24:34,052\tINFO trainer.py:790 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34497)\u001b[0m 2022-03-23 01:24:43,534\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34516)\u001b[0m 2022-03-23 01:24:43,579\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34515)\u001b[0m 2022-03-23 01:24:43,610\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34499)\u001b[0m 2022-03-23 01:24:43,611\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34514)\u001b[0m 2022-03-23 01:24:43,620\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34508)\u001b[0m 2022-03-23 01:24:43,563\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34517)\u001b[0m 2022-03-23 01:24:43,545\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34505)\u001b[0m 2022-03-23 01:24:43,541\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34493)\u001b[0m 2022-03-23 01:24:43,677\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34513)\u001b[0m 2022-03-23 01:24:43,647\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34501)\u001b[0m 2022-03-23 01:24:43,680\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34509)\u001b[0m 2022-03-23 01:24:43,659\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34500)\u001b[0m 2022-03-23 01:24:43,676\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34488)\u001b[0m 2022-03-23 01:24:43,663\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34507)\u001b[0m 2022-03-23 01:24:43,674\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34511)\u001b[0m 2022-03-23 01:24:43,646\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "2022-03-23 01:24:58,488\tINFO trainable.py:125 -- Trainable.setup took 24.438 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-03-23 01:24:58,491\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_workers': 16, 'num_envs_per_worker': 16, 'create_env_on_driver': False, 'rollout_fragment_length': 273, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 0.0003, 'train_batch_size': 70000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'F110RaceEnv', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 16, 'num_envs_per_worker': 16, 'create_env_on_driver': False, 'rollout_fragment_length': 273, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 0.0003, 'train_batch_size': 70000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'F110RaceEnv', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1.0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.agents.ppo.ppo_torch_policy.PPOTorchPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 1.0, 'sgd_minibatch_size': 4096, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1.0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.agents.ppo.ppo_torch_policy.PPOTorchPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 1.0, 'sgd_minibatch_size': 4096, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-23 01:25:40,722\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward:-0.4186440677966101\n",
      "checkpoint saved at ./checkpoints/race_vfh_robust4/checkpoint_000001/checkpoint-1\n",
      "episode: 1 reward:-0.307475317348378\n",
      "episode: 2 reward:-0.10535714285714286\n",
      "episode: 3 reward:0.31729729729729733\n",
      "episode: 4 reward:0.9834951456310681\n",
      "episode: 5 reward:1.8441379310344834\n",
      "checkpoint saved at ./checkpoints/race_vfh_robust4/checkpoint_000006/checkpoint-6\n",
      "episode: 6 reward:2.4796296296296303\n",
      "episode: 7 reward:3.7867924528301895\n",
      "episode: 8 reward:4.809999999999998\n",
      "episode: 9 reward:5.875999999999997\n",
      "episode: 10 reward:7.149999999999992\n",
      "checkpoint saved at ./checkpoints/race_vfh_robust4/checkpoint_000011/checkpoint-11\n",
      "episode: 11 reward:7.949999999999991\n",
      "episode: 12 reward:8.887999999999987\n",
      "episode: 13 reward:9.345999999999988\n",
      "episode: 14 reward:8.719999999999992\n",
      "episode: 15 reward:10.265999999999986\n",
      "checkpoint saved at ./checkpoints/race_vfh_robust4/checkpoint_000016/checkpoint-16\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.agents import ppo, sac, ddpg, dqn\n",
    "from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "# with half cheeta configs\n",
    "\n",
    "DEFAULT_CONFIG['framework'] = 'torch'\n",
    "DEFAULT_CONFIG['num_workers'] = 16\n",
    "DEFAULT_CONFIG['num_gpus'] = 1.0\n",
    "# DEFAULT_CONFIG['num_gpus_per_worker'] = 1/10\n",
    "# DEFAULT_CONFIG['model']['fcnet_hiddens'] = [300, 300]\n",
    "DEFAULT_CONFIG['kl_coeff'] = 1.0\n",
    "DEFAULT_CONFIG['clip_param'] = 0.2\n",
    "DEFAULT_CONFIG['num_envs_per_worker'] = 16\n",
    "DEFAULT_CONFIG['train_batch_size'] = 70000\n",
    "DEFAULT_CONFIG['sgd_minibatch_size'] = 4096\n",
    "DEFAULT_CONFIG['batch_mode'] = 'truncate_episodes'\n",
    "# DEFAULT_CONFIG['model']['use_lstm'] = True\n",
    "# DEFAULT_CONFIG['model']['lstm_use_prev_action'] = True\n",
    "# DEFAULT_CONFIG['model']['max_seq_len'] = 10\n",
    "DEFAULT_CONFIG['lr'] = .0003\n",
    "\n",
    "trainer = ppo.PPOTrainer(env=F110RaceEnv, config=DEFAULT_CONFIG)\n",
    "# print(\"FINE TUNING\")\n",
    "# trainer.restore('./checkpoints/race_vfh/checkpoint_000031/checkpoint-31')\n",
    "\n",
    "print(trainer.config)\n",
    "rewards = []\n",
    "\n",
    "import pickle\n",
    "\n",
    "for i in range(500):\n",
    "    result = trainer.train()\n",
    "    print(f\"episode: {i} reward:{result['episode_reward_mean']}\")\n",
    "    rewards.append(result['episode_reward_mean'])\n",
    "    if i%5 == 0:\n",
    "        with open('./checkpoints/race_vfh_robust_r4', 'wb') as f:\n",
    "            pickle.dump(rewards, f)\n",
    "        cp = trainer.save(\"./checkpoints/race_vfh_robust4\")\n",
    "        print(\"checkpoint saved at\", cp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720183a4",
   "metadata": {},
   "source": [
    "### q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ab8c572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.agents import dqn\n",
    "from ray.rllib.agents.dqn.dqn import DEFAULT_CONFIG\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "\n",
    "DEFAULT_CONFIG['framework'] = 'torch'\n",
    "DEFAULT_CONFIG['num_atoms'] = 10\n",
    "DEFAULT_CONFIG['v_min'] = -1\n",
    "DEFAULT_CONFIG['v_max'] = 10\n",
    "DEFAULT_CONFIG['noisy'] = True\n",
    "DEFAULT_CONFIG['n_step'] = 5\n",
    "DEFAULT_CONFIG['num_gpus'] = 1.0\n",
    "DEFAULT_CONFIG['hiddens'] = [100, 100]\n",
    "DEFAULT_CONFIG['replay_buffer_config']['capacity'] = 1000000\n",
    "# DEFAULT_CONFIG['model']['use_lstm'] = True\n",
    "# DEFAULT_CONFIG['model']['lstm_use_prev_action'] = True\n",
    "# DEFAULT_CONFIG['model']['max_seq_len'] = 10\n",
    "# DEFAULT_CONFIG['lr'] = 5e-5\n",
    "\n",
    "trainer = dqn.DQNTrainer(env=F110RaceEnv, config=DEFAULT_CONFIG)\n",
    "# print(\"FINE TUNING\")\n",
    "# trainer.restore('./checkpoints/race_vfh/checkpoint_000031/checkpoint-31')\n",
    "\n",
    "print(trainer.config)\n",
    "rewards = []\n",
    "\n",
    "import pickle\n",
    "\n",
    "for i in range(200):\n",
    "    result = trainer.train()\n",
    "    print(f\"episode: {i} reward:{result['episode_reward_mean']}\")\n",
    "    rewards.append(result['episode_reward_mean'])\n",
    "    if i%5 == 0:\n",
    "        with open('./checkpoints/race_vfh_robust_r', 'wb') as f:\n",
    "            pickle.dump(rewards, f)\n",
    "        cp = trainer.save(\"./checkpoints/race_vfh_robust\")\n",
    "        print(\"checkpoint saved at\", cp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f3bd250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-23 15:27:33,320\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-03-23 15:27:33,332\tINFO trainable.py:495 -- Restored on 192.168.0.33 from checkpoint: ./checkpoints/race_vfh_robust4/checkpoint_000171/checkpoint-171\n",
      "2022-03-23 15:27:33,333\tINFO trainable.py:503 -- Current state after restoring: {'_iteration': 171, '_timesteps_total': 23901696, '_time_total': 3766.555737733841, '_episodes_total': 8404}\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.agents import ppo, sac, ddpg, dqn\n",
    "from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG\n",
    "\n",
    "DEFAULT_CONFIG['framework'] = 'torch'\n",
    "DEFAULT_CONFIG['num_workers'] = 0\n",
    "DEFAULT_CONFIG['num_gpus'] = 0\n",
    "# DEFAULT_CONFIG['model']['fcnet_hiddens'] = [100, 100]\n",
    "DEFAULT_CONFIG[\"evaluation_num_workers\"] = 0\n",
    "DEFAULT_CONFIG[\"evaluation_config\"][\"render_env\"] = False\n",
    "DEFAULT_CONFIG[\"evaluation_config\"][\"explore\"] = False\n",
    "DEFAULT_CONFIG['explore'] = False\n",
    "\n",
    "trainer = ppo.PPOTrainer(env=F110RaceEnv, config=DEFAULT_CONFIG)\n",
    "trainer.restore('./checkpoints/race_vfh_robust4/checkpoint_000171/checkpoint-171')\n",
    "# trainer.evaluate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3be7502",
   "metadata": {},
   "source": [
    "\n",
    "### render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f40a514",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "done = False\n",
    "env = F110RaceEnv({}, n_step=1)\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = trainer.compute_single_action(obs, explore=False)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7b5c4e",
   "metadata": {},
   "source": [
    "### Race environment multiple cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eef51ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import yaml\n",
    "import gym\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "from f1tenth.pkg.src.pkg.vfh_gym import VFH\n",
    "    \n",
    "class F110MultiRaceEnv(gym.Env):\n",
    "    def __init__(self, env_config, deterministic=False, max_v=10.0, map_name='SOCHI-orig', scan_range=10.0, n_step=1):\n",
    "        self.deterministic = deterministic\n",
    "        self.n_step = n_step\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(110,), dtype=np.float32)\n",
    "        self.max_v = max_v\n",
    "        self.map_name = map_name\n",
    "        self.min_cp_dist = 2.0\n",
    "        self.cp_reward = 0.2\n",
    "        self.scan_range = scan_range\n",
    "        \n",
    "#         self.speed_map = {\n",
    "#             0: self.max_v,\n",
    "#             1: self.max_v*3./4,\n",
    "#             2: self.max_v*1./2,\n",
    "#             3: self.max_v*1./4, \n",
    "#             4: 0.0\n",
    "#         }\n",
    "        #self.action_space = gym.spaces.Discrete(len(self.speed_map),)\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "        \n",
    "        self.vfh = None\n",
    "        self.reset()\n",
    "        \n",
    "  \n",
    "    def reset(self):\n",
    "        self.vfh = VFH()\n",
    "        def render_callback(env_renderer):\n",
    "            # custom extra drawing function\n",
    "            \n",
    "            e = env_renderer\n",
    "\n",
    "            # update camera to follow car\n",
    "            x = e.cars[0].vertices[::2]\n",
    "            y = e.cars[0].vertices[1::2]\n",
    "            top, bottom, left, right = max(y), min(y), min(x), max(x)\n",
    "            e.score_label.x = left\n",
    "            e.score_label.y = top - 700\n",
    "            e.left = left - 800\n",
    "            e.right = right + 800\n",
    "            e.top = top + 800\n",
    "            e.bottom = bottom - 800\n",
    "            \n",
    "        map_path = f\"./f1tenth_gym/examples/{self.map_name}\"\n",
    "        \n",
    "        self.env = gym.make('f110_gym:f110-v0', map=map_path, map_ext='.png', num_agents=2)\n",
    "        self.env.add_render_callback(render_callback)\n",
    "        \n",
    "        with open('./sochi-wp.np', 'rb') as f:\n",
    "            wps = np.load(f)\n",
    "            \n",
    "        idxs = [i%20 == 0 for i in range(len(wps))]\n",
    "        self.checkpoints = wps[idxs]\n",
    "\n",
    "        #random_idx = np.random.randint(0, len(self.checkpoints)-1)\n",
    "        start_points = [[-0.4, -2.0, 1], [-0.4, .3, 1]]\n",
    "        random_idx = 1#np.random.randint(0,2)\n",
    "        start_point = start_points[random_idx]\n",
    "        \n",
    "#         if random_idx == 1:\n",
    "#             print(\"rl on left\")\n",
    "#         else:\n",
    "#             print(\"rl on right\")\n",
    "        \n",
    "        op_start_point = start_points[1-random_idx]\n",
    "        obs, step_reward, done, info = self.env.reset(\n",
    "            np.array([\n",
    "                start_point,\n",
    "                op_start_point\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        self.cp_idx = 1\n",
    "        self.op_cp_idx = 1\n",
    "        \n",
    "        self.vfh_state = None\n",
    "        self.op_vfh_state = None\n",
    "        self.t = 0\n",
    "\n",
    "        return self.to_vector_state(obs)\n",
    "    \n",
    "    def to_vector_state(self, obs):\n",
    "        \n",
    "        scanner = np.clip(obs['scans'][0], 0, self.scan_range)\n",
    "        self.vfh_state = [scanner, obs['linear_vels_x'][0]]\n",
    "        \n",
    "        op_scanner = np.clip(obs['scans'][1], 0, self.scan_range)\n",
    "        self.op_vfh_state = [op_scanner, obs['linear_vels_x'][1], obs['ang_vels_z'][1]]\n",
    "        \n",
    "        buck = 10\n",
    "        size = 1080//buck\n",
    "        agg_scanner = np.zeros(size,)\n",
    "        for i in range(size):\n",
    "            agg_scanner[i] = np.mean(scanner[i*buck: i*buck+buck])\n",
    "        \n",
    "        agg_scanner /= self.scan_range\n",
    "        state = np.concatenate([\n",
    "            agg_scanner,\n",
    "            np.array(obs['linear_vels_x'][:1])/self.max_v,\n",
    "            np.array(obs['ang_vels_z'][:1])/3.0,\n",
    "        ])\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def checkpoint(self, pos, pos_op):\n",
    "        reward = 0\n",
    "        dist = np.linalg.norm(pos - self.checkpoints[self.cp_idx%len(self.checkpoints)][:2])\n",
    "        if dist < self.min_cp_dist:\n",
    "            reward += self.cp_reward\n",
    "            self.cp_idx += 1\n",
    "            \n",
    "        dist = np.linalg.norm(pos_op - self.checkpoints[self.op_cp_idx%len(self.checkpoints)][:2])\n",
    "        if dist < self.min_cp_dist:\n",
    "            self.op_cp_idx += 1\n",
    "        \n",
    "        reward -= (self.op_cp_idx - self.cp_idx)*0.1\n",
    "        \n",
    "        return reward\n",
    "        \n",
    "        \n",
    "    def _step(self, action):\n",
    "        reward = 0\n",
    "        \n",
    "#         pos = Vector('p')\n",
    "#         pos.set(0, 0)\n",
    "#         vfh_v_op = Vector('v')\n",
    "#         vfh_v_op.set(self.vfh_state[1], 0)\n",
    "        op_v, op_th = self.vfh.process_observation(self.op_vfh_state[0], \n",
    "                        {'linear_vel_x': self.vfh_state[1], 'angular_vel_z': self.vfh_state[1]})\n",
    "\n",
    "        \n",
    "        act_v = action[0]*(self.max_v/2)+(self.max_v/2)\n",
    "        act_th = action[1]*np.pi/6\n",
    "        act = np.array([[act_th, act_v], [op_th, op_v]])\n",
    "        \n",
    "        obs, step_reward, done, info = self.env.step(act)\n",
    "        \n",
    "        pose_x = obs['poses_x'][0]\n",
    "        pose_y = obs['poses_y'][0]\n",
    "        position = np.array([pose_x, pose_y])\n",
    "        \n",
    "        pose_x = obs['poses_x'][1]\n",
    "        pose_y = obs['poses_y'][1]\n",
    "        position_op = np.array([pose_x, pose_y])\n",
    "\n",
    "        if obs['collisions'][0] == 1.0:\n",
    "            reward = -1\n",
    "            done = True\n",
    "            \n",
    "        if (self.op_cp_idx - self.cp_idx) > 10:\n",
    "            reward = -1\n",
    "            done = True\n",
    "        \n",
    "        if obs['lap_counts'][0] == 1.0:\n",
    "            reward = 1.0\n",
    "            done = True\n",
    "        elif obs['lap_counts'][1] == 1.0:\n",
    "            reward = -1.0\n",
    "            done = True\n",
    "        \n",
    "\n",
    "        cp_reward = self.checkpoint(position, position_op)\n",
    "        next_state = self.to_vector_state(obs)\n",
    "        reward += cp_reward\n",
    "        self.t += 1\n",
    "        \n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = 0\n",
    "        self.t += 1\n",
    "        \n",
    "        if self.n_step > 1:\n",
    "            while not done and self.t % self.n_step != 0:\n",
    "                next_state, reward, done, info = self._step(action)\n",
    "            return next_state, reward, done, info\n",
    "        else:\n",
    "            return self._step(action)\n",
    "    \n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "F110MultiRaceEnv({}).render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1bc950",
   "metadata": {},
   "source": [
    "### train multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "654639a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.agents import ppo, sac, ddpg, dqn\n",
    "from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "\n",
    "DEFAULT_CONFIG['framework'] = 'torch'\n",
    "DEFAULT_CONFIG['num_workers'] = 10\n",
    "DEFAULT_CONFIG['num_gpus'] = 1.0\n",
    "DEFAULT_CONFIG['num_gpus_per_worker'] = 1/10\n",
    "DEFAULT_CONFIG['model']['fcnet_hiddens'] = [100, 100]\n",
    "DEFAULT_CONFIG['entropy_coeff'] = 0.0\n",
    "DEFAULT_CONFIG['clip_param'] = 0.3\n",
    "DEFAULT_CONFIG['train_batch_size'] = 10000\n",
    "DEFAULT_CONFIG['batch_mode'] = 'truncate_episodes'\n",
    "# DEFAULT_CONFIG['model']['use_lstm'] = True\n",
    "# DEFAULT_CONFIG['model']['lstm_use_prev_action'] = True\n",
    "# DEFAULT_CONFIG['model']['max_seq_len'] = 10\n",
    "# DEFAULT_CONFIG['lr'] = 5e-5\n",
    "\n",
    "trainer = ppo.PPOTrainer(env=F110MultiRaceEnv, config=DEFAULT_CONFIG)\n",
    "# print(\"FINE TUNING\")\n",
    "# trainer.restore('./checkpoints/race_vfh/checkpoint_000031/checkpoint-31')\n",
    "\n",
    "print(trainer.config)\n",
    "rewards = []\n",
    "\n",
    "import pickle\n",
    "\n",
    "for i in range(200):\n",
    "    result = trainer.train()\n",
    "    print(f\"episode: {i} reward:{result['episode_reward_mean']}\")\n",
    "    rewards.append(result['episode_reward_mean'])\n",
    "    if i%5==0:\n",
    "        with open('./checkpoints/race_vfh_r2', 'wb') as f:\n",
    "            pickle.dump(rewards, f)\n",
    "        cp = trainer.save(\"./checkpoints/race_vfh2\")\n",
    "        print(\"checkpoint saved at\", cp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f81ea13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Install gputil for GPU system monitoring.\n",
      "2022-03-23 14:43:46,882\tINFO trainable.py:495 -- Restored on 192.168.0.33 from checkpoint: ./checkpoints/race_vfh_robust4/checkpoint_000171/checkpoint-171\n",
      "2022-03-23 14:43:46,883\tINFO trainable.py:503 -- Current state after restoring: {'_iteration': 171, '_timesteps_total': 23901696, '_time_total': 3766.555737733841, '_episodes_total': 8404}\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.agents import ppo, sac, ddpg, dqn\n",
    "from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG\n",
    "\n",
    "DEFAULT_CONFIG['framework'] = 'torch'\n",
    "DEFAULT_CONFIG['num_workers'] = 0\n",
    "DEFAULT_CONFIG['num_gpus'] = 0\n",
    "# DEFAULT_CONFIG['model']['fcnet_hiddens'] = [100, 100]\n",
    "DEFAULT_CONFIG[\"evaluation_num_workers\"] = 0\n",
    "DEFAULT_CONFIG[\"evaluation_config\"][\"render_env\"] = False\n",
    "DEFAULT_CONFIG[\"evaluation_config\"][\"explore\"] = False\n",
    "DEFAULT_CONFIG['explore'] = False\n",
    "\n",
    "trainer = ppo.PPOTrainer(env=F110MultiRaceEnv, config=DEFAULT_CONFIG)\n",
    "trainer.restore('./checkpoints/race_vfh_robust4/checkpoint_000171/checkpoint-171')\n",
    "# trainer.evaluate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b975f0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "done = False\n",
    "env = F110MultiRaceEnv({}, n_step=1)\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = trainer.compute_single_action(obs, explore=False)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f0963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "738px",
    "left": "552px",
    "top": "111.125px",
    "width": "383.976px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
