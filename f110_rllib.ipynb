{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1bd4eb",
   "metadata": {},
   "source": [
    "# F1tenth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b781558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f07c65",
   "metadata": {},
   "source": [
    "## Environment playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e382dbcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# racecar_env = gym.make(\n",
    "#     'f110_gym:f110-v0',\n",
    "#     map='./f1tenth_gym/gym/f110_gym/envs/maps/vegas',\n",
    "#     map_ext='.png'\n",
    "# )\n",
    "\n",
    "racecar_env = gym.make(\n",
    "    'f110_gym:f110-v0',\n",
    "    map='./f1tenth_gym/examples/example_map',\n",
    "    map_ext='.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acba6632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ego_idx': 0,\n",
       " 'scans': [array([29.98576175, 30.01263728, 29.99129338, ..., 29.99864406,\n",
       "         29.9889291 , 30.01278365]),\n",
       "  array([29.98576175, 30.01263728, 29.99129338, ..., 29.99864406,\n",
       "         29.9889291 , 30.01278365])],\n",
       " 'poses_x': [0.0, 2.0],\n",
       " 'poses_y': [0.0, 0.0],\n",
       " 'poses_theta': [0.0, 0.0],\n",
       " 'linear_vels_x': [0.0, 0.0],\n",
       " 'linear_vels_y': [0.0, 0.0],\n",
       " 'ang_vels_z': [0.0, 0.0],\n",
       " 'collisions': array([0., 0.]),\n",
       " 'lap_times': array([0.01, 0.01]),\n",
       " 'lap_counts': array([0., 0.])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, step_reward, done, info = racecar_env.reset(\n",
    "    poses=np.array([[0., 0., 0.], # pose of ego\n",
    "             [2., 0., 0.]])  # pose of 2nd agent\n",
    ") \n",
    "\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2df1290a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1082,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = np.concatenate([\n",
    "    obs['scans'][0],\n",
    "    np.array(obs['linear_vels_x'][:1]),\n",
    "    np.array(obs['linear_vels_y'][:1]),\n",
    "])\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54349ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0596855 ,  0.44670051],\n",
       "       [-0.05675966,  1.8354974 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeds = np.random.rand(2, 1)*2\n",
    "pi_4 = 3.1415/8\n",
    "pi_2 = 3.1415/4\n",
    "angles = np.random.rand(2, 1)*pi_2-pi_4\n",
    "actions = np.concatenate([angles, speeds], axis=1)\n",
    "\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "133a6cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## action consists of ndarray(num_agent, 2) 0: steering angle 1: velocity\n",
    "## the reward function is only for the first agent\n",
    "\n",
    "import time\n",
    "import gym \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "racecar_env = gym.make(\n",
    "    'f110_gym:f110-v0',\n",
    "    map='./f1tenth_gym/examples/example_map',\n",
    "    map_ext='.png',\n",
    "    num_agents=1\n",
    ")\n",
    "steps = 0\n",
    "\n",
    "obs, step_reward, done, info = racecar_env.reset(\n",
    "    poses=np.array([[0., 0., 1.5]]) \n",
    ") \n",
    "\n",
    "rewards = []\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    speeds = np.random.rand(2, 1)*20\n",
    "    pi_4 = 3.1415/8\n",
    "    pi_2 = 3.1415/4\n",
    "    speeds[1][0] = 0.1\n",
    "    angles = np.random.rand(2, 1)*pi_2-pi_4\n",
    "    actions = np.concatenate([angles, speeds], axis=1)\n",
    "\n",
    "    obs, step_reward, done, info = racecar_env.step(actions)\n",
    "    rewards.append(step_reward)\n",
    "    \n",
    "    racecar_env.render()\n",
    "    steps += 1\n",
    "    \n",
    "    if steps > 500:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa1c0a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa25f6c8460>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab60a5e8",
   "metadata": {},
   "source": [
    "## Define environment wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8052fa",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### waypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a6250",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import yaml\n",
    "import gym\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "\n",
    "from numba import njit\n",
    "\n",
    "from pyglet.gl import GL_POINTS\n",
    "\n",
    "\n",
    "# @njit(fastmath=False, cache=True)\n",
    "def nearest_point_on_trajectory(point, trajectory):\n",
    "    \"\"\"\n",
    "    Return the nearest point along the given piecewise linear trajectory.\n",
    "\n",
    "    Same as nearest_point_on_line_segment, but vectorized. This method is quite fast, time constraints should\n",
    "    not be an issue so long as trajectories are not insanely long.\n",
    "\n",
    "        Order of magnitude: trajectory length: 1000 --> 0.0002 second computation (5000fps)\n",
    "\n",
    "    point: size 2 numpy array\n",
    "    trajectory: Nx2 matrix of (x,y) trajectory waypoints\n",
    "        - these must be unique. If they are not unique, a divide by 0 error will destroy the world\n",
    "    \"\"\"\n",
    "    diffs = trajectory[1:,:] - trajectory[:-1,:]\n",
    "    l2s   = diffs[:,0]**2 + diffs[:,1]**2\n",
    "    # this is equivalent to the elementwise dot product\n",
    "    # dots = np.sum((point - trajectory[:-1,:]) * diffs[:,:], axis=1)\n",
    "    dots = np.empty((trajectory.shape[0]-1, ))\n",
    "    for i in range(dots.shape[0]):\n",
    "        dots[i] = np.dot((point - trajectory[i, :]), diffs[i, :])\n",
    "    t = dots / l2s\n",
    "    t[t<0.0] = 0.0\n",
    "    t[t>1.0] = 1.0\n",
    "    # t = np.clip(dots / l2s, 0.0, 1.0)\n",
    "    projections = trajectory[:-1,:] + (t*diffs.T).T\n",
    "    # dists = np.linalg.norm(point - projections, axis=1)\n",
    "    dists = np.empty((projections.shape[0],))\n",
    "    for i in range(dists.shape[0]):\n",
    "        temp = point - projections[i]\n",
    "        dists[i] = np.sqrt(np.sum(temp*temp))\n",
    "    min_dist_segment = np.argmin(dists)\n",
    "    return projections[min_dist_segment], dists[min_dist_segment], t[min_dist_segment], min_dist_segment\n",
    "\n",
    "# @njit(fastmath=False, cache=True)\n",
    "def first_point_on_trajectory_intersecting_circle(point, radius, trajectory, t=0.0, wrap=False):\n",
    "    \"\"\"\n",
    "    starts at beginning of trajectory, and find the first point one radius away from the given point along the trajectory.\n",
    "\n",
    "    Assumes that the first segment passes within a single radius of the point\n",
    "\n",
    "    http://codereview.stackexchange.com/questions/86421/line-segment-to-circle-collision-algorithm\n",
    "    \"\"\"\n",
    "    start_i = int(t)\n",
    "    start_t = t % 1.0\n",
    "    first_t = None\n",
    "    first_i = None\n",
    "    first_p = None\n",
    "    trajectory = np.ascontiguousarray(trajectory)\n",
    "    for i in range(start_i, trajectory.shape[0]-1):\n",
    "        start = trajectory[i,:]\n",
    "        end = trajectory[i+1,:]+1e-6\n",
    "        V = np.ascontiguousarray(end - start)\n",
    "\n",
    "        a = np.dot(V,V)\n",
    "        b = 2.0*np.dot(V, start - point)\n",
    "        c = np.dot(start, start) + np.dot(point,point) - 2.0*np.dot(start, point) - radius*radius\n",
    "        discriminant = b*b-4*a*c\n",
    "\n",
    "        if discriminant < 0:\n",
    "            continue\n",
    "        #   print \"NO INTERSECTION\"\n",
    "        # else:\n",
    "        # if discriminant >= 0.0:\n",
    "        discriminant = np.sqrt(discriminant)\n",
    "        t1 = (-b - discriminant) / (2.0*a)\n",
    "        t2 = (-b + discriminant) / (2.0*a)\n",
    "        if i == start_i:\n",
    "            if t1 >= 0.0 and t1 <= 1.0 and t1 >= start_t:\n",
    "                first_t = t1\n",
    "                first_i = i\n",
    "                first_p = start + t1 * V\n",
    "                break\n",
    "            if t2 >= 0.0 and t2 <= 1.0 and t2 >= start_t:\n",
    "                first_t = t2\n",
    "                first_i = i\n",
    "                first_p = start + t2 * V\n",
    "                break\n",
    "        elif t1 >= 0.0 and t1 <= 1.0:\n",
    "            first_t = t1\n",
    "            first_i = i\n",
    "            first_p = start + t1 * V\n",
    "            break\n",
    "        elif t2 >= 0.0 and t2 <= 1.0:\n",
    "            first_t = t2\n",
    "            first_i = i\n",
    "            first_p = start + t2 * V\n",
    "            break\n",
    "    # wrap around to the beginning of the trajectory if no intersection is found1\n",
    "    if wrap and first_p is None:\n",
    "        for i in range(-1, start_i):\n",
    "            start = trajectory[i % trajectory.shape[0],:]\n",
    "            end = trajectory[(i+1) % trajectory.shape[0],:]+1e-6\n",
    "            V = end - start\n",
    "\n",
    "            a = np.dot(V,V)\n",
    "            b = 2.0*np.dot(V, start - point)\n",
    "            c = np.dot(start, start) + np.dot(point,point) - 2.0*np.dot(start, point) - radius*radius\n",
    "            discriminant = b*b-4*a*c\n",
    "\n",
    "            if discriminant < 0:\n",
    "                continue\n",
    "            discriminant = np.sqrt(discriminant)\n",
    "            t1 = (-b - discriminant) / (2.0*a)\n",
    "            t2 = (-b + discriminant) / (2.0*a)\n",
    "            if t1 >= 0.0 and t1 <= 1.0:\n",
    "                first_t = t1\n",
    "                first_i = i\n",
    "                first_p = start + t1 * V\n",
    "                break\n",
    "            elif t2 >= 0.0 and t2 <= 1.0:\n",
    "                first_t = t2\n",
    "                first_i = i\n",
    "                first_p = start + t2 * V\n",
    "                break\n",
    "\n",
    "    return first_p, first_i, first_t\n",
    "\n",
    "# @njit(fastmath=False, cache=True)\n",
    "def get_actuation(pose_theta, lookahead_point, position, lookahead_distance, wheelbase):\n",
    "    \"\"\"\n",
    "    Returns actuation\n",
    "    \"\"\"\n",
    "    waypoint_y = np.dot(np.array([np.sin(-pose_theta), np.cos(-pose_theta)]), lookahead_point[0:2]-position)\n",
    "    speed = lookahead_point[2]\n",
    "    if np.abs(waypoint_y) < 1e-6:\n",
    "        return speed, 0.\n",
    "    radius = 1/(2.0*waypoint_y/lookahead_distance**2)\n",
    "    steering_angle = np.arctan(wheelbase/radius)\n",
    "    return speed, steering_angle\n",
    "\n",
    "class PurePursuitPlanner:\n",
    "    \"\"\"\n",
    "    Example Planner\n",
    "    \"\"\"\n",
    "    def __init__(self, conf, wb):\n",
    "        self.wheelbase = wb\n",
    "        self.conf = conf\n",
    "        self.load_waypoints(conf)\n",
    "        self.max_reacquire = 20.\n",
    "\n",
    "        self.drawn_waypoints = []\n",
    "\n",
    "    def load_waypoints(self, conf):\n",
    "        \"\"\"\n",
    "        loads waypoints\n",
    "        \"\"\"\n",
    "        self.waypoints = np.loadtxt(conf.wpt_path, delimiter=conf.wpt_delim, skiprows=conf.wpt_rowskip)\n",
    "\n",
    "    def render_waypoints(self, e):\n",
    "        \"\"\"\n",
    "        update waypoints being drawn by EnvRenderer\n",
    "        \"\"\"\n",
    "\n",
    "        #points = self.waypoints\n",
    "\n",
    "        points = np.vstack((self.waypoints[:, self.conf.wpt_xind], self.waypoints[:, self.conf.wpt_yind])).T\n",
    "        \n",
    "        scaled_points = 50.*points\n",
    "\n",
    "        for i in range(points.shape[0]):\n",
    "            if len(self.drawn_waypoints) < points.shape[0]:\n",
    "                b = e.batch.add(1, GL_POINTS, None, ('v3f/stream', [scaled_points[i, 0], scaled_points[i, 1], 0.]),\n",
    "                                ('c3B/stream', [183, 193, 222]))\n",
    "                self.drawn_waypoints.append(b)\n",
    "            else:\n",
    "                self.drawn_waypoints[i].vertices = [scaled_points[i, 0], scaled_points[i, 1], 0.]\n",
    "        \n",
    "    def _get_current_waypoint(self, waypoints, lookahead_distance, position, theta):\n",
    "        \"\"\"\n",
    "        gets the current waypoint to follow\n",
    "        \"\"\"\n",
    "        wpts = np.vstack((self.waypoints[:, self.conf.wpt_xind], self.waypoints[:, self.conf.wpt_yind])).T\n",
    "        nearest_point, nearest_dist, t, i = nearest_point_on_trajectory(position, wpts)\n",
    "        if nearest_dist < lookahead_distance:\n",
    "            lookahead_point, i2, t2 = first_point_on_trajectory_intersecting_circle(position, lookahead_distance, wpts, i+t, wrap=True)\n",
    "            if i2 == None:\n",
    "                return None\n",
    "            current_waypoint = np.empty((3, ))\n",
    "            # x, y\n",
    "            current_waypoint[0:2] = wpts[i2, :]\n",
    "            # speed\n",
    "            current_waypoint[2] = waypoints[i, self.conf.wpt_vind]\n",
    "            return current_waypoint\n",
    "        elif nearest_dist < self.max_reacquire:\n",
    "            return np.append(wpts[i, :], waypoints[i, self.conf.wpt_vind])\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def plan(self, pose_x, pose_y, pose_theta, lookahead_distance, vgain):\n",
    "        \"\"\"\n",
    "        gives actuation given observation\n",
    "        \"\"\"\n",
    "        position = np.array([pose_x, pose_y])\n",
    "        lookahead_point = self._get_current_waypoint(self.waypoints, lookahead_distance, position, pose_theta)\n",
    "\n",
    "        if lookahead_point is None:\n",
    "            return 4.0, 0.0\n",
    "\n",
    "        speed, steering_angle = get_actuation(pose_theta, lookahead_point, position, lookahead_distance, self.wheelbase)\n",
    "        speed = vgain * speed\n",
    "\n",
    "        return speed, steering_angle\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    main entry point\n",
    "    \"\"\"\n",
    "\n",
    "    work = {'mass': 3.463388126201571, 'lf': 0.15597534362552312, 'tlad': 1.82461887897713965, 'vgain': 0.90338203837889}\n",
    "    \n",
    "    with open('./f1tenth_gym/examples/config_example_map.yaml') as file:\n",
    "        conf_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    conf = Namespace(**conf_dict)\n",
    "\n",
    "    planner = PurePursuitPlanner(conf, 0.17145+0.15875)\n",
    "\n",
    "    def render_callback(env_renderer):\n",
    "        # custom extra drawing function\n",
    "\n",
    "        e = env_renderer\n",
    "\n",
    "        # update camera to follow car\n",
    "        x = e.cars[0].vertices[::2]\n",
    "        y = e.cars[0].vertices[1::2]\n",
    "        top, bottom, left, right = max(y), min(y), min(x), max(x)\n",
    "        e.score_label.x = left\n",
    "        e.score_label.y = top - 700\n",
    "        e.left = left - 800\n",
    "        e.right = right + 800\n",
    "        e.top = top + 800\n",
    "        e.bottom = bottom - 800\n",
    "\n",
    "        planner.render_waypoints(env_renderer)\n",
    "\n",
    "    env = gym.make('f110_gym:f110-v0', map=conf.map_path, map_ext=conf.map_ext, num_agents=1)\n",
    "    env.add_render_callback(render_callback)\n",
    "    \n",
    "    obs, step_reward, done, info = env.reset(np.array([[conf.sx, conf.sy, conf.stheta]]))\n",
    "    env.render()\n",
    "\n",
    "    laptime = 0.0\n",
    "    start = time.time()\n",
    "\n",
    "    while not done:\n",
    "        speed, steer = planner.plan(obs['poses_x'][0], obs['poses_y'][0], obs['poses_theta'][0], work['tlad'], work['vgain'])\n",
    "        obs, step_reward, done, info = env.step(np.array([[steer, speed]]))\n",
    "        laptime += step_reward\n",
    "        env.render(mode='human')\n",
    "        \n",
    "    print('Sim elapsed time:', laptime, 'Real elapsed time:', time.time()-start)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717804a9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### waypoint handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78ecc7a6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import yaml\n",
    "import gym\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "\n",
    "# @njit(fastmath=False, cache=True)\n",
    "def nearest_point_on_trajectory(point, trajectory):\n",
    "    \"\"\"\n",
    "    Return the nearest point along the given piecewise linear trajectory.\n",
    "\n",
    "    Same as nearest_point_on_line_segment, but vectorized. This method is quite fast, time constraints should\n",
    "    not be an issue so long as trajectories are not insanely long.\n",
    "\n",
    "        Order of magnitude: trajectory length: 1000 --> 0.0002 second computation (5000fps)\n",
    "\n",
    "    point: size 2 numpy array\n",
    "    trajectory: Nx2 matrix of (x,y) trajectory waypoints\n",
    "        - these must be unique. If they are not unique, a divide by 0 error will destroy the world\n",
    "    \"\"\"\n",
    "    diffs = trajectory[1:,:] - trajectory[:-1,:]\n",
    "    l2s   = diffs[:,0]**2 + diffs[:,1]**2\n",
    "    # this is equivalent to the elementwise dot product\n",
    "    # dots = np.sum((point - trajectory[:-1,:]) * diffs[:,:], axis=1)\n",
    "    dots = np.empty((trajectory.shape[0]-1, ))\n",
    "    for i in range(dots.shape[0]):\n",
    "        dots[i] = np.dot((point - trajectory[i, :]), diffs[i, :])\n",
    "    t = dots / l2s\n",
    "    t[t<0.0] = 0.0\n",
    "    t[t>1.0] = 1.0\n",
    "    # t = np.clip(dots / l2s, 0.0, 1.0)\n",
    "    projections = trajectory[:-1,:] + (t*diffs.T).T\n",
    "    # dists = np.linalg.norm(point - projections, axis=1)\n",
    "    dists = np.empty((projections.shape[0],))\n",
    "    for i in range(dists.shape[0]):\n",
    "        temp = point - projections[i]\n",
    "        dists[i] = np.sqrt(np.sum(temp*temp))\n",
    "    min_dist_segment = np.argmin(dists)\n",
    "    return projections[min_dist_segment], dists[min_dist_segment], t[min_dist_segment], min_dist_segment\n",
    "\n",
    "# @njit(fastmath=False, cache=True)\n",
    "def first_point_on_trajectory_intersecting_circle(point, radius, trajectory, t=0.0, wrap=False):\n",
    "    \"\"\"\n",
    "    starts at beginning of trajectory, and find the first point one radius away from the given point along the trajectory.\n",
    "\n",
    "    Assumes that the first segment passes within a single radius of the point\n",
    "\n",
    "    http://codereview.stackexchange.com/questions/86421/line-segment-to-circle-collision-algorithm\n",
    "    \"\"\"\n",
    "    start_i = int(t)\n",
    "    start_t = t % 1.0\n",
    "    first_t = None\n",
    "    first_i = None\n",
    "    first_p = None\n",
    "    trajectory = np.ascontiguousarray(trajectory)\n",
    "    for i in range(start_i, trajectory.shape[0]-1):\n",
    "        start = trajectory[i,:]\n",
    "        end = trajectory[i+1,:]+1e-6\n",
    "        V = np.ascontiguousarray(end - start)\n",
    "\n",
    "        a = np.dot(V,V)\n",
    "        b = 2.0*np.dot(V, start - point)\n",
    "        c = np.dot(start, start) + np.dot(point,point) - 2.0*np.dot(start, point) - radius*radius\n",
    "        discriminant = b*b-4*a*c\n",
    "\n",
    "        if discriminant < 0:\n",
    "            continue\n",
    "        #   print \"NO INTERSECTION\"\n",
    "        # else:\n",
    "        # if discriminant >= 0.0:\n",
    "        discriminant = np.sqrt(discriminant)\n",
    "        t1 = (-b - discriminant) / (2.0*a)\n",
    "        t2 = (-b + discriminant) / (2.0*a)\n",
    "        if i == start_i:\n",
    "            if t1 >= 0.0 and t1 <= 1.0 and t1 >= start_t:\n",
    "                first_t = t1\n",
    "                first_i = i\n",
    "                first_p = start + t1 * V\n",
    "                break\n",
    "            if t2 >= 0.0 and t2 <= 1.0 and t2 >= start_t:\n",
    "                first_t = t2\n",
    "                first_i = i\n",
    "                first_p = start + t2 * V\n",
    "                break\n",
    "        elif t1 >= 0.0 and t1 <= 1.0:\n",
    "            first_t = t1\n",
    "            first_i = i\n",
    "            first_p = start + t1 * V\n",
    "            break\n",
    "        elif t2 >= 0.0 and t2 <= 1.0:\n",
    "            first_t = t2\n",
    "            first_i = i\n",
    "            first_p = start + t2 * V\n",
    "            break\n",
    "    # wrap around to the beginning of the trajectory if no intersection is found1\n",
    "    if wrap and first_p is None:\n",
    "        for i in range(-1, start_i):\n",
    "            start = trajectory[i % trajectory.shape[0],:]\n",
    "            end = trajectory[(i+1) % trajectory.shape[0],:]+1e-6\n",
    "            V = end - start\n",
    "\n",
    "            a = np.dot(V,V)\n",
    "            b = 2.0*np.dot(V, start - point)\n",
    "            c = np.dot(start, start) + np.dot(point,point) - 2.0*np.dot(start, point) - radius*radius\n",
    "            discriminant = b*b-4*a*c\n",
    "\n",
    "            if discriminant < 0:\n",
    "                continue\n",
    "            discriminant = np.sqrt(discriminant)\n",
    "            t1 = (-b - discriminant) / (2.0*a)\n",
    "            t2 = (-b + discriminant) / (2.0*a)\n",
    "            if t1 >= 0.0 and t1 <= 1.0:\n",
    "                first_t = t1\n",
    "                first_i = i\n",
    "                first_p = start + t1 * V\n",
    "                break\n",
    "            elif t2 >= 0.0 and t2 <= 1.0:\n",
    "                first_t = t2\n",
    "                first_i = i\n",
    "                first_p = start + t2 * V\n",
    "                break\n",
    "\n",
    "    return first_p, first_i, first_t\n",
    "    \n",
    "    \n",
    "CAPTURE_TIME = 100\n",
    "class F110Env(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(2,))\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(217,), dtype=np.float32)\n",
    "\n",
    "        \n",
    "        work = {'mass': 3.463388126201571, 'lf': 0.15597534362552312, 'tlad': 1.82461887897713965, 'vgain': 0.90338203837889}\n",
    "\n",
    "        with open('./f1tenth_gym/examples/config_example_map.yaml') as file:\n",
    "            conf_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        conf = Namespace(**conf_dict)\n",
    "        self.conf = conf\n",
    "        self.load_waypoints(conf)\n",
    "        self.max_reacquire = 20.\n",
    "\n",
    "        def render_callback(env_renderer):\n",
    "            # custom extra drawing function\n",
    "\n",
    "            e = env_renderer\n",
    "\n",
    "            # update camera to follow car\n",
    "            x = e.cars[0].vertices[::2]\n",
    "            y = e.cars[0].vertices[1::2]\n",
    "            top, bottom, left, right = max(y), min(y), min(x), max(x)\n",
    "            e.score_label.x = left\n",
    "            e.score_label.y = top - 700\n",
    "            e.left = left - 800\n",
    "            e.right = right + 800\n",
    "            e.top = top + 800\n",
    "            e.bottom = bottom - 800\n",
    "\n",
    "\n",
    "        self.env = gym.make('f110_gym:f110-v0', map=conf.map_path, map_ext=conf.map_ext, num_agents=1)\n",
    "        self.env.add_render_callback(render_callback)\n",
    "        self.prev_capture_coord = None\n",
    "        self.reset()\n",
    "        \n",
    "    def load_waypoints(self, conf):\n",
    "        \"\"\"\n",
    "        loads waypoints\n",
    "        \"\"\"\n",
    "        self.waypoints = np.loadtxt(conf.wpt_path, delimiter=conf.wpt_delim, skiprows=conf.wpt_rowskip)\n",
    "\n",
    "        \n",
    "    def _get_current_waypoint(self, waypoints, lookahead_distance, position, theta):\n",
    "        \"\"\"\n",
    "        gets the current waypoint to follow\n",
    "        \"\"\"\n",
    "        wpts = np.vstack((self.waypoints[:, self.conf.wpt_xind], self.waypoints[:, self.conf.wpt_yind])).T\n",
    "        nearest_point, nearest_dist, t, i = nearest_point_on_trajectory(position, wpts)\n",
    "        if nearest_dist < lookahead_distance:\n",
    "            lookahead_point, i2, t2 = first_point_on_trajectory_intersecting_circle(position, lookahead_distance, wpts, i+t, wrap=True)\n",
    "            if i2 == None:\n",
    "                return None\n",
    "            current_waypoint = np.empty((3, ))\n",
    "            # x, y\n",
    "            current_waypoint[0:2] = wpts[i2, :]\n",
    "            # speed\n",
    "            current_waypoint[2] = waypoints[i, self.conf.wpt_vind]\n",
    "            return current_waypoint\n",
    "        elif nearest_dist < self.max_reacquire:\n",
    "            return np.append(wpts[i, :], waypoints[i, self.conf.wpt_vind])\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def reset(self):\n",
    "        obs, step_reward, done, info = self.env.reset(np.array([[self.conf.sx, self.conf.sy, self.conf.stheta]]))\n",
    "        self.prev_capture_coord = [obs['poses_x'][0], obs['poses_y'][0]]\n",
    "        self.time_to_capture = CAPTURE_TIME\n",
    "        self.init_x = 0\n",
    "        return self.to_vector_state(obs)\n",
    "    \n",
    "    def to_vector_state(self, obs):\n",
    "\n",
    "        scanner = np.zeros(1080//5,)\n",
    "        for i in range(1080//5):\n",
    "            scanner[i] = np.clip(np.mean(obs['scans'][0][i*5: i*5+5]), 0, 10)\n",
    "\n",
    "        scanner /= 10\n",
    "        state = np.concatenate([\n",
    "            scanner,\n",
    "            np.array(obs['linear_vels_x'][:1])/5,\n",
    "        ])\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "\n",
    "        action[0] = action[0]*np.pi/4\n",
    "        action[1] = action[1]*2.5 + 2.5\n",
    "        action = action.reshape(1, 2)\n",
    "        action = np.repeat(action, repeats=2, axis=0)\n",
    "        action[1][1] = 0\n",
    "        # print(action)\n",
    "        \n",
    "\n",
    "        obs, step_reward, done, info = self.env.step(action)\n",
    "        \n",
    "        pose_x = obs['poses_x'][0]\n",
    "        pose_y = obs['poses_y'][0]\n",
    "        \n",
    "        position = np.array([pose_x, pose_y])\n",
    "        lookahead_point = self._get_current_waypoint(self.waypoints, 1.8, position, 0.9033)\n",
    "        \n",
    "        print(position, lookahead_point)\n",
    "        \n",
    "        reward = 0\n",
    "        if obs['collisions'][0] == 1.0:\n",
    "            reward = -10\n",
    "        \n",
    "\n",
    "        next_state = self.to_vector_state(obs)\n",
    "        reward += (step_reward + obs['linear_vels_x'][0]*0.01)\n",
    "\n",
    "        self.time_to_capture -= 1\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08e1bb2",
   "metadata": {},
   "source": [
    "### checkpoint handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99d8936d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.F110Env at 0x2abd9b02ff10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import yaml\n",
    "import gym\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "\n",
    "    \n",
    "class F110Env(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        self.action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(1,))\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(362,), dtype=np.float32)\n",
    "        self.min_cp_dist = 2.0\n",
    "        self.cp_reward = 1.0\n",
    "\n",
    "        with open('./f1tenth_gym/examples/config_example_map.yaml') as file:\n",
    "            conf_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        conf = Namespace(**conf_dict)\n",
    "        self.conf = conf\n",
    "        wps = np.loadtxt(conf.wpt_path, delimiter=conf.wpt_delim, skiprows=conf.wpt_rowskip)[:, 1:3]\n",
    "        idxs = [i%10 == 0 for i in range(len(wps))]\n",
    "        self.checkpoints = wps[idxs][1:]\n",
    "        self.t = 0\n",
    "        \n",
    "        \n",
    "        def render_callback(env_renderer):\n",
    "            # custom extra drawing function\n",
    "\n",
    "            e = env_renderer\n",
    "\n",
    "            # update camera to follow car\n",
    "            x = e.cars[0].vertices[::2]\n",
    "            y = e.cars[0].vertices[1::2]\n",
    "            top, bottom, left, right = max(y), min(y), min(x), max(x)\n",
    "            e.score_label.x = left\n",
    "            e.score_label.y = top - 700\n",
    "            e.left = left - 800\n",
    "            e.right = right + 800\n",
    "            e.top = top + 800\n",
    "            e.bottom = bottom - 800\n",
    "\n",
    "\n",
    "        self.env = gym.make('f110_gym:f110-v0', map=conf.map_path, map_ext=conf.map_ext, num_agents=1)\n",
    "        self.env.add_render_callback(render_callback)\n",
    "        self.prev_capture_coord = None\n",
    "        self.reset()\n",
    "        \n",
    "  \n",
    "    def reset(self):\n",
    "        obs, step_reward, done, info = self.env.reset(np.array([[self.conf.sx, self.conf.sy, self.conf.stheta]]))\n",
    "        self.next_cp_idx = 0\n",
    "        self.t = 0\n",
    "        return self.to_vector_state(obs)\n",
    "    \n",
    "    def to_vector_state(self, obs):\n",
    "        buck = 3\n",
    "        scanner = np.zeros(1080//buck,)\n",
    "        for i in range(1080//buck):\n",
    "            scanner[i] = np.clip(np.mean(obs['scans'][0][i*buck: i*buck+buck]), 0, 10)\n",
    "        scanner /= 10\n",
    "        \n",
    "        state = np.concatenate([\n",
    "            scanner,\n",
    "            np.array(obs['linear_vels_x'])/5,\n",
    "            np.array(obs['ang_vels_z'])\n",
    "        ])\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def checkpoint(self, position):\n",
    "        dist = np.linalg.norm(position - self.checkpoints[self.next_cp_idx])\n",
    "        reward = 0\n",
    "        if dist < self.min_cp_dist:\n",
    "#             print(f\"Got to CP {self.next_cp_idx}\")\n",
    "            reward = self.cp_reward\n",
    "    \n",
    "            self.next_cp_idx = (self.next_cp_idx + 1)%len(self.checkpoints)\n",
    "        return reward\n",
    "        \n",
    "    def step(self, action):\n",
    "\n",
    "        angle = action[0]*(np.pi/4)\n",
    "#         action[1] = action[1]*5\n",
    "        speed = 5\n",
    "        act = np.array([[angle, speed]])\n",
    "#         action = np.repeat(action, repeats=2, axis=0)\n",
    "#         action[1][1] = 0\n",
    "        # print(action)\n",
    "        \n",
    "\n",
    "        obs, step_reward, done, info = self.env.step(act)\n",
    "        pose_x = obs['poses_x'][0]\n",
    "        pose_y = obs['poses_y'][0]\n",
    "        \n",
    "        position = np.array([pose_x, pose_y])\n",
    "#         print(action, position)\n",
    "        \n",
    "        \n",
    "        reward = 0\n",
    "        if obs['collisions'][0] == 1.0:\n",
    "            reward = -1\n",
    "            \n",
    "#         if int(self.t+1) % 100 == 0:\n",
    "#             print(self.t+1, position, action[0], angle)\n",
    "        \n",
    "        cp_reward = self.checkpoint(position)\n",
    "        next_state = self.to_vector_state(obs)\n",
    "        reward += cp_reward\n",
    "        self.t += 1\n",
    "\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "F110Env({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fbd2cb",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Env simple wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbf8e2c7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "CAPTURE_TIME = 100\n",
    "class F110Env(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(2,))\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(217,), dtype=np.float32)\n",
    "        \n",
    "        self.env = gym.make(\n",
    "            'f110_gym:f110-v0',\n",
    "            map='./f1tenth_gym/examples/example_map',\n",
    "            map_ext='.png'\n",
    "        )\n",
    "        \n",
    "        self.reset()\n",
    "        self.prev_capture_coord = None\n",
    "        \n",
    "    def reset(self):\n",
    "        obs, step_reward, done, info = self.env.reset(\n",
    "            poses=np.array([[0., 0., 0.], \n",
    "                     [-1., -1., 0.]]) \n",
    "        )\n",
    "        self.prev_capture_coord = [obs['poses_x'][0], obs['poses_y'][0]]\n",
    "        self.time_to_capture = CAPTURE_TIME\n",
    "        self.init_x = 0\n",
    "        return self.to_vector_state(obs)\n",
    "    \n",
    "    def to_vector_state(self, obs):\n",
    "\n",
    "        scanner = np.zeros(1080//5,)\n",
    "        for i in range(1080//5):\n",
    "            scanner[i] = np.clip(np.mean(obs['scans'][0][i*5: i*5+5]), 0, 10)\n",
    "\n",
    "        scanner /= 10\n",
    "        state = np.concatenate([\n",
    "            scanner,\n",
    "            np.array(obs['linear_vels_x'][:1])/5,\n",
    "        ])\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "\n",
    "        action[0] = action[0]*np.pi/4\n",
    "        action[1] = action[1]*2.5 + 2.5\n",
    "        action = action.reshape(1, 2)\n",
    "        action = np.repeat(action, repeats=2, axis=0)\n",
    "        action[1][1] = 0\n",
    "        # print(action)\n",
    "        obs, step_reward, done, info = self.env.step(action)\n",
    "        reward = 0\n",
    "        if obs['collisions'][0] == 1.0:\n",
    "            reward = -10\n",
    "        \n",
    "        moving_forward_rew = 0\n",
    "\n",
    "        if self.time_to_capture == 0:\n",
    "            current_coord = [obs['poses_x'][0], obs['poses_y'][0]]\n",
    "            dist = abs(current_coord[0] - self.prev_capture_coord[0]) + abs(current_coord[1] - self.prev_capture_coord[1])\n",
    "            # print(f\"prev coord:{self.prev_capture_coord}, current_coord:{current_coord}, dist:{dist}\")\n",
    "            \n",
    "            self.prev_capture_coord = current_coord\n",
    "            if dist < 2:\n",
    "                # print(\"Neg reward\")\n",
    "                moving_forward_rew = -10\n",
    "\n",
    "            self.time_to_capture = CAPTURE_TIME + 1\n",
    "\n",
    "        next_state = self.to_vector_state(obs)\n",
    "        # ang_vel = obs['ang_vels_z'][0]\n",
    "        # print(obs['ang_vels_z'][0]*0.1)\n",
    "        reward += (step_reward + obs['linear_vels_x'][0]*0.01)\n",
    "\n",
    "        self.time_to_capture -= 1\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc32abc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde1bf5b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "done = False\n",
    "env = F110Env({'explore':False})\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = np.array([1.0, 0.0])\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311e9da4",
   "metadata": {},
   "source": [
    "# RAY algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1526925b",
   "metadata": {},
   "source": [
    "## ppo continuous actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe30888f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 18:47:39,250\tWARNING trainer.py:2279 -- You have specified 20 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-02-23 18:47:39,250\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-02-23 18:47:39,251\tINFO trainer.py:790 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11951)\u001b[0m 2022-02-23 18:47:45,528\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11966)\u001b[0m 2022-02-23 18:47:45,625\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11976)\u001b[0m 2022-02-23 18:47:45,686\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11980)\u001b[0m 2022-02-23 18:47:45,710\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11977)\u001b[0m 2022-02-23 18:47:45,661\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11981)\u001b[0m 2022-02-23 18:47:45,700\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11963)\u001b[0m 2022-02-23 18:47:45,726\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11962)\u001b[0m 2022-02-23 18:47:45,746\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11941)\u001b[0m 2022-02-23 18:47:45,793\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11970)\u001b[0m 2022-02-23 18:47:45,813\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "2022-02-23 18:47:53,482\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-02-23 18:47:53,483\tWARNING trainer.py:2279 -- You have specified 20 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-02-23 18:47:53,624\tINFO trainable.py:125 -- Trainable.setup took 14.376 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-02-23 18:47:53,625\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-02-23 18:47:53,767\tINFO trainable.py:472 -- Restored on 10.64.91.46 from checkpoint: ./checkpoints/ppo_cp_3/checkpoint_000501/checkpoint-501\n",
      "2022-02-23 18:47:53,767\tINFO trainable.py:480 -- Current state after restoring: {'_iteration': 501, '_timesteps_total': 2004000, '_time_total': 6652.915095567703, '_episodes_total': 797}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11969)\u001b[0m 2022-02-23 18:48:01,596\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11967)\u001b[0m 2022-02-23 18:48:02,191\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11961)\u001b[0m 2022-02-23 18:48:02,334\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11973)\u001b[0m 2022-02-23 18:48:02,812\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11978)\u001b[0m 2022-02-23 18:48:02,759\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11968)\u001b[0m 2022-02-23 18:48:02,996\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11965)\u001b[0m 2022-02-23 18:48:03,028\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11959)\u001b[0m 2022-02-23 18:48:03,147\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11953)\u001b[0m 2022-02-23 18:48:03,113\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11972)\u001b[0m 2022-02-23 18:48:03,121\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11979)\u001b[0m 2022-02-23 18:48:03,129\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11929)\u001b[0m 2022-02-23 18:48:03,196\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11948)\u001b[0m 2022-02-23 18:48:03,167\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11950)\u001b[0m 2022-02-23 18:48:03,193\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11952)\u001b[0m 2022-02-23 18:48:03,168\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11975)\u001b[0m 2022-02-23 18:48:03,184\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11971)\u001b[0m 2022-02-23 18:48:03,178\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11939)\u001b[0m 2022-02-23 18:48:03,206\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11958)\u001b[0m 2022-02-23 18:48:03,187\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11954)\u001b[0m 2022-02-23 18:48:03,209\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 18:48:10,154\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward:4.0\n",
      "checkpoint saved at ./ppo_retrain_cp/checkpoint_000502/checkpoint-502\n",
      "episode: 1 reward:4.0\n",
      "episode: 2 reward:17.0\n",
      "episode: 3 reward:21.2\n",
      "episode: 4 reward:21.916666666666668\n",
      "episode: 5 reward:20.857142857142858\n",
      "episode: 6 reward:24.57894736842105\n",
      "episode: 7 reward:24.695652173913043\n",
      "episode: 8 reward:25.32\n",
      "episode: 9 reward:25.296296296296298\n",
      "episode: 10 reward:25.466666666666665\n",
      "episode: 11 reward:24.914285714285715\n",
      "episode: 12 reward:25.083333333333332\n",
      "episode: 13 reward:26.186046511627907\n",
      "episode: 14 reward:26.177777777777777\n",
      "episode: 15 reward:26.083333333333332\n",
      "episode: 16 reward:25.40740740740741\n",
      "episode: 17 reward:24.810344827586206\n",
      "episode: 18 reward:24.688524590163933\n",
      "episode: 19 reward:24.791044776119403\n",
      "episode: 20 reward:24.32857142857143\n",
      "episode: 21 reward:23.986486486486488\n",
      "episode: 22 reward:23.873417721518987\n",
      "episode: 23 reward:24.02469135802469\n",
      "episode: 24 reward:24.337349397590362\n",
      "episode: 25 reward:24.564705882352943\n",
      "episode: 26 reward:24.862068965517242\n",
      "episode: 27 reward:24.674157303370787\n",
      "episode: 28 reward:25.25\n",
      "episode: 29 reward:25.393617021276597\n",
      "episode: 30 reward:25.34375\n",
      "episode: 31 reward:25.816326530612244\n",
      "episode: 32 reward:26.77\n",
      "episode: 33 reward:27.05\n",
      "episode: 34 reward:27.41\n",
      "episode: 35 reward:27.48\n",
      "episode: 36 reward:27.71\n",
      "episode: 37 reward:29.31\n",
      "episode: 38 reward:30.1\n",
      "episode: 39 reward:30.11\n",
      "episode: 40 reward:29.99\n",
      "episode: 41 reward:29.97\n",
      "episode: 42 reward:30.27\n",
      "episode: 43 reward:30.81\n",
      "episode: 44 reward:30.75\n",
      "episode: 45 reward:30.98\n",
      "episode: 46 reward:30.98\n",
      "episode: 47 reward:31.51\n",
      "episode: 48 reward:31.49\n",
      "episode: 49 reward:31.38\n",
      "episode: 50 reward:31.64\n",
      "checkpoint saved at ./ppo_retrain_cp/checkpoint_000552/checkpoint-552\n",
      "episode: 51 reward:31.74\n",
      "episode: 52 reward:31.48\n",
      "episode: 53 reward:31.98\n",
      "episode: 54 reward:32.82\n",
      "episode: 55 reward:33.19\n",
      "episode: 56 reward:33.14\n",
      "episode: 57 reward:33.47\n",
      "episode: 58 reward:33.81\n",
      "episode: 59 reward:35.29\n",
      "episode: 60 reward:36.19\n",
      "episode: 61 reward:35.75\n",
      "episode: 62 reward:37.92\n",
      "episode: 63 reward:38.15\n",
      "episode: 64 reward:37.85\n",
      "episode: 65 reward:38.13\n",
      "episode: 66 reward:39.49\n",
      "episode: 67 reward:39.41\n",
      "episode: 68 reward:39.46\n",
      "episode: 69 reward:39.31\n",
      "episode: 70 reward:38.6\n",
      "episode: 71 reward:38.77\n",
      "episode: 72 reward:37.95\n",
      "episode: 73 reward:37.46\n",
      "episode: 74 reward:37.5\n",
      "episode: 75 reward:37.28\n",
      "episode: 76 reward:35.94\n",
      "episode: 77 reward:35.47\n",
      "episode: 78 reward:34.79\n",
      "episode: 79 reward:34.79\n",
      "episode: 80 reward:35.17\n",
      "episode: 81 reward:34.69\n",
      "episode: 82 reward:34.57\n",
      "episode: 83 reward:34.18\n",
      "episode: 84 reward:34.53\n",
      "episode: 85 reward:34.55\n",
      "episode: 86 reward:34.3\n",
      "episode: 87 reward:33.8\n",
      "episode: 88 reward:33.35\n",
      "episode: 89 reward:33.1\n",
      "episode: 90 reward:32.92\n",
      "episode: 91 reward:32.07\n",
      "episode: 92 reward:31.55\n",
      "episode: 93 reward:31.29\n",
      "episode: 94 reward:31.78\n",
      "episode: 95 reward:30.87\n",
      "episode: 96 reward:30.0\n",
      "episode: 97 reward:29.63\n",
      "episode: 98 reward:28.94\n",
      "episode: 99 reward:29.08\n",
      "episode: 100 reward:28.79\n",
      "checkpoint saved at ./ppo_retrain_cp/checkpoint_000602/checkpoint-602\n",
      "episode: 101 reward:29.06\n",
      "episode: 102 reward:28.77\n",
      "episode: 103 reward:29.03\n",
      "episode: 104 reward:29.21\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/scratch/9706084/ipykernel_11700/693906877.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"episode: {i} reward:{result['episode_reward_mean']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'episode_reward_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/ray/tune/trainable.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \"\"\"\n\u001b[1;32m    314\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"step() needs to return a dict.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    961\u001b[0m                 \u001b[0;31m# Try to train one step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                     \u001b[0mstep_attempt_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_attempt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m                 \u001b[0;31m# @ray.remote RolloutWorker failure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mRayError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36mstep_attempt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1040\u001b[0m         \u001b[0;31m# No evaluation necessary, just run the next training iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevaluate_this_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m             \u001b[0mstep_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exec_plan_or_training_iteration_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m         \u001b[0;31m# We have to evaluate in this training iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36m_exec_plan_or_training_iteration_fn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1960\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1961\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1962\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_exec_impl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1963\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_filter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"LocalIterator[T]\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mapply_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_filter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"LocalIterator[T]\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mapply_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_flatten\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"LocalIterator[T[0]]\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mapply_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mbase_iterator\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mactive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m                     \u001b[0mfutures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpar_iter_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactive\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                     \u001b[0;31m# Always yield after each round of gets with timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m         \u001b[0;31m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1726\u001b[0;31m         values, debugger_breakpoint = worker.get_objects(\n\u001b[0m\u001b[1;32m   1727\u001b[0m             object_refs, timeout=timeout)\n\u001b[1;32m   1728\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget_objects\u001b[0;34m(self, object_refs, timeout)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mtimeout_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         data_metadata_pairs = self.core_worker.get_objects(\n\u001b[0m\u001b[1;32m    355\u001b[0m             object_refs, self.current_task_id, timeout_ms)\n\u001b[1;32m    356\u001b[0m         \u001b[0mdebugger_breakpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.get_objects\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.agents import ppo, sac, ddpg\n",
    "\n",
    "gpu_configs = {\n",
    "    \"num_workers\": 1,\n",
    "    \"framework\": \"torch\",\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [1024, 512, 128],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": False,\n",
    "    },\n",
    "    \"num_gpus\": 2\n",
    "}\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"num_workers\": 1,\n",
    "    \"framework\": \"torch\",\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [512, 512, 128],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "ppo_config = {\n",
    "    # Should use a critic as a baseline (otherwise don't use value baseline;\n",
    "    # required for using GAE).\n",
    "    \"use_critic\": True,\n",
    "    # If true, use the Generalized Advantage Estimator (GAE)\n",
    "    # with a value function, see https://arxiv.org/pdf/1506.02438.pdf.\n",
    "    \"use_gae\": True,\n",
    "    # The GAE (lambda) parameter.\n",
    "    \"lambda\": 1.0,\n",
    "    # Initial coefficient for KL divergence.\n",
    "    \"kl_coeff\": 0.2,\n",
    "    # Size of batches collected from each worker.\n",
    "    \"rollout_fragment_length\": 200,\n",
    "    # Number of timesteps collected for each SGD round. This defines the size\n",
    "    # of each SGD epoch.\n",
    "    \"train_batch_size\": 4000,\n",
    "    # Total SGD batch size across all devices for SGD. This defines the\n",
    "    # minibatch size within each epoch.\n",
    "    \"sgd_minibatch_size\": 128,\n",
    "    # Whether to shuffle sequences in the batch when training (recommended).\n",
    "    \"shuffle_sequences\": True,\n",
    "    # Number of SGD iterations in each outer loop (i.e., number of epochs to\n",
    "    # execute per train batch).\n",
    "    \"num_sgd_iter\": 30,\n",
    "    # Stepsize of SGD.\n",
    "    \"lr\": 5e-4,\n",
    "    # Learning rate schedule.\n",
    "    \"lr_schedule\": None,\n",
    "    # Coefficient of the value function loss. IMPORTANT: you must tune this if\n",
    "    # you set vf_share_layers=True inside your model's config.\n",
    "    \"vf_loss_coeff\": 1.0,\n",
    "    \"model\": {\n",
    "        # Share layers for value function. If you set this to True, it's\n",
    "        # important to tune vf_loss_coeff.\n",
    "        \"vf_share_layers\": False,\n",
    "    },\n",
    "    # Coefficient of the entropy regularizer.\n",
    "    \"entropy_coeff\": 0.0,\n",
    "    # Decay schedule for the entropy regularizer.\n",
    "    \"entropy_coeff_schedule\": None,\n",
    "    # PPO clip parameter.\n",
    "    \"clip_param\": 0.3,\n",
    "    # Clip param for the value function. Note that this is sensitive to the\n",
    "    # scale of the rewards. If your expected V is large, increase this.\n",
    "    \"vf_clip_param\": 10.0,\n",
    "    # If specified, clip the global norm of gradients by this amount.\n",
    "    \"grad_clip\": None,\n",
    "    # Target value for KL divergence.\n",
    "    \"kl_target\": 0.01,\n",
    "    # Whether to rollout \"complete_episodes\" or \"truncate_episodes\".\n",
    "    \"batch_mode\": \"truncate_episodes\",\n",
    "    # Which observation filter to apply to the observation.\n",
    "    \"observation_filter\": \"NoFilter\",\n",
    "\n",
    "    \"num_workers\": 10,\n",
    "    \"framework\": \"torch\",\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [300, 300],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    \"evaluation_num_workers\": 20,\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": False,\n",
    "    },\n",
    "    \"num_gpus\": 2\n",
    "}\n",
    "\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "trainer = ppo.PPOTrainer(env=F110Env, config=ppo_config)\n",
    "\n",
    "trainer.restore('./checkpoints/ppo_cp_3/checkpoint_000501/checkpoint-501')\n",
    "\n",
    "rewards = []\n",
    "\n",
    "for i in range(10000):\n",
    "\n",
    "    result = trainer.train()\n",
    "    print(f\"episode: {i} reward:{result['episode_reward_mean']}\")\n",
    "    rewards.append(result['episode_reward_mean'])\n",
    "    if i%50 == 0:\n",
    "        cp = trainer.save(\"./ppo_retrain_cp\")\n",
    "        print(\"checkpoint saved at\", cp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1f18b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b66d1174d60>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7ea0126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./ppo_cp_2/checkpoint_000043/checkpoint-43'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save(\"./ppo_cp_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64aa25d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 18:45:37,709\tWARNING trainer.py:2279 -- You have specified 20 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-02-23 18:45:38,006\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "2022-02-23 18:45:38,007\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-02-23 18:45:38,007\tWARNING trainer.py:2279 -- You have specified 20 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-02-23 18:45:41,867\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-02-23 18:45:41,879\tINFO trainable.py:472 -- Restored on 10.64.91.46 from checkpoint: ./checkpoints/ppo_cp_3/checkpoint_000501/checkpoint-501\n",
      "2022-02-23 18:45:41,880\tINFO trainable.py:480 -- Current state after restoring: {'_iteration': 501, '_timesteps_total': 2004000, '_time_total': 6652.915095567703, '_episodes_total': 797}\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.agents import ppo, sac, ddpg\n",
    "\n",
    "config = {\n",
    "    \n",
    "    \"num_workers\": 0,\n",
    "    \"framework\": \"torch\",\n",
    "    \"evaluation_num_workers\": 20,\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": False,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [300, 300],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    \"num_gpus\": 0\n",
    "}\n",
    "\n",
    "\n",
    "trainer = ppo.PPOTrainer(env=F110Env, config=config)\n",
    "trainer.restore('./checkpoints/ppo_cp_3/checkpoint_000501/checkpoint-501')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e6cdb1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchDisplayException",
     "evalue": "Cannot connect to \"None\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchDisplayException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/scratch/9706084/ipykernel_9829/259374577.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/9706084/ipykernel_9829/70690.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0mF110Env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/f110/f1tenth_gym/gym/f110_gym/envs/f110_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mF110Env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;31m# first call, initialize everything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mf110_gym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrendering\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnvRenderer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m             \u001b[0mF110Env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvRenderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWINDOW_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWINDOW_H\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0mF110Env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_ext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/f110/f1tenth_gym/gym/f110_gym/envs/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# opengl stuff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# other\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;31m# trickery is for circular import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0m_pyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pyglet_doc_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m     \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m     \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_shadow_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0m_shadow_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0m_shadow_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_handlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_can_detect_autorepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, file_drops, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m             \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/pyglet/canvas/__init__.py\u001b[0m in \u001b[0;36mget_display\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Otherwise, create a new display and return it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/academic/kjoseph/navid/anaconda3/envs/py38/lib/python3.8/site-packages/pyglet/canvas/xlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, x_screen)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXOpenDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchDisplayException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot connect to \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mscreen_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXScreenCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchDisplayException\u001b[0m: Cannot connect to \"None\""
     ]
    }
   ],
   "source": [
    "done = False\n",
    "env = F110Env({'explore':False})\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = trainer.compute_action(obs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fbaea3",
   "metadata": {},
   "source": [
    "## discrete action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5da93e",
   "metadata": {},
   "source": [
    "### define environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eef26d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "number of waypoints:79\n",
      "using map:./f1tenth_gym/examples/hard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.F110Env at 0x2aebbbf326d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import yaml\n",
    "import gym\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "\n",
    "    \n",
    "class F110Env(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        \"\"\"\n",
    "        break: [0., 0.]\n",
    "        fast forward: [0., 5.]\n",
    "        fast left: [-pi/4, 5.]\n",
    "        fast right: [pi/4, 5]\n",
    "        slow left: [-pi/4, 2] #later\n",
    "        slow right: [pi/4, 2] \n",
    "        \n",
    "        \"\"\"\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(364,), dtype=np.float32)\n",
    "        self.min_cp_dist = 2.0\n",
    "        self.cp_reward = 1.0\n",
    "        self.max_v = 5.0\n",
    "        \n",
    "        self.action_map = {\n",
    "            0: [0., 0.],\n",
    "            1: [0., self.max_v],\n",
    "            2: [-np.pi/4, self.max_v],\n",
    "            3: [np.pi/4, self.max_v],\n",
    "            4: [-np.pi/6, self.max_v],\n",
    "            5: [np.pi/6, self.max_v],\n",
    "            6: [-np.pi/4, self.max_v/2], #later\n",
    "            7: [np.pi/4, self.max_v/2] #later\n",
    "        }\n",
    "        self.action_space = gym.spaces.Discrete(len(self.action_map),)\n",
    "        \n",
    "\n",
    "        with open('./f1tenth_gym/examples/config_example_map.yaml') as file:\n",
    "            conf_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        conf = Namespace(**conf_dict)\n",
    "        self.conf = conf\n",
    "        wps = np.loadtxt(conf.wpt_path, delimiter=conf.wpt_delim, skiprows=0)[:, 1:3]\n",
    "        idxs = [i%10 == 0 for i in range(len(wps))]\n",
    "        \n",
    "        self.min_x, self.max_x = np.min(wps[:,0]), np.max(wps[:, 0])\n",
    "        self.min_y, self.max_y = np.min(wps[:,1]), np.max(wps[:, 1])\n",
    "\n",
    "        self.checkpoints = wps[idxs]\n",
    "        self.t = 0\n",
    "        \n",
    "        print(f\"x and y range: {self.min_x}to{self.max_x} and {self.min_y}to{self.max_y}\")\n",
    "        print(f\"number of waypoints:{len(self.checkpoints)}\")        \n",
    "        \n",
    "        def render_callback(env_renderer):\n",
    "            # custom extra drawing function\n",
    "\n",
    "            e = env_renderer\n",
    "\n",
    "            # update camera to follow car\n",
    "            x = e.cars[0].vertices[::2]\n",
    "            y = e.cars[0].vertices[1::2]\n",
    "            top, bottom, left, right = max(y), min(y), min(x), max(x)\n",
    "            e.score_label.x = left\n",
    "            e.score_label.y = top - 700\n",
    "            e.left = left - 800\n",
    "            e.right = right + 800\n",
    "            e.top = top + 800\n",
    "            e.bottom = bottom - 800\n",
    "\n",
    "        print(f\"using map:{conf.map_path}\")\n",
    "        self.env = gym.make('f110_gym:f110-v0', map=conf.map_path, map_ext=conf.map_ext, num_agents=1)\n",
    "        self.env.add_render_callback(render_callback)\n",
    "        self.prev_capture_coord = None\n",
    "        self.reset()\n",
    "        \n",
    "  \n",
    "    def reset(self):\n",
    "        obs, step_reward, done, info = self.env.reset(np.array([[self.conf.sx, self.conf.sy, self.conf.stheta]]))\n",
    "#         obs, step_reward, done, info = self.env.reset(np.array([[0.0, 3, 1.55]]))\n",
    "        \n",
    "        self.next_cp_idx = 1\n",
    "        self.t = 0\n",
    "        return self.to_vector_state(obs)\n",
    "    \n",
    "    def to_vector_state(self, obs):\n",
    "        scanner = obs['scans'][0]\n",
    "        \n",
    "        buck = 3\n",
    "        size = 1080//buck\n",
    "        scanner = np.zeros(size,)\n",
    "        for i in range(size):\n",
    "            scanner[i] = np.clip(np.mean(obs['scans'][0][i*buck: i*buck+buck]), 0, 10)\n",
    "        \n",
    "        scanner /= 10.0\n",
    "        \n",
    "        state = np.concatenate([\n",
    "            scanner,\n",
    "            np.array(obs['linear_vels_x'][:1])/self.max_v,\n",
    "            np.array(obs['ang_vels_z'][:1])/2,\n",
    "            np.array(obs['poses_x'][:1]/(self.max_x-self.min_x)),\n",
    "            np.array(obs['poses_y'][:1]/(self.max_y-self.min_y))\n",
    "        ])\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def checkpoint(self, position):\n",
    "        dist = np.linalg.norm(position - self.checkpoints[self.next_cp_idx])\n",
    "        reward = 0\n",
    "        if dist < self.min_cp_dist:\n",
    "#             print(f\"Got to CP {self.next_cp_idx}\")\n",
    "            reward = self.cp_reward\n",
    "    \n",
    "            self.next_cp_idx = (self.next_cp_idx + 1)%len(self.checkpoints)\n",
    "        return reward\n",
    "        \n",
    "    def step(self, action):\n",
    "\n",
    "        act = np.array([self.action_map[action]])\n",
    "        \n",
    "        obs, step_reward, done, info = self.env.step(act)\n",
    "        pose_x = obs['poses_x'][0]\n",
    "        pose_y = obs['poses_y'][0]\n",
    "        \n",
    "        position = np.array([pose_x, pose_y])\n",
    "        \n",
    "        reward = 0\n",
    "        if obs['collisions'][0] == 1.0:\n",
    "            reward = -1\n",
    "            \n",
    "#         if int(self.t+1) % 100 == 0:\n",
    "#             print(action)\n",
    "        \n",
    "        cp_reward = self.checkpoint(position)\n",
    "        next_state = self.to_vector_state(obs)\n",
    "        reward += cp_reward\n",
    "        self.t += 1\n",
    "\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "F110Env({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a5dd80",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c50c87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 22:01:22,311\tWARNING ppo.py:223 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=30 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 133.\n",
      "2022-02-23 22:01:22,312\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-02-23 22:01:22,312\tINFO trainer.py:790 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=44330)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44330)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44330)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44373)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44373)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44373)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44351)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44351)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44351)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44360)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44360)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44360)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44353)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44353)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44353)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44363)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44363)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44363)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44377)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44377)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44377)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44369)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44369)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44369)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44375)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44375)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44375)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44361)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44361)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44361)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44374)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44374)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44374)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44357)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44357)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44357)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44343)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44343)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44343)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44376)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44376)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44376)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44364)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44364)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44364)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44371)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44371)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44371)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44333)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44333)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44333)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44337)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44337)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44337)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44370)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44370)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44370)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44325)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44325)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44325)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44334)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44334)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44334)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44335)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44335)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44335)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44336)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44336)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44336)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44367)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44367)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44367)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44332)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44332)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44332)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44356)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44356)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44356)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44368)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44368)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44368)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44362)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44362)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44362)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44359)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44359)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44359)\u001b[0m using map:./f1tenth_gym/examples/hard\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44372)\u001b[0m x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44372)\u001b[0m number of waypoints:79\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44372)\u001b[0m using map:./f1tenth_gym/examples/hard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=44330)\u001b[0m 2022-02-23 22:01:35,449\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44373)\u001b[0m 2022-02-23 22:01:35,489\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44363)\u001b[0m 2022-02-23 22:01:35,464\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44375)\u001b[0m 2022-02-23 22:01:35,425\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44357)\u001b[0m 2022-02-23 22:01:35,427\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44343)\u001b[0m 2022-02-23 22:01:35,448\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44376)\u001b[0m 2022-02-23 22:01:35,488\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44333)\u001b[0m 2022-02-23 22:01:35,460\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44325)\u001b[0m 2022-02-23 22:01:35,488\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44335)\u001b[0m 2022-02-23 22:01:35,427\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44367)\u001b[0m 2022-02-23 22:01:35,455\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44369)\u001b[0m 2022-02-23 22:01:35,557\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44371)\u001b[0m 2022-02-23 22:01:35,580\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44332)\u001b[0m 2022-02-23 22:01:35,608\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44368)\u001b[0m 2022-02-23 22:01:35,593\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44372)\u001b[0m 2022-02-23 22:01:35,524\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44360)\u001b[0m 2022-02-23 22:01:35,686\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44356)\u001b[0m 2022-02-23 22:01:35,632\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44359)\u001b[0m 2022-02-23 22:01:35,693\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44351)\u001b[0m 2022-02-23 22:01:35,727\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44353)\u001b[0m 2022-02-23 22:01:35,790\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44377)\u001b[0m 2022-02-23 22:01:35,737\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44361)\u001b[0m 2022-02-23 22:01:35,809\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44374)\u001b[0m 2022-02-23 22:01:35,809\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44364)\u001b[0m 2022-02-23 22:01:35,777\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44337)\u001b[0m 2022-02-23 22:01:35,800\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44370)\u001b[0m 2022-02-23 22:01:35,749\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44334)\u001b[0m 2022-02-23 22:01:35,803\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44336)\u001b[0m 2022-02-23 22:01:35,749\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44362)\u001b[0m 2022-02-23 22:01:35,718\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 22:01:43,397\tINFO trainable.py:125 -- Trainable.setup took 21.087 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-02-23 22:01:43,400\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-02-23 22:02:06,026\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward:1.53125\n",
      "checkpoint saved at ./checkpoints/ppo_disc_hard_cp2/checkpoint_000001/checkpoint-1\n",
      "episode: 1 reward:1.9344262295081966\n",
      "episode: 2 reward:2.3493975903614457\n",
      "episode: 3 reward:3.02\n",
      "episode: 4 reward:3.81\n",
      "episode: 5 reward:4.35\n",
      "episode: 6 reward:5.11\n",
      "episode: 7 reward:5.77\n",
      "episode: 8 reward:6.23\n",
      "episode: 9 reward:6.74\n",
      "episode: 10 reward:6.84\n",
      "episode: 11 reward:7.12\n",
      "episode: 12 reward:7.87\n",
      "episode: 13 reward:9.0\n",
      "episode: 14 reward:9.48\n",
      "episode: 15 reward:9.67\n",
      "episode: 16 reward:10.61\n",
      "episode: 17 reward:10.66\n",
      "episode: 18 reward:11.68\n",
      "episode: 19 reward:12.68\n",
      "episode: 20 reward:13.27\n",
      "episode: 21 reward:13.41\n",
      "episode: 22 reward:14.04\n",
      "episode: 23 reward:14.97\n",
      "episode: 24 reward:16.05\n",
      "episode: 25 reward:16.18\n",
      "episode: 26 reward:16.35\n",
      "episode: 27 reward:16.54\n",
      "episode: 28 reward:16.41\n",
      "episode: 29 reward:16.64\n",
      "episode: 30 reward:16.71\n",
      "episode: 31 reward:16.86\n",
      "episode: 32 reward:16.77\n",
      "episode: 33 reward:16.55\n",
      "episode: 34 reward:16.7\n",
      "episode: 35 reward:16.72\n",
      "episode: 36 reward:16.77\n",
      "episode: 37 reward:16.79\n",
      "episode: 38 reward:16.94\n",
      "episode: 39 reward:16.82\n",
      "episode: 40 reward:16.92\n",
      "episode: 41 reward:17.05\n",
      "episode: 42 reward:17.03\n",
      "episode: 43 reward:17.15\n",
      "episode: 44 reward:17.42\n",
      "episode: 45 reward:17.43\n",
      "episode: 46 reward:17.68\n",
      "episode: 47 reward:17.7\n",
      "episode: 48 reward:17.81\n",
      "episode: 49 reward:17.69\n",
      "episode: 50 reward:17.69\n",
      "checkpoint saved at ./checkpoints/ppo_disc_hard_cp2/checkpoint_000051/checkpoint-51\n",
      "episode: 51 reward:18.23\n",
      "episode: 52 reward:18.36\n",
      "episode: 53 reward:18.52\n",
      "episode: 54 reward:19.12\n",
      "episode: 55 reward:19.25\n",
      "episode: 56 reward:19.15\n",
      "episode: 57 reward:18.79\n",
      "episode: 58 reward:18.73\n",
      "episode: 59 reward:18.35\n",
      "episode: 60 reward:18.5\n",
      "episode: 61 reward:18.88\n",
      "episode: 62 reward:18.88\n",
      "episode: 63 reward:19.15\n",
      "episode: 64 reward:19.04\n",
      "episode: 65 reward:20.85\n",
      "episode: 66 reward:21.68\n",
      "episode: 67 reward:21.66\n",
      "episode: 68 reward:21.93\n",
      "episode: 69 reward:21.63\n",
      "episode: 70 reward:22.06\n",
      "episode: 71 reward:21.66\n",
      "episode: 72 reward:21.96\n",
      "episode: 73 reward:21.89\n",
      "episode: 74 reward:21.79\n",
      "episode: 75 reward:21.79\n",
      "episode: 76 reward:21.86\n",
      "episode: 77 reward:22.09\n",
      "episode: 78 reward:21.85\n",
      "episode: 79 reward:21.71\n",
      "episode: 80 reward:22.22\n",
      "episode: 81 reward:20.54\n",
      "episode: 82 reward:19.92\n",
      "episode: 83 reward:19.65\n",
      "episode: 84 reward:19.02\n",
      "episode: 85 reward:19.65\n",
      "episode: 86 reward:19.78\n",
      "episode: 87 reward:19.37\n",
      "episode: 88 reward:19.37\n",
      "episode: 89 reward:19.42\n",
      "episode: 90 reward:20.06\n",
      "episode: 91 reward:20.31\n",
      "episode: 92 reward:20.72\n",
      "episode: 93 reward:20.92\n",
      "episode: 94 reward:21.0\n",
      "episode: 95 reward:21.16\n",
      "episode: 96 reward:21.08\n",
      "episode: 97 reward:21.03\n",
      "episode: 98 reward:21.32\n",
      "episode: 99 reward:20.85\n",
      "episode: 100 reward:20.67\n",
      "checkpoint saved at ./checkpoints/ppo_disc_hard_cp2/checkpoint_000101/checkpoint-101\n",
      "episode: 101 reward:21.15\n",
      "episode: 102 reward:20.5\n",
      "episode: 103 reward:20.3\n",
      "episode: 104 reward:20.66\n",
      "episode: 105 reward:20.49\n",
      "episode: 106 reward:21.01\n",
      "episode: 107 reward:20.99\n",
      "episode: 108 reward:20.97\n",
      "episode: 109 reward:21.24\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.agents import ppo, sac, ddpg, dqn\n",
    "from ray.rllib.agents.dqn.apex import APEX_DEFAULT_CONFIG\n",
    "from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "\n",
    "DEFAULT_CONFIG['framework'] = 'torch'\n",
    "DEFAULT_CONFIG['num_workers'] = 30\n",
    "DEFAULT_CONFIG['num_gpus'] = 2\n",
    "DEFAULT_CONFIG['model']['fcnet_hiddens'] = [1024, 1024]\n",
    "\n",
    "\n",
    "APEX_DEFAULT_CONFIG['framework'] = 'torch'\n",
    "APEX_DEFAULT_CONFIG['num_gpus'] = 1\n",
    "APEX_DEFAULT_CONFIG['exploration_config']['epsilon_timesteps'] = int(1e8)\n",
    "APEX_DEFAULT_CONFIG['model']['fcnet_hiddens'] = [1024, 1024]\n",
    "APEX_DEFAULT_CONFIG['final_epsilon'] = 0.1\n",
    "APEX_DEFAULT_CONFIG['buffer_size'] =  1000000\n",
    "\n",
    "# trainer = dqn.ApexTrainer(env=F110Env, config=APEX_DEFAULT_CONFIG)\n",
    "trainer = ppo.PPOTrainer(env=F110Env, config=DEFAULT_CONFIG)\n",
    "rewards = []\n",
    "\n",
    "import pickle\n",
    "\n",
    "for i in range(10000):\n",
    "    result = trainer.train()\n",
    "    print(f\"episode: {i} reward:{result['episode_reward_mean']}\")\n",
    "    rewards.append(result['episode_reward_mean'])\n",
    "    if i%50 == 0:\n",
    "        with open('./checkpoints/ppo_disc_r2', 'wb') as f:\n",
    "            pickle.dump(rewards, f)\n",
    "        cp = trainer.save(\"./checkpoints/ppo_disc_hard_cp2\")\n",
    "        print(\"checkpoint saved at\", cp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "568a5022",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_workers': 32,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'create_env_on_driver': False,\n",
       " 'rollout_fragment_length': 50,\n",
       " 'batch_mode': 'truncate_episodes',\n",
       " 'gamma': 0.99,\n",
       " 'lr': 0.0005,\n",
       " 'train_batch_size': 512,\n",
       " 'model': {'_use_default_native_models': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  'fcnet_hiddens': [256, 256],\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'post_fcnet_hiddens': [],\n",
       "  'post_fcnet_activation': 'relu',\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': True,\n",
       "  'use_lstm': False,\n",
       "  'max_seq_len': 20,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action': False,\n",
       "  'lstm_use_prev_reward': False,\n",
       "  '_time_major': False,\n",
       "  'use_attention': False,\n",
       "  'attention_num_transformer_units': 1,\n",
       "  'attention_dim': 64,\n",
       "  'attention_num_heads': 1,\n",
       "  'attention_head_dim': 32,\n",
       "  'attention_memory_inference': 50,\n",
       "  'attention_memory_training': 50,\n",
       "  'attention_position_wise_mlp_dim': 32,\n",
       "  'attention_init_gru_gate_bias': 2.0,\n",
       "  'attention_use_n_prev_actions': 0,\n",
       "  'attention_use_n_prev_rewards': 0,\n",
       "  'framestack': True,\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None,\n",
       "  'lstm_use_prev_action_reward': -1},\n",
       " 'optimizer': {'max_weight_sync_delay': 400,\n",
       "  'num_replay_buffer_shards': 4,\n",
       "  'debug': False},\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'env': None,\n",
       " 'observation_space': None,\n",
       " 'action_space': None,\n",
       " 'env_config': {},\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'env_task_fn': None,\n",
       " 'render_env': False,\n",
       " 'record_env': False,\n",
       " 'clip_rewards': None,\n",
       " 'normalize_actions': True,\n",
       " 'clip_actions': False,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'log_level': 'WARN',\n",
       " 'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       " 'ignore_worker_failures': False,\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'framework': 'torch',\n",
       " 'eager_tracing': False,\n",
       " 'eager_max_retraces': 20,\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'PerWorkerEpsilonGreedy',\n",
       "  'initial_epsilon': 1.0,\n",
       "  'final_epsilon': 0.02,\n",
       "  'epsilon_timesteps': 10000},\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_duration': 10,\n",
       " 'evaluation_duration_unit': 'episodes',\n",
       " 'evaluation_parallel_to_training': False,\n",
       " 'in_evaluation': False,\n",
       " 'evaluation_config': {'explore': False},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'custom_eval_function': None,\n",
       " 'always_attach_evaluation_results': False,\n",
       " 'sample_async': False,\n",
       " 'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'compress_observations': False,\n",
       " 'metrics_episode_collection_timeout_s': 180,\n",
       " 'metrics_num_episodes_for_smoothing': 100,\n",
       " 'min_time_s_per_reporting': None,\n",
       " 'min_train_timesteps_per_reporting': None,\n",
       " 'min_sample_timesteps_per_reporting': None,\n",
       " 'seed': None,\n",
       " 'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_gpus': 2,\n",
       " '_fake_gpus': False,\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'num_cpus_for_driver': 1,\n",
       " 'placement_strategy': 'PACK',\n",
       " 'input': 'sampler',\n",
       " 'input_config': {},\n",
       " 'actions_in_input_normalized': False,\n",
       " 'input_evaluation': ['is', 'wis'],\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'multiagent': {'policies': {},\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_map_cache': None,\n",
       "  'policy_mapping_fn': None,\n",
       "  'policies_to_train': None,\n",
       "  'observation_fn': None,\n",
       "  'replay_mode': 'independent',\n",
       "  'count_steps_by': 'env_steps'},\n",
       " 'logger_config': None,\n",
       " '_tf_policy_handles_more_than_one_loss': False,\n",
       " '_disable_preprocessor_api': False,\n",
       " '_disable_action_flattening': False,\n",
       " '_disable_execution_plan_api': False,\n",
       " 'simple_optimizer': -1,\n",
       " 'monitor': -1,\n",
       " 'evaluation_num_episodes': -1,\n",
       " 'metrics_smoothing_episodes': -1,\n",
       " 'timesteps_per_iteration': 25000,\n",
       " 'min_iter_time_s': 30,\n",
       " 'collect_metrics_timeout': -1,\n",
       " 'target_network_update_freq': 500000,\n",
       " 'buffer_size': 2000000,\n",
       " 'replay_buffer_config': None,\n",
       " 'store_buffer_in_checkpoints': False,\n",
       " 'replay_sequence_length': 1,\n",
       " 'lr_schedule': None,\n",
       " 'adam_epsilon': 1e-08,\n",
       " 'grad_clip': 40,\n",
       " 'learning_starts': 50000,\n",
       " 'num_atoms': 1,\n",
       " 'v_min': -10.0,\n",
       " 'v_max': 10.0,\n",
       " 'noisy': False,\n",
       " 'sigma0': 0.5,\n",
       " 'dueling': True,\n",
       " 'hiddens': [256],\n",
       " 'double_q': True,\n",
       " 'n_step': 3,\n",
       " 'prioritized_replay': True,\n",
       " 'prioritized_replay_alpha': 0.6,\n",
       " 'prioritized_replay_beta': 0.4,\n",
       " 'final_prioritized_replay_beta': 0.4,\n",
       " 'prioritized_replay_beta_annealing_timesteps': 20000,\n",
       " 'prioritized_replay_eps': 1e-06,\n",
       " 'before_learn_on_batch': None,\n",
       " 'training_intensity': None,\n",
       " 'worker_side_prioritization': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=1477)\u001b[0m 2022-02-19 17:34:53,395\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1486)\u001b[0m 2022-02-19 17:34:53,556\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1506)\u001b[0m 2022-02-19 17:34:53,728\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1480)\u001b[0m 2022-02-19 17:34:53,666\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1455)\u001b[0m 2022-02-19 17:34:53,676\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1495)\u001b[0m 2022-02-19 17:34:53,740\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1501)\u001b[0m 2022-02-19 17:34:53,727\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1465)\u001b[0m 2022-02-19 17:34:53,738\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1491)\u001b[0m 2022-02-19 17:34:53,672\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1457)\u001b[0m 2022-02-19 17:34:53,724\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1494)\u001b[0m 2022-02-19 17:34:53,727\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1462)\u001b[0m 2022-02-19 17:34:53,737\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1468)\u001b[0m 2022-02-19 17:34:53,809\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1509)\u001b[0m 2022-02-19 17:34:53,797\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1502)\u001b[0m 2022-02-19 17:34:53,811\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1493)\u001b[0m 2022-02-19 17:34:53,758\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1479)\u001b[0m 2022-02-19 17:34:53,818\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1454)\u001b[0m 2022-02-19 17:34:53,777\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1461)\u001b[0m 2022-02-19 17:34:53,802\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1472)\u001b[0m 2022-02-19 17:34:53,755\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1487)\u001b[0m 2022-02-19 17:34:53,763\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1481)\u001b[0m 2022-02-19 17:34:53,789\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1453)\u001b[0m 2022-02-19 17:34:53,803\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1508)\u001b[0m 2022-02-19 17:34:53,765\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1464)\u001b[0m 2022-02-19 17:34:53,845\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1466)\u001b[0m 2022-02-19 17:34:53,789\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1460)\u001b[0m 2022-02-19 17:34:53,846\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1463)\u001b[0m 2022-02-19 17:34:53,821\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1467)\u001b[0m 2022-02-19 17:34:53,871\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1458)\u001b[0m 2022-02-19 17:34:53,881\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1482)\u001b[0m 2022-02-19 17:34:53,867\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1483)\u001b[0m 2022-02-19 17:34:53,869\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "APEX_DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e25b554a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "number of waypoints:79\n",
      "using map:./f1tenth_gym/examples/hard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 22:43:01,861\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-02-23 22:43:01,909\tINFO trainable.py:472 -- Restored on 128.205.218.23 from checkpoint: ./checkpoints/ppo_disc_hard_cp2/checkpoint_000101/checkpoint-101\n",
      "2022-02-23 22:43:01,910\tINFO trainable.py:480 -- Current state after restoring: {'_iteration': 101, '_timesteps_total': 805980, '_time_total': 2203.026338815689, '_episodes_total': 793}\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.agents import ppo, sac, ddpg, dqn\n",
    "from ray.rllib.agents.dqn.apex import APEX_DEFAULT_CONFIG\n",
    "from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG\n",
    "\n",
    "DEFAULT_CONFIG['framework'] = 'torch'\n",
    "DEFAULT_CONFIG['num_workers'] = 0\n",
    "DEFAULT_CONFIG['num_gpus'] = 0\n",
    "DEFAULT_CONFIG['model']['fcnet_hiddens'] = [1024, 1024]\n",
    "\n",
    "\n",
    "APEX_DEFAULT_CONFIG['framework'] = 'torch'\n",
    "APEX_DEFAULT_CONFIG['num_gpus'] = 0\n",
    "APEX_DEFAULT_CONFIG['num_workers'] = 0\n",
    "APEX_DEFAULT_CONFIG['exploration_config']['epsilon_timesteps'] = int(1e8)\n",
    "APEX_DEFAULT_CONFIG['model']['fcnet_hiddens'] = [1024, 1024]\n",
    "APEX_DEFAULT_CONFIG['final_epsilon'] = 0.1\n",
    "\n",
    "\n",
    "trainer = ppo.PPOTrainer(env=F110Env, config=DEFAULT_CONFIG)\n",
    "trainer.restore('./checkpoints/ppo_disc_hard_cp2/checkpoint_000051/checkpoint-51')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bc61f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "number of waypoints:79\n",
      "using map:./f1tenth_gym/examples/hard\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "env = F110Env({'explore':False})\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = trainer.compute_action(obs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a46ef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x and y range: -52.1124185to1.0823861 and -13.8182946to25.381983\n",
      "number of waypoints:79\n",
      "using map:./f1tenth_gym/examples/hard\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "env = F110Env({'explore':False})\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    policy = trainer.get_policy()\n",
    "    action, _, info = policy.compute_single_action(obs)\n",
    "    logits = info['action_dist_inputs']\n",
    "    best_action_id = logits.argmax() # deterministic\n",
    "    obs, reward, done, _ = env.step(best_action_id)\n",
    "    env.render()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d748a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
