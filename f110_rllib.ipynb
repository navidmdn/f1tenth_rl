{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1bd4eb",
   "metadata": {},
   "source": [
    "# F1tenth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b781558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f07c65",
   "metadata": {},
   "source": [
    "## Environment playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e382dbcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# racecar_env = gym.make(\n",
    "#     'f110_gym:f110-v0',\n",
    "#     map='./f1tenth_gym/gym/f110_gym/envs/maps/vegas',\n",
    "#     map_ext='.png'\n",
    "# )\n",
    "\n",
    "racecar_env = gym.make(\n",
    "    'f110_gym:f110-v0',\n",
    "    map='./f1tenth_gym/examples/example_map',\n",
    "    map_ext='.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acba6632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ego_idx': 0,\n",
       " 'scans': [array([29.98576175, 30.01263728, 29.99129338, ..., 29.99864406,\n",
       "         29.9889291 , 30.01278365]),\n",
       "  array([29.98576175, 30.01263728, 29.99129338, ..., 29.99864406,\n",
       "         29.9889291 , 30.01278365])],\n",
       " 'poses_x': [0.0, 2.0],\n",
       " 'poses_y': [0.0, 0.0],\n",
       " 'poses_theta': [0.0, 0.0],\n",
       " 'linear_vels_x': [0.0, 0.0],\n",
       " 'linear_vels_y': [0.0, 0.0],\n",
       " 'ang_vels_z': [0.0, 0.0],\n",
       " 'collisions': array([0., 0.]),\n",
       " 'lap_times': array([0.01, 0.01]),\n",
       " 'lap_counts': array([0., 0.])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, step_reward, done, info = racecar_env.reset(\n",
    "    poses=np.array([[0., 0., 0.], # pose of ego\n",
    "             [2., 0., 0.]])  # pose of 2nd agent\n",
    ") \n",
    "\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2df1290a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1082,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = np.concatenate([\n",
    "    obs['scans'][0],\n",
    "    np.array(obs['linear_vels_x'][:1]),\n",
    "    np.array(obs['linear_vels_y'][:1]),\n",
    "])\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54349ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0596855 ,  0.44670051],\n",
       "       [-0.05675966,  1.8354974 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeds = np.random.rand(2, 1)*2\n",
    "pi_4 = 3.1415/8\n",
    "pi_2 = 3.1415/4\n",
    "angles = np.random.rand(2, 1)*pi_2-pi_4\n",
    "actions = np.concatenate([angles, speeds], axis=1)\n",
    "\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "133a6cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## action consists of ndarray(num_agent, 2) 0: steering angle 1: velocity\n",
    "## the reward function is only for the first agent\n",
    "\n",
    "import time\n",
    "import gym \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "racecar_env = gym.make(\n",
    "    'f110_gym:f110-v0',\n",
    "    map='./f1tenth_gym/examples/example_map',\n",
    "    map_ext='.png',\n",
    "    num_agents=1\n",
    ")\n",
    "steps = 0\n",
    "\n",
    "obs, step_reward, done, info = racecar_env.reset(\n",
    "    poses=np.array([[0., 0., 1.5]]) \n",
    ") \n",
    "\n",
    "rewards = []\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    speeds = np.random.rand(2, 1)*20\n",
    "    pi_4 = 3.1415/8\n",
    "    pi_2 = 3.1415/4\n",
    "    speeds[1][0] = 0.1\n",
    "    angles = np.random.rand(2, 1)*pi_2-pi_4\n",
    "    actions = np.concatenate([angles, speeds], axis=1)\n",
    "\n",
    "    obs, step_reward, done, info = racecar_env.step(actions)\n",
    "    rewards.append(step_reward)\n",
    "    \n",
    "    racecar_env.render()\n",
    "    steps += 1\n",
    "    \n",
    "    if steps > 500:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa1c0a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa25f6c8460>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab60a5e8",
   "metadata": {},
   "source": [
    "## Define environment wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2389d472",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### waypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a39e4e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import yaml\n",
    "import gym\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "\n",
    "from numba import njit\n",
    "\n",
    "from pyglet.gl import GL_POINTS\n",
    "\n",
    "\n",
    "# @njit(fastmath=False, cache=True)\n",
    "def nearest_point_on_trajectory(point, trajectory):\n",
    "    \"\"\"\n",
    "    Return the nearest point along the given piecewise linear trajectory.\n",
    "\n",
    "    Same as nearest_point_on_line_segment, but vectorized. This method is quite fast, time constraints should\n",
    "    not be an issue so long as trajectories are not insanely long.\n",
    "\n",
    "        Order of magnitude: trajectory length: 1000 --> 0.0002 second computation (5000fps)\n",
    "\n",
    "    point: size 2 numpy array\n",
    "    trajectory: Nx2 matrix of (x,y) trajectory waypoints\n",
    "        - these must be unique. If they are not unique, a divide by 0 error will destroy the world\n",
    "    \"\"\"\n",
    "    diffs = trajectory[1:,:] - trajectory[:-1,:]\n",
    "    l2s   = diffs[:,0]**2 + diffs[:,1]**2\n",
    "    # this is equivalent to the elementwise dot product\n",
    "    # dots = np.sum((point - trajectory[:-1,:]) * diffs[:,:], axis=1)\n",
    "    dots = np.empty((trajectory.shape[0]-1, ))\n",
    "    for i in range(dots.shape[0]):\n",
    "        dots[i] = np.dot((point - trajectory[i, :]), diffs[i, :])\n",
    "    t = dots / l2s\n",
    "    t[t<0.0] = 0.0\n",
    "    t[t>1.0] = 1.0\n",
    "    # t = np.clip(dots / l2s, 0.0, 1.0)\n",
    "    projections = trajectory[:-1,:] + (t*diffs.T).T\n",
    "    # dists = np.linalg.norm(point - projections, axis=1)\n",
    "    dists = np.empty((projections.shape[0],))\n",
    "    for i in range(dists.shape[0]):\n",
    "        temp = point - projections[i]\n",
    "        dists[i] = np.sqrt(np.sum(temp*temp))\n",
    "    min_dist_segment = np.argmin(dists)\n",
    "    return projections[min_dist_segment], dists[min_dist_segment], t[min_dist_segment], min_dist_segment\n",
    "\n",
    "# @njit(fastmath=False, cache=True)\n",
    "def first_point_on_trajectory_intersecting_circle(point, radius, trajectory, t=0.0, wrap=False):\n",
    "    \"\"\"\n",
    "    starts at beginning of trajectory, and find the first point one radius away from the given point along the trajectory.\n",
    "\n",
    "    Assumes that the first segment passes within a single radius of the point\n",
    "\n",
    "    http://codereview.stackexchange.com/questions/86421/line-segment-to-circle-collision-algorithm\n",
    "    \"\"\"\n",
    "    start_i = int(t)\n",
    "    start_t = t % 1.0\n",
    "    first_t = None\n",
    "    first_i = None\n",
    "    first_p = None\n",
    "    trajectory = np.ascontiguousarray(trajectory)\n",
    "    for i in range(start_i, trajectory.shape[0]-1):\n",
    "        start = trajectory[i,:]\n",
    "        end = trajectory[i+1,:]+1e-6\n",
    "        V = np.ascontiguousarray(end - start)\n",
    "\n",
    "        a = np.dot(V,V)\n",
    "        b = 2.0*np.dot(V, start - point)\n",
    "        c = np.dot(start, start) + np.dot(point,point) - 2.0*np.dot(start, point) - radius*radius\n",
    "        discriminant = b*b-4*a*c\n",
    "\n",
    "        if discriminant < 0:\n",
    "            continue\n",
    "        #   print \"NO INTERSECTION\"\n",
    "        # else:\n",
    "        # if discriminant >= 0.0:\n",
    "        discriminant = np.sqrt(discriminant)\n",
    "        t1 = (-b - discriminant) / (2.0*a)\n",
    "        t2 = (-b + discriminant) / (2.0*a)\n",
    "        if i == start_i:\n",
    "            if t1 >= 0.0 and t1 <= 1.0 and t1 >= start_t:\n",
    "                first_t = t1\n",
    "                first_i = i\n",
    "                first_p = start + t1 * V\n",
    "                break\n",
    "            if t2 >= 0.0 and t2 <= 1.0 and t2 >= start_t:\n",
    "                first_t = t2\n",
    "                first_i = i\n",
    "                first_p = start + t2 * V\n",
    "                break\n",
    "        elif t1 >= 0.0 and t1 <= 1.0:\n",
    "            first_t = t1\n",
    "            first_i = i\n",
    "            first_p = start + t1 * V\n",
    "            break\n",
    "        elif t2 >= 0.0 and t2 <= 1.0:\n",
    "            first_t = t2\n",
    "            first_i = i\n",
    "            first_p = start + t2 * V\n",
    "            break\n",
    "    # wrap around to the beginning of the trajectory if no intersection is found1\n",
    "    if wrap and first_p is None:\n",
    "        for i in range(-1, start_i):\n",
    "            start = trajectory[i % trajectory.shape[0],:]\n",
    "            end = trajectory[(i+1) % trajectory.shape[0],:]+1e-6\n",
    "            V = end - start\n",
    "\n",
    "            a = np.dot(V,V)\n",
    "            b = 2.0*np.dot(V, start - point)\n",
    "            c = np.dot(start, start) + np.dot(point,point) - 2.0*np.dot(start, point) - radius*radius\n",
    "            discriminant = b*b-4*a*c\n",
    "\n",
    "            if discriminant < 0:\n",
    "                continue\n",
    "            discriminant = np.sqrt(discriminant)\n",
    "            t1 = (-b - discriminant) / (2.0*a)\n",
    "            t2 = (-b + discriminant) / (2.0*a)\n",
    "            if t1 >= 0.0 and t1 <= 1.0:\n",
    "                first_t = t1\n",
    "                first_i = i\n",
    "                first_p = start + t1 * V\n",
    "                break\n",
    "            elif t2 >= 0.0 and t2 <= 1.0:\n",
    "                first_t = t2\n",
    "                first_i = i\n",
    "                first_p = start + t2 * V\n",
    "                break\n",
    "\n",
    "    return first_p, first_i, first_t\n",
    "\n",
    "# @njit(fastmath=False, cache=True)\n",
    "def get_actuation(pose_theta, lookahead_point, position, lookahead_distance, wheelbase):\n",
    "    \"\"\"\n",
    "    Returns actuation\n",
    "    \"\"\"\n",
    "    waypoint_y = np.dot(np.array([np.sin(-pose_theta), np.cos(-pose_theta)]), lookahead_point[0:2]-position)\n",
    "    speed = lookahead_point[2]\n",
    "    if np.abs(waypoint_y) < 1e-6:\n",
    "        return speed, 0.\n",
    "    radius = 1/(2.0*waypoint_y/lookahead_distance**2)\n",
    "    steering_angle = np.arctan(wheelbase/radius)\n",
    "    return speed, steering_angle\n",
    "\n",
    "class PurePursuitPlanner:\n",
    "    \"\"\"\n",
    "    Example Planner\n",
    "    \"\"\"\n",
    "    def __init__(self, conf, wb):\n",
    "        self.wheelbase = wb\n",
    "        self.conf = conf\n",
    "        self.load_waypoints(conf)\n",
    "        self.max_reacquire = 20.\n",
    "\n",
    "        self.drawn_waypoints = []\n",
    "\n",
    "    def load_waypoints(self, conf):\n",
    "        \"\"\"\n",
    "        loads waypoints\n",
    "        \"\"\"\n",
    "        self.waypoints = np.loadtxt(conf.wpt_path, delimiter=conf.wpt_delim, skiprows=conf.wpt_rowskip)\n",
    "\n",
    "    def render_waypoints(self, e):\n",
    "        \"\"\"\n",
    "        update waypoints being drawn by EnvRenderer\n",
    "        \"\"\"\n",
    "\n",
    "        #points = self.waypoints\n",
    "\n",
    "        points = np.vstack((self.waypoints[:, self.conf.wpt_xind], self.waypoints[:, self.conf.wpt_yind])).T\n",
    "        \n",
    "        scaled_points = 50.*points\n",
    "\n",
    "        for i in range(points.shape[0]):\n",
    "            if len(self.drawn_waypoints) < points.shape[0]:\n",
    "                b = e.batch.add(1, GL_POINTS, None, ('v3f/stream', [scaled_points[i, 0], scaled_points[i, 1], 0.]),\n",
    "                                ('c3B/stream', [183, 193, 222]))\n",
    "                self.drawn_waypoints.append(b)\n",
    "            else:\n",
    "                self.drawn_waypoints[i].vertices = [scaled_points[i, 0], scaled_points[i, 1], 0.]\n",
    "        \n",
    "    def _get_current_waypoint(self, waypoints, lookahead_distance, position, theta):\n",
    "        \"\"\"\n",
    "        gets the current waypoint to follow\n",
    "        \"\"\"\n",
    "        wpts = np.vstack((self.waypoints[:, self.conf.wpt_xind], self.waypoints[:, self.conf.wpt_yind])).T\n",
    "        nearest_point, nearest_dist, t, i = nearest_point_on_trajectory(position, wpts)\n",
    "        if nearest_dist < lookahead_distance:\n",
    "            lookahead_point, i2, t2 = first_point_on_trajectory_intersecting_circle(position, lookahead_distance, wpts, i+t, wrap=True)\n",
    "            if i2 == None:\n",
    "                return None\n",
    "            current_waypoint = np.empty((3, ))\n",
    "            # x, y\n",
    "            current_waypoint[0:2] = wpts[i2, :]\n",
    "            # speed\n",
    "            current_waypoint[2] = waypoints[i, self.conf.wpt_vind]\n",
    "            return current_waypoint\n",
    "        elif nearest_dist < self.max_reacquire:\n",
    "            return np.append(wpts[i, :], waypoints[i, self.conf.wpt_vind])\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def plan(self, pose_x, pose_y, pose_theta, lookahead_distance, vgain):\n",
    "        \"\"\"\n",
    "        gives actuation given observation\n",
    "        \"\"\"\n",
    "        position = np.array([pose_x, pose_y])\n",
    "        lookahead_point = self._get_current_waypoint(self.waypoints, lookahead_distance, position, pose_theta)\n",
    "\n",
    "        if lookahead_point is None:\n",
    "            return 4.0, 0.0\n",
    "\n",
    "        speed, steering_angle = get_actuation(pose_theta, lookahead_point, position, lookahead_distance, self.wheelbase)\n",
    "        speed = vgain * speed\n",
    "\n",
    "        return speed, steering_angle\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    main entry point\n",
    "    \"\"\"\n",
    "\n",
    "    work = {'mass': 3.463388126201571, 'lf': 0.15597534362552312, 'tlad': 1.82461887897713965, 'vgain': 0.90338203837889}\n",
    "    \n",
    "    with open('./f1tenth_gym/examples/config_example_map.yaml') as file:\n",
    "        conf_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    conf = Namespace(**conf_dict)\n",
    "\n",
    "    planner = PurePursuitPlanner(conf, 0.17145+0.15875)\n",
    "\n",
    "    def render_callback(env_renderer):\n",
    "        # custom extra drawing function\n",
    "\n",
    "        e = env_renderer\n",
    "\n",
    "        # update camera to follow car\n",
    "        x = e.cars[0].vertices[::2]\n",
    "        y = e.cars[0].vertices[1::2]\n",
    "        top, bottom, left, right = max(y), min(y), min(x), max(x)\n",
    "        e.score_label.x = left\n",
    "        e.score_label.y = top - 700\n",
    "        e.left = left - 800\n",
    "        e.right = right + 800\n",
    "        e.top = top + 800\n",
    "        e.bottom = bottom - 800\n",
    "\n",
    "        planner.render_waypoints(env_renderer)\n",
    "\n",
    "    env = gym.make('f110_gym:f110-v0', map=conf.map_path, map_ext=conf.map_ext, num_agents=1)\n",
    "    env.add_render_callback(render_callback)\n",
    "    \n",
    "    obs, step_reward, done, info = env.reset(np.array([[conf.sx, conf.sy, conf.stheta]]))\n",
    "    env.render()\n",
    "\n",
    "    laptime = 0.0\n",
    "    start = time.time()\n",
    "\n",
    "    while not done:\n",
    "        speed, steer = planner.plan(obs['poses_x'][0], obs['poses_y'][0], obs['poses_theta'][0], work['tlad'], work['vgain'])\n",
    "        obs, step_reward, done, info = env.step(np.array([[steer, speed]]))\n",
    "        laptime += step_reward\n",
    "        env.render(mode='human')\n",
    "        \n",
    "    print('Sim elapsed time:', laptime, 'Real elapsed time:', time.time()-start)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148409d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### waypoint handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f37253a0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import yaml\n",
    "import gym\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "\n",
    "# @njit(fastmath=False, cache=True)\n",
    "def nearest_point_on_trajectory(point, trajectory):\n",
    "    \"\"\"\n",
    "    Return the nearest point along the given piecewise linear trajectory.\n",
    "\n",
    "    Same as nearest_point_on_line_segment, but vectorized. This method is quite fast, time constraints should\n",
    "    not be an issue so long as trajectories are not insanely long.\n",
    "\n",
    "        Order of magnitude: trajectory length: 1000 --> 0.0002 second computation (5000fps)\n",
    "\n",
    "    point: size 2 numpy array\n",
    "    trajectory: Nx2 matrix of (x,y) trajectory waypoints\n",
    "        - these must be unique. If they are not unique, a divide by 0 error will destroy the world\n",
    "    \"\"\"\n",
    "    diffs = trajectory[1:,:] - trajectory[:-1,:]\n",
    "    l2s   = diffs[:,0]**2 + diffs[:,1]**2\n",
    "    # this is equivalent to the elementwise dot product\n",
    "    # dots = np.sum((point - trajectory[:-1,:]) * diffs[:,:], axis=1)\n",
    "    dots = np.empty((trajectory.shape[0]-1, ))\n",
    "    for i in range(dots.shape[0]):\n",
    "        dots[i] = np.dot((point - trajectory[i, :]), diffs[i, :])\n",
    "    t = dots / l2s\n",
    "    t[t<0.0] = 0.0\n",
    "    t[t>1.0] = 1.0\n",
    "    # t = np.clip(dots / l2s, 0.0, 1.0)\n",
    "    projections = trajectory[:-1,:] + (t*diffs.T).T\n",
    "    # dists = np.linalg.norm(point - projections, axis=1)\n",
    "    dists = np.empty((projections.shape[0],))\n",
    "    for i in range(dists.shape[0]):\n",
    "        temp = point - projections[i]\n",
    "        dists[i] = np.sqrt(np.sum(temp*temp))\n",
    "    min_dist_segment = np.argmin(dists)\n",
    "    return projections[min_dist_segment], dists[min_dist_segment], t[min_dist_segment], min_dist_segment\n",
    "\n",
    "# @njit(fastmath=False, cache=True)\n",
    "def first_point_on_trajectory_intersecting_circle(point, radius, trajectory, t=0.0, wrap=False):\n",
    "    \"\"\"\n",
    "    starts at beginning of trajectory, and find the first point one radius away from the given point along the trajectory.\n",
    "\n",
    "    Assumes that the first segment passes within a single radius of the point\n",
    "\n",
    "    http://codereview.stackexchange.com/questions/86421/line-segment-to-circle-collision-algorithm\n",
    "    \"\"\"\n",
    "    start_i = int(t)\n",
    "    start_t = t % 1.0\n",
    "    first_t = None\n",
    "    first_i = None\n",
    "    first_p = None\n",
    "    trajectory = np.ascontiguousarray(trajectory)\n",
    "    for i in range(start_i, trajectory.shape[0]-1):\n",
    "        start = trajectory[i,:]\n",
    "        end = trajectory[i+1,:]+1e-6\n",
    "        V = np.ascontiguousarray(end - start)\n",
    "\n",
    "        a = np.dot(V,V)\n",
    "        b = 2.0*np.dot(V, start - point)\n",
    "        c = np.dot(start, start) + np.dot(point,point) - 2.0*np.dot(start, point) - radius*radius\n",
    "        discriminant = b*b-4*a*c\n",
    "\n",
    "        if discriminant < 0:\n",
    "            continue\n",
    "        #   print \"NO INTERSECTION\"\n",
    "        # else:\n",
    "        # if discriminant >= 0.0:\n",
    "        discriminant = np.sqrt(discriminant)\n",
    "        t1 = (-b - discriminant) / (2.0*a)\n",
    "        t2 = (-b + discriminant) / (2.0*a)\n",
    "        if i == start_i:\n",
    "            if t1 >= 0.0 and t1 <= 1.0 and t1 >= start_t:\n",
    "                first_t = t1\n",
    "                first_i = i\n",
    "                first_p = start + t1 * V\n",
    "                break\n",
    "            if t2 >= 0.0 and t2 <= 1.0 and t2 >= start_t:\n",
    "                first_t = t2\n",
    "                first_i = i\n",
    "                first_p = start + t2 * V\n",
    "                break\n",
    "        elif t1 >= 0.0 and t1 <= 1.0:\n",
    "            first_t = t1\n",
    "            first_i = i\n",
    "            first_p = start + t1 * V\n",
    "            break\n",
    "        elif t2 >= 0.0 and t2 <= 1.0:\n",
    "            first_t = t2\n",
    "            first_i = i\n",
    "            first_p = start + t2 * V\n",
    "            break\n",
    "    # wrap around to the beginning of the trajectory if no intersection is found1\n",
    "    if wrap and first_p is None:\n",
    "        for i in range(-1, start_i):\n",
    "            start = trajectory[i % trajectory.shape[0],:]\n",
    "            end = trajectory[(i+1) % trajectory.shape[0],:]+1e-6\n",
    "            V = end - start\n",
    "\n",
    "            a = np.dot(V,V)\n",
    "            b = 2.0*np.dot(V, start - point)\n",
    "            c = np.dot(start, start) + np.dot(point,point) - 2.0*np.dot(start, point) - radius*radius\n",
    "            discriminant = b*b-4*a*c\n",
    "\n",
    "            if discriminant < 0:\n",
    "                continue\n",
    "            discriminant = np.sqrt(discriminant)\n",
    "            t1 = (-b - discriminant) / (2.0*a)\n",
    "            t2 = (-b + discriminant) / (2.0*a)\n",
    "            if t1 >= 0.0 and t1 <= 1.0:\n",
    "                first_t = t1\n",
    "                first_i = i\n",
    "                first_p = start + t1 * V\n",
    "                break\n",
    "            elif t2 >= 0.0 and t2 <= 1.0:\n",
    "                first_t = t2\n",
    "                first_i = i\n",
    "                first_p = start + t2 * V\n",
    "                break\n",
    "\n",
    "    return first_p, first_i, first_t\n",
    "    \n",
    "    \n",
    "CAPTURE_TIME = 100\n",
    "class F110Env(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(2,))\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(217,), dtype=np.float32)\n",
    "\n",
    "        \n",
    "        work = {'mass': 3.463388126201571, 'lf': 0.15597534362552312, 'tlad': 1.82461887897713965, 'vgain': 0.90338203837889}\n",
    "\n",
    "        with open('./f1tenth_gym/examples/config_example_map.yaml') as file:\n",
    "            conf_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        conf = Namespace(**conf_dict)\n",
    "        self.conf = conf\n",
    "        self.load_waypoints(conf)\n",
    "        self.max_reacquire = 20.\n",
    "\n",
    "        def render_callback(env_renderer):\n",
    "            # custom extra drawing function\n",
    "\n",
    "            e = env_renderer\n",
    "\n",
    "            # update camera to follow car\n",
    "            x = e.cars[0].vertices[::2]\n",
    "            y = e.cars[0].vertices[1::2]\n",
    "            top, bottom, left, right = max(y), min(y), min(x), max(x)\n",
    "            e.score_label.x = left\n",
    "            e.score_label.y = top - 700\n",
    "            e.left = left - 800\n",
    "            e.right = right + 800\n",
    "            e.top = top + 800\n",
    "            e.bottom = bottom - 800\n",
    "\n",
    "\n",
    "        self.env = gym.make('f110_gym:f110-v0', map=conf.map_path, map_ext=conf.map_ext, num_agents=1)\n",
    "        self.env.add_render_callback(render_callback)\n",
    "        self.prev_capture_coord = None\n",
    "        self.reset()\n",
    "        \n",
    "    def load_waypoints(self, conf):\n",
    "        \"\"\"\n",
    "        loads waypoints\n",
    "        \"\"\"\n",
    "        self.waypoints = np.loadtxt(conf.wpt_path, delimiter=conf.wpt_delim, skiprows=conf.wpt_rowskip)\n",
    "\n",
    "        \n",
    "    def _get_current_waypoint(self, waypoints, lookahead_distance, position, theta):\n",
    "        \"\"\"\n",
    "        gets the current waypoint to follow\n",
    "        \"\"\"\n",
    "        wpts = np.vstack((self.waypoints[:, self.conf.wpt_xind], self.waypoints[:, self.conf.wpt_yind])).T\n",
    "        nearest_point, nearest_dist, t, i = nearest_point_on_trajectory(position, wpts)\n",
    "        if nearest_dist < lookahead_distance:\n",
    "            lookahead_point, i2, t2 = first_point_on_trajectory_intersecting_circle(position, lookahead_distance, wpts, i+t, wrap=True)\n",
    "            if i2 == None:\n",
    "                return None\n",
    "            current_waypoint = np.empty((3, ))\n",
    "            # x, y\n",
    "            current_waypoint[0:2] = wpts[i2, :]\n",
    "            # speed\n",
    "            current_waypoint[2] = waypoints[i, self.conf.wpt_vind]\n",
    "            return current_waypoint\n",
    "        elif nearest_dist < self.max_reacquire:\n",
    "            return np.append(wpts[i, :], waypoints[i, self.conf.wpt_vind])\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def reset(self):\n",
    "        obs, step_reward, done, info = self.env.reset(np.array([[self.conf.sx, self.conf.sy, self.conf.stheta]]))\n",
    "        self.prev_capture_coord = [obs['poses_x'][0], obs['poses_y'][0]]\n",
    "        self.time_to_capture = CAPTURE_TIME\n",
    "        self.init_x = 0\n",
    "        return self.to_vector_state(obs)\n",
    "    \n",
    "    def to_vector_state(self, obs):\n",
    "\n",
    "        scanner = np.zeros(1080//5,)\n",
    "        for i in range(1080//5):\n",
    "            scanner[i] = np.clip(np.mean(obs['scans'][0][i*5: i*5+5]), 0, 10)\n",
    "\n",
    "        scanner /= 10\n",
    "        state = np.concatenate([\n",
    "            scanner,\n",
    "            np.array(obs['linear_vels_x'][:1])/5,\n",
    "        ])\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "\n",
    "        action[0] = action[0]*np.pi/4\n",
    "        action[1] = action[1]*2.5 + 2.5\n",
    "        action = action.reshape(1, 2)\n",
    "        action = np.repeat(action, repeats=2, axis=0)\n",
    "        action[1][1] = 0\n",
    "        # print(action)\n",
    "        \n",
    "\n",
    "        obs, step_reward, done, info = self.env.step(action)\n",
    "        \n",
    "        pose_x = obs['poses_x'][0]\n",
    "        pose_y = obs['poses_y'][0]\n",
    "        \n",
    "        position = np.array([pose_x, pose_y])\n",
    "        lookahead_point = self._get_current_waypoint(self.waypoints, 1.8, position, 0.9033)\n",
    "        \n",
    "        print(position, lookahead_point)\n",
    "        \n",
    "        reward = 0\n",
    "        if obs['collisions'][0] == 1.0:\n",
    "            reward = -10\n",
    "        \n",
    "\n",
    "        next_state = self.to_vector_state(obs)\n",
    "        reward += (step_reward + obs['linear_vels_x'][0]*0.01)\n",
    "\n",
    "        self.time_to_capture -= 1\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913f8e6d",
   "metadata": {},
   "source": [
    "### checkpoint handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f6f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import yaml\n",
    "import gym\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "    \n",
    "class F110Env(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(2,))\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(217,), dtype=np.float32)\n",
    "\n",
    "\n",
    "        with open('./f1tenth_gym/examples/config_example_map.yaml') as file:\n",
    "            conf_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        conf = Namespace(**conf_dict)\n",
    "        self.conf = conf\n",
    "        wps = np.loadtxt(conf.wpt_path, delimiter=conf.wpt_delim, skiprows=conf.wpt_rowskip)[:, 1:3]\n",
    "        idxs = [i%10 == 0 for i in range(len(wps))]\n",
    "        self.checkpoints = wps[idxs]\n",
    "\n",
    "        def render_callback(env_renderer):\n",
    "            # custom extra drawing function\n",
    "\n",
    "            e = env_renderer\n",
    "\n",
    "            # update camera to follow car\n",
    "            x = e.cars[0].vertices[::2]\n",
    "            y = e.cars[0].vertices[1::2]\n",
    "            top, bottom, left, right = max(y), min(y), min(x), max(x)\n",
    "            e.score_label.x = left\n",
    "            e.score_label.y = top - 700\n",
    "            e.left = left - 800\n",
    "            e.right = right + 800\n",
    "            e.top = top + 800\n",
    "            e.bottom = bottom - 800\n",
    "\n",
    "\n",
    "        self.env = gym.make('f110_gym:f110-v0', map=conf.map_path, map_ext=conf.map_ext, num_agents=1)\n",
    "        self.env.add_render_callback(render_callback)\n",
    "        self.prev_capture_coord = None\n",
    "        self.reset()\n",
    "        \n",
    "  \n",
    "    def reset(self):\n",
    "        obs, step_reward, done, info = self.env.reset(np.array([[self.conf.sx, self.conf.sy, self.conf.stheta]]))\n",
    "        self.next_cp_idx = 0\n",
    "        self.next_cp = self.checkpoints[0]\n",
    "        \n",
    "        return self.to_vector_state(obs)\n",
    "    \n",
    "    def to_vector_state(self, obs):\n",
    "\n",
    "        scanner = np.zeros(1080//5,)\n",
    "        for i in range(1080//5):\n",
    "            scanner[i] = np.clip(np.mean(obs['scans'][0][i*5: i*5+5]), 0, 10)\n",
    "\n",
    "        scanner /= 10\n",
    "        state = np.concatenate([\n",
    "            scanner,\n",
    "            np.array(obs['linear_vels_x'][:1])/5,\n",
    "        ])\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "\n",
    "        action[0] = action[0]*np.pi/4\n",
    "        action[1] = action[1]*2.5 + 2.5\n",
    "        action = action.reshape(1, 2)\n",
    "        action = np.repeat(action, repeats=2, axis=0)\n",
    "        action[1][1] = 0\n",
    "        # print(action)\n",
    "        \n",
    "\n",
    "        obs, step_reward, done, info = self.env.step(action)\n",
    "        \n",
    "        pose_x = obs['poses_x'][0]\n",
    "        pose_y = obs['poses_y'][0]\n",
    "        \n",
    "        position = np.array([pose_x, pose_y])\n",
    "        \n",
    "        \n",
    "        reward = 0\n",
    "        if obs['collisions'][0] == 1.0:\n",
    "            reward = -10\n",
    "        \n",
    "        \n",
    "        next_state = self.to_vector_state(obs)\n",
    "        reward += (step_reward + obs['linear_vels_x'][0]*0.01)\n",
    "\n",
    "        self.time_to_capture -= 1\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bfc78319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(783, 79)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./f1tenth_gym/examples/config_example_map.yaml') as file:\n",
    "    conf_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "conf = Namespace(**conf_dict)\n",
    "wps = np.loadtxt(conf.wpt_path, delimiter=conf.wpt_delim, skiprows=conf.wpt_rowskip)[:, 1:3]\n",
    "\n",
    "idxs = [i%10 == 0 for i in range(len(wps))]\n",
    "len(wps), len(wps[idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ece758ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.0748956,  -0.1528569],\n",
       "       [  0.8466241,   1.8314144],\n",
       "       [  0.3016407,   3.7528228],\n",
       "       [ -0.5603282,   5.5541122],\n",
       "       [ -1.7351692,   7.1682953],\n",
       "       [ -3.1851516,   8.541534 ],\n",
       "       [ -4.8298448,   9.6757256],\n",
       "       [ -6.604086 ,  10.5953911],\n",
       "       [ -8.4606165,  11.3361637],\n",
       "       [-10.3661907,  11.9407187],\n",
       "       [-12.2982359,  12.455185 ],\n",
       "       [-14.2418471,  12.9244482],\n",
       "       [-16.1855925,  13.3931604],\n",
       "       [-18.1180402,  13.9061229],\n",
       "       [-20.0246207,  14.5075524],\n",
       "       [-21.8844765,  15.2400831],\n",
       "       [-23.6675167,  16.142842 ],\n",
       "       [-25.3510718,  17.2202863],\n",
       "       [-26.9525754,  18.4169563],\n",
       "       [-28.505573 ,  19.676279 ],\n",
       "       [-30.0497215,  20.9464969],\n",
       "       [-31.6272337,  22.1747787],\n",
       "       [-33.2778988,  23.3021821],\n",
       "       [-35.0308761,  24.2615582],\n",
       "       [-36.8968526,  24.9742812],\n",
       "       [-38.8576636,  25.3485088],\n",
       "       [-40.8529426,  25.3050478],\n",
       "       [-42.8004334,  24.8655834],\n",
       "       [-44.6319325,  24.0713612],\n",
       "       [-46.2154422,  22.8623476],\n",
       "       [-47.2784185,  21.1861414],\n",
       "       [-47.4665358,  19.2139858],\n",
       "       [-46.938431 ,  17.2914458],\n",
       "       [-46.0959954,  15.4786377],\n",
       "       [-45.2813632,  13.6536393],\n",
       "       [-44.8399044,  11.710772 ],\n",
       "       [-45.1834447,   9.7639284],\n",
       "       [-46.482786 ,   8.2670799],\n",
       "       [-48.1613401,   7.1827276],\n",
       "       [-49.7644997,   5.9955483],\n",
       "       [-50.9656284,   4.4059889],\n",
       "       [-51.7170487,   2.5579025],\n",
       "       [-52.0677701,   0.5924342],\n",
       "       [-52.0815914,  -1.4050918],\n",
       "       [-51.8295912,  -3.3874906],\n",
       "       [-51.3834446,  -5.3359786],\n",
       "       [-50.8107716,  -7.2514294],\n",
       "       [-50.0984696,  -9.1187506],\n",
       "       [-49.1343787, -10.8675746],\n",
       "       [-47.8272187, -12.3737687],\n",
       "       [-46.144309 , -13.4353153],\n",
       "       [-44.1933758, -13.8180171],\n",
       "       [-42.2169598, -13.5615541],\n",
       "       [-40.3587194, -12.8335271],\n",
       "       [-38.6612586, -11.7805658],\n",
       "       [-37.1087679, -10.5218374],\n",
       "       [-35.6516366,  -9.1529192],\n",
       "       [-34.2227965,  -7.7542447],\n",
       "       [-32.7504217,  -6.4018946],\n",
       "       [-31.1698935,  -5.1789265],\n",
       "       [-29.4376056,  -4.1850292],\n",
       "       [-27.5505493,  -3.5349231],\n",
       "       [-25.5727565,  -3.2591824],\n",
       "       [-23.5753739,  -3.2997176],\n",
       "       [-21.5987312,  -3.5939469],\n",
       "       [-19.6601738,  -4.0813055],\n",
       "       [-17.7601249,  -4.7031213],\n",
       "       [-15.8869607,  -5.4023193],\n",
       "       [-14.021819 ,  -6.1227976],\n",
       "       [-12.1435231,  -6.8079792],\n",
       "       [-10.233913 ,  -7.3994845],\n",
       "       [ -8.2835961,  -7.8366788],\n",
       "       [ -6.2963019,  -8.0405142],\n",
       "       [ -4.3058035,  -7.8935891],\n",
       "       [ -2.4108932,  -7.2759753],\n",
       "       [ -0.8097158,  -6.0967245],\n",
       "       [  0.2961629,  -4.4407912],\n",
       "       [  0.9030599,  -2.5409633],\n",
       "       [  1.0823861,  -0.5526523]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wps[idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcb812e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Env simple wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbf8e2c7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "CAPTURE_TIME = 100\n",
    "class F110Env(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(2,))\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(217,), dtype=np.float32)\n",
    "        \n",
    "        self.env = gym.make(\n",
    "            'f110_gym:f110-v0',\n",
    "            map='./f1tenth_gym/examples/example_map',\n",
    "            map_ext='.png'\n",
    "        )\n",
    "        \n",
    "        self.reset()\n",
    "        self.prev_capture_coord = None\n",
    "        \n",
    "    def reset(self):\n",
    "        obs, step_reward, done, info = self.env.reset(\n",
    "            poses=np.array([[0., 0., 0.], \n",
    "                     [-1., -1., 0.]]) \n",
    "        )\n",
    "        self.prev_capture_coord = [obs['poses_x'][0], obs['poses_y'][0]]\n",
    "        self.time_to_capture = CAPTURE_TIME\n",
    "        self.init_x = 0\n",
    "        return self.to_vector_state(obs)\n",
    "    \n",
    "    def to_vector_state(self, obs):\n",
    "\n",
    "        scanner = np.zeros(1080//5,)\n",
    "        for i in range(1080//5):\n",
    "            scanner[i] = np.clip(np.mean(obs['scans'][0][i*5: i*5+5]), 0, 10)\n",
    "\n",
    "        scanner /= 10\n",
    "        state = np.concatenate([\n",
    "            scanner,\n",
    "            np.array(obs['linear_vels_x'][:1])/5,\n",
    "        ])\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "\n",
    "        action[0] = action[0]*np.pi/4\n",
    "        action[1] = action[1]*2.5 + 2.5\n",
    "        action = action.reshape(1, 2)\n",
    "        action = np.repeat(action, repeats=2, axis=0)\n",
    "        action[1][1] = 0\n",
    "        # print(action)\n",
    "        obs, step_reward, done, info = self.env.step(action)\n",
    "        reward = 0\n",
    "        if obs['collisions'][0] == 1.0:\n",
    "            reward = -10\n",
    "        \n",
    "        moving_forward_rew = 0\n",
    "\n",
    "        if self.time_to_capture == 0:\n",
    "            current_coord = [obs['poses_x'][0], obs['poses_y'][0]]\n",
    "            dist = abs(current_coord[0] - self.prev_capture_coord[0]) + abs(current_coord[1] - self.prev_capture_coord[1])\n",
    "            # print(f\"prev coord:{self.prev_capture_coord}, current_coord:{current_coord}, dist:{dist}\")\n",
    "            \n",
    "            self.prev_capture_coord = current_coord\n",
    "            if dist < 2:\n",
    "                # print(\"Neg reward\")\n",
    "                moving_forward_rew = -10\n",
    "\n",
    "            self.time_to_capture = CAPTURE_TIME + 1\n",
    "\n",
    "        next_state = self.to_vector_state(obs)\n",
    "        # ang_vel = obs['ang_vels_z'][0]\n",
    "        # print(obs['ang_vels_z'][0]*0.1)\n",
    "        reward += (step_reward + obs['linear_vels_x'][0]*0.01)\n",
    "\n",
    "        self.time_to_capture -= 1\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306289e2",
   "metadata": {},
   "source": [
    "## Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17999492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.9175751 1.4378904 6.4459574]\n",
      "[4.72336334e-05 2.33010829e-04] [0.9175751 1.4378904 6.4459574]\n",
      "[0.0002834  0.00139806] [0.9175751 1.4378904 6.4459574]\n",
      "[0.0007085  0.00349516] [0.9175751 1.4378904 6.4459574]\n",
      "[0.00132191 0.00652443] [0.9175751 1.4378904 6.4459574]\n",
      "[0.00212169 0.01048626] [0.9175751 1.4378904 6.4459574]\n",
      "[0.00310387 0.01538145] [0.9175751 1.4378904 6.4459574]\n",
      "[0.0042618  0.02121132] [0.9175751 1.4378904 6.4459574]\n",
      "[0.00512406 0.02800671] [0.9175751 1.4378904 6.4459574]\n",
      "[0.00612723 0.0356541 ] [0.9175751 1.4378904 6.4459574]\n",
      "[0.00690455 0.04415355] [0.9175751 1.4378904 6.4459574]\n",
      "[0.00769209 0.05343803] [0.9175751 1.4378904 6.4459574]\n",
      "[0.00830871 0.06348264] [0.8836811 1.634946  6.4459574]\n",
      "[0.0087803  0.07424606] [0.8836811 1.634946  6.4459574]\n",
      "[0.00904746 0.08569315] [0.8836811 1.634946  6.4459574]\n",
      "[0.00907341 0.09778762] [0.8836811 1.634946  6.4459574]\n",
      "[0.00881482 0.11049314] [0.8836811 1.634946  6.4459574]\n",
      "[0.00822775 0.1237728 ] [0.8836811 1.634946  6.4459574]\n",
      "[0.00733035 0.13759301] [0.8836811 1.634946  6.4459574]\n",
      "[0.00613375 0.15192267] [0.8836811 1.634946  6.4459574]\n",
      "[0.0046372  0.16673178] [0.8836811 1.634946  6.4459574]\n",
      "[0.00283274 0.18199098] [0.8836811 1.634946  6.4603765]\n",
      "[0.00070822 0.19767136] [0.8836811 1.634946  6.4603765]\n",
      "[-0.0017509  0.2137443] [0.8836811 1.634946  6.4603765]\n",
      "[-0.00456039  0.2301815 ] [0.8836811 1.634946  6.4603765]\n",
      "[-0.00773652  0.24695483] [0.8836811 1.634946  6.4603765]\n",
      "[-0.01129564  0.26403638] [0.8466241 1.8314144 6.4603765]\n",
      "[-0.01525379  0.28139839] [0.8466241 1.8314144 6.4603765]\n",
      "[-0.01962655  0.29901323] [0.8466241 1.8314144 6.4603765]\n",
      "[-0.02442887  0.3168534 ] [0.8466241 1.8314144 6.4603765]\n",
      "[-0.02967496  0.33489149] [0.8466241 1.8314144 6.4603765]\n",
      "[-0.03537823  0.35310019] [0.8466241 1.8314144 6.4603765]\n",
      "[-0.04155119  0.37145225] [0.8466241 1.8314144 6.4743457]\n",
      "[-0.04820546  0.38992052] [0.8466241 1.8314144 6.4743457]\n",
      "[-0.05535169  0.40847792] [0.8466241 1.8314144 6.4743457]\n",
      "[-0.06299956  0.42709743] [0.8466241 1.8314144 6.4743457]\n",
      "[-0.07115775  0.44575215] [0.8466241 1.8314144 6.4743457]\n",
      "[-0.07983391  0.46441523] [0.8063948 2.0272831 6.4743457]\n",
      "[-0.08903467  0.48305994] [0.8063948 2.0272831 6.4743457]\n",
      "[-0.0987656   0.50165968] [0.8063948 2.0272831 6.4743457]\n",
      "[-0.10903122  0.52018794] [0.8063948 2.0272831 6.4743457]\n",
      "[-0.11983501  0.53861838] [0.8063948 2.0272831 6.4875975]\n",
      "[-0.13117938  0.55692485] [0.8063948 2.0272831 6.4875975]\n",
      "[-0.14306567  0.57508134] [0.8063948 2.0272831 6.4875975]\n",
      "[-0.15549417  0.59306208] [0.8063948 2.0272831 6.4875975]\n",
      "[-0.16846412  0.61084152] [0.8063948 2.0272831 6.4875975]\n",
      "[-0.18197368  0.62839439] [0.8063948 2.0272831 6.4875975]\n",
      "[-0.19602001  0.64569567] [0.8063948 2.0272831 6.4875975]\n",
      "[-0.2105992   0.66272068] [0.8063948 2.0272831 6.4875975]\n",
      "[-0.22570634  0.67944506] [0.8063948 2.0272831 6.4875975]\n",
      "[-0.24133548  0.69584483] [0.8063948 2.0272831 6.4875975]\n",
      "[-0.2574797   0.71189638] [0.8063948 2.0272831 6.4998704]\n",
      "[-0.2741311   0.72757654] [0.8063948 2.0272831 6.4998704]\n",
      "[-0.29128079  0.74286259] [0.8063948 2.0272831 6.4998704]\n",
      "[-0.30891895  0.75773227] [0.8063948 2.0272831 6.4998704]\n",
      "[-0.32703484  0.77216383] [0.8063948 2.0272831 6.4998704]\n",
      "[-0.3456168   0.78613605] [0.8063948 2.0272831 6.4998704]\n",
      "[-0.36465231  0.79962829] [0.8063948 2.0272831 6.4998704]\n",
      "[-0.38412798  0.81262045] [0.8063948 2.0272831 6.4998704]\n",
      "[-0.40402959  0.82509308] [0.8063948 2.0272831 6.4998704]\n",
      "[-0.42434211  0.83702734] [0.8063948 2.0272831 6.4998704]\n",
      "[-0.44504973  0.84840506] [0.8063948 2.0272831 6.4998704]\n",
      "[-0.46613591  0.85920875] [0.8063948 2.0272831 6.5111965]\n",
      "[-0.48758337  0.8694216 ] [0.8063948 2.0272831 6.5111965]\n",
      "[-0.50937414  0.87902757] [0.8063948 2.0272831 6.5111965]\n",
      "[-0.53148959  0.88801132] [0.8063948 2.0272831 6.5111965]\n",
      "[-0.55391047  0.8963583 ] [0.8063948 2.0272831 6.5111965]\n",
      "[-0.57661693  0.90405473] [0.8063948 2.0272831 6.5111965]\n",
      "[-0.59958854  0.91108763] [0.8063948 2.0272831 6.5111965]\n",
      "[-0.62280437  0.91744484] [0.8466241 1.8314144 6.5111965]\n",
      "[-0.64624295  0.92311503] [0.8466241 1.8314144 6.5111965]\n",
      "[-0.66988238  0.92808771] [0.8466241 1.8314144 6.5111965]\n",
      "[-0.69370033  0.93235322] [0.8466241 1.8314144 6.5111965]\n",
      "[-0.71767404  0.93590281] [0.8836811 1.634946  6.5111965]\n",
      "[-0.74178043  0.93872858] [0.8836811 1.634946  6.5111965]\n",
      "[-0.76599607  0.94082351] [0.8836811 1.634946  6.5111965]\n",
      "[-0.79029724  0.94218147] [0.9175751 1.4378904 6.5111965]\n",
      "[-0.81465996  0.94279724] [0.948304  1.2403209 6.5111965]\n",
      "[-0.83906005  0.9426665 ] [0.9758694 1.0422891 6.5111965]\n",
      "[-0.86347312  0.94178582] [0.9758694 1.0422891 6.5111965]\n",
      "[-0.88787464  0.94015269] [0.9758694 1.0422891 6.5111965]\n",
      "[-0.91223994  0.93776552] [0.9758694 1.0422891 6.5111965]\n",
      "[-0.9365443  0.9346236] [0.9758694 1.0422891 6.5111965]\n",
      "[-0.96076291  0.93072716] [0.9758694 1.0422891 6.5111965]\n",
      "[-0.98487099  0.92607731] [0.9758694 1.0422891 6.5111965]\n",
      "[-1.00884374  0.92067607] [0.9758694 1.0422891 6.5111965]\n",
      "[-1.03265643  0.91452638] [0.9758694 1.0422891 6.5111965]\n",
      "[-1.0562844   0.90763203] [0.9758694 1.0422891 6.5111965]\n",
      "[-1.07970311  0.89999775] [0.9758694 1.0422891 6.5111965]\n",
      "[-1.10288818  0.8916291 ] [0.9758694 1.0422891 6.5111965]\n",
      "[-1.12581539  0.88253255] [0.9758694 1.0422891 6.5111965]\n",
      "[-1.14846074  0.87271539] [0.9758694 1.0422891 6.5111965]\n",
      "[-1.17080047  0.86218581] [0.9758694 1.0422891 6.5111965]\n",
      "[-1.19281108  0.8509528 ] [0.9758694 1.0422891 6.5111965]\n",
      "[-1.21446939  0.83902618] [0.9758694 1.0422891 6.5111965]\n",
      "[-1.23575253  0.82641661] [0.9758694 1.0422891 6.5111965]\n",
      "[-1.25663799  0.81313554] [0.9758694 1.0422891 6.5111965]\n",
      "[-1.27710364  0.79919518] [0.9758694 1.0422891 6.5111965]\n",
      "[-1.29712779  0.78460853] [0.9758694 1.0422891 6.5111965]\n",
      "[-1.31668914  0.76938936] [0.9758694 1.0422891 6.5111965]\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "env = F110Env({'explore':False})\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = np.array([1.0, 0.0])\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311e9da4",
   "metadata": {},
   "source": [
    "# RAY algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c311619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.agents import ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe30888f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 14:37:31,315\tWARNING trainer.py:2279 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-02-17 14:37:31,317\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-02-17 14:37:31,317\tINFO trainer.py:790 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=70737)\u001b[0m 2022-02-17 14:37:36,439\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "2022-02-17 14:37:36,631\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-02-17 14:37:36,631\tWARNING trainer.py:2279 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-02-17 14:37:36,678\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=70746)\u001b[0m 2022-02-17 14:37:40,829\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0073087267538017\n",
      "-0.1823945720170311\n",
      "1.6037537150713765\n",
      "4.909399370721818\n",
      "8.70403750859794\n",
      "8.70403750859794\n",
      "13.216170279304219\n",
      "17.478133594774885\n",
      "20.52533360195168\n",
      "23.597259909522897\n",
      "23.597259909522897\n",
      "29.66429361702801\n",
      "29.66429361702801\n",
      "35.10973669965799\n",
      "36.255164324282894\n",
      "37.50851107936982\n",
      "38.27671078435274\n",
      "39.6887965069401\n",
      "41.53247733462491\n",
      "43.67414608966989\n",
      "47.38808584961096\n",
      "47.248051949373114\n",
      "49.68825811794391\n",
      "50.188523610608264\n",
      "50.188523610608264\n",
      "52.70060246332673\n",
      "55.171815254050344\n",
      "55.171815254050344\n",
      "58.873031821832775\n",
      "58.873031821832775\n",
      "58.873031821832775\n",
      "65.64481921081504\n",
      "64.25066056442819\n",
      "65.68746129677642\n",
      "65.88271741899324\n",
      "67.13139536260067\n",
      "66.38003709948019\n",
      "66.38003709948019\n",
      "66.38003709948019\n",
      "74.86855697043801\n",
      "73.42489785781564\n",
      "73.9378841033698\n",
      "75.50548737404736\n",
      "75.50548737404736\n",
      "75.50548737404736\n",
      "75.50548737404736\n",
      "75.50548737404736\n",
      "85.88937186724016\n",
      "85.87170155860409\n",
      "86.28238757260947\n",
      "88.84621620681379\n",
      "88.4008919300649\n",
      "89.14561423644403\n",
      "89.0983891155621\n",
      "90.24951227019051\n",
      "92.11496271491632\n",
      "92.11496271491632\n",
      "95.59365254908413\n",
      "96.2715964250137\n",
      "96.2715964250137\n",
      "96.2715964250137\n",
      "100.64431540632148\n",
      "101.45771039543992\n",
      "104.13326361972311\n",
      "106.12502024484489\n",
      "106.55039736777\n",
      "109.71131008865132\n",
      "111.58164740234956\n",
      "112.3544781161613\n",
      "115.4080859756637\n",
      "117.45502004296596\n",
      "117.45502004296596\n",
      "117.45502004296596\n",
      "122.51001272674084\n",
      "122.51001272674084\n",
      "122.51001272674084\n",
      "128.92664322852943\n",
      "131.8336571308297\n",
      "131.8336571308297\n",
      "135.09356706056442\n",
      "136.68872356858478\n",
      "139.82452164950922\n",
      "141.5298997120354\n",
      "142.604535249926\n",
      "145.41989537978745\n",
      "146.64010227410253\n",
      "147.15101545491711\n",
      "147.15101545491711\n",
      "151.03296407378343\n",
      "152.0011300760985\n",
      "151.3226728207087\n",
      "151.33025586044693\n",
      "148.72444329642593\n",
      "148.72444329642593\n",
      "150.1167601591853\n",
      "151.73983688370774\n",
      "151.36851566384613\n",
      "151.36851566384613\n",
      "154.2974621109847\n",
      "156.1386600504163\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "config = {\n",
    "    \"num_workers\": 1,\n",
    "    \"framework\": \"torch\",\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [512, 512, 128],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "trainer = ppo.PPOTrainer(env=F110Env, config=config)\n",
    "\n",
    "for _ in range(100):\n",
    "    result = trainer.train()\n",
    "    print(result['episode_reward_mean'])\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c7ea0126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-09 16:38:59,610\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./rllib_checkpoint/checkpoint_000038/checkpoint-38'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save(\"./rllib_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64aa25d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 15:56:46,197\tWARNING trainer.py:2279 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=70740)\u001b[0m 2022-02-17 15:56:50,270\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "2022-02-17 15:56:50,308\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-02-17 15:56:50,308\tWARNING trainer.py:2279 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-02-17 15:56:50,345\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-02-17 15:56:50,393\tINFO trainable.py:472 -- Restored on 128.205.218.23 from checkpoint: ./rllib_checkpoint/checkpoint_000090/checkpoint-90\n",
      "2022-02-17 15:56:50,393\tINFO trainable.py:480 -- Current state after restoring: {'_iteration': 90, '_timesteps_total': 360000, '_time_total': 1223.2796750068665, '_episodes_total': 197}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=70741)\u001b[0m 2022-02-17 15:56:54,434\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "ltrainer = ppo.PPOTrainer(env=F110Env, config=config)\n",
    "ltrainer.restore('./rllib_checkpoint/checkpoint_000090/checkpoint-90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e6cdb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "env = F110Env({'explore':False})\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = ltrainer.compute_action(obs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e6688d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad5077f6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66f44b61",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to : cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "if(torch.cuda.is_available()): \n",
    "    device = torch.device('cuda:0') \n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    print(\"Device set to : cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eddb773f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "\n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_dim = action_dim\n",
    "            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
    "\n",
    "        # actor\n",
    "        if has_continuous_action_space :\n",
    "            self.actor = nn.Sequential(\n",
    "                            nn.Linear(state_dim, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, action_dim),\n",
    "                            nn.Tanh()\n",
    "                        )\n",
    "        else:\n",
    "            self.actor = nn.Sequential(\n",
    "                            nn.Linear(state_dim, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, action_dim),\n",
    "                            nn.Softmax(dim=-1)\n",
    "                        )\n",
    "\n",
    "        \n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "                        nn.Linear(state_dim, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 1)\n",
    "                    )\n",
    "        \n",
    "    def set_action_std(self, new_action_std):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling ActorCritic::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        \n",
    "        return action.detach(), action_logprob.detach()\n",
    "    \n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "            action_var = self.action_var.expand_as(action_mean)\n",
    "            cov_mat = torch.diag_embed(action_var).to(device)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "            \n",
    "            # for single action continuous environments\n",
    "            if self.action_dim == 1:\n",
    "                action = action.reshape(-1, self.action_dim)\n",
    "\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_values = self.critic(state)\n",
    "        \n",
    "        return action_logprobs, state_values, dist_entropy\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_std = action_std_init\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.buffer = RolloutBuffer()\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "                    ])\n",
    "\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "        \n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = new_action_std\n",
    "            self.policy.set_action_std(new_action_std)\n",
    "            self.policy_old.set_action_std(new_action_std)\n",
    "        \n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling PPO::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = self.action_std - action_std_decay_rate\n",
    "            self.action_std = round(self.action_std, 4)\n",
    "            if (self.action_std <= min_action_std):\n",
    "                self.action_std = min_action_std\n",
    "                print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
    "            else:\n",
    "                print(\"setting actor output action_std to : \", self.action_std)\n",
    "            self.set_action_std(self.action_std)\n",
    "\n",
    "        else:\n",
    "            print(\"WARNING : Calling PPO::decay_action_std() on discrete action space policy\")\n",
    "\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device)\n",
    "                action, action_logprob = self.policy_old.act(state)\n",
    "\n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "\n",
    "            return action.detach().cpu().numpy().flatten()\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device)\n",
    "                action, action_logprob = self.policy_old.act(state)\n",
    "            \n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "\n",
    "            return action.item()\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "\n",
    "        # Monte Carlo estimate of returns\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "            \n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
    "\n",
    "        \n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # match state_values tensor dimensions with rewards tensor\n",
    "            state_values = torch.squeeze(state_values)\n",
    "            \n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss\n",
    "            advantages = rewards - state_values.detach()   \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "\n",
    "            # final loss of clipped objective PPO\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
    "            \n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # clear buffer\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    \n",
    "    def save(self, checkpoint_path):\n",
    "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
    "   \n",
    "\n",
    "    def load(self, checkpoint_path):\n",
    "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        \n",
    "        \n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f93ddd8",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current logging run number for F110 :  0\n",
      "logging at : PPO_logs/F110//PPO_F110_log_0.csv\n",
      "save checkpoint path : PPO_preTrained/F110/PPO_F110_0_0.pth\n",
      "--------------------------------------------------------------------------------------------\n",
      "max training timesteps :  100000\n",
      "max timesteps per episode :  400\n",
      "model saving frequency : 20000 timesteps\n",
      "log frequency : 800 timesteps\n",
      "printing average reward over episodes in last : 1600 timesteps\n",
      "--------------------------------------------------------------------------------------------\n",
      "state space dimension :  1082\n",
      "action space dimension :  2\n",
      "--------------------------------------------------------------------------------------------\n",
      "Initializing a continuous action space policy\n",
      "--------------------------------------------------------------------------------------------\n",
      "starting std of action distribution :  None\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'action_std_decay_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--------------------------------------------------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstarting std of action distribution : \u001b[39m\u001b[38;5;124m\"\u001b[39m, action_std)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecay rate of std of action distribution : \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43maction_std_decay_rate\u001b[49m)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimum std of action distribution : \u001b[39m\u001b[38;5;124m\"\u001b[39m, min_action_std)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecay frequency of std of action distribution : \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(action_std_decay_freq) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m timesteps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'action_std_decay_rate' is not defined"
     ]
    }
   ],
   "source": [
    "has_continuous_action_space = True\n",
    "env_name = \"F110\"\n",
    "\n",
    "max_ep_len = 400                    # max timesteps in one episode\n",
    "max_training_timesteps = int(1e5)   # break training loop if timeteps > max_training_timesteps\n",
    "\n",
    "print_freq = max_ep_len * 4     # print avg reward in the interval (in num timesteps)\n",
    "log_freq = max_ep_len * 2       # log avg reward in the interval (in num timesteps)\n",
    "save_model_freq = int(2e4)      # save model frequency (in num timesteps)\n",
    "\n",
    "action_std = None\n",
    "\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "## Note : print/log frequencies should be > than max_ep_len\n",
    "\n",
    "\n",
    "################ PPO hyperparameters ################\n",
    "\n",
    "\n",
    "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
    "K_epochs = 40               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003       # learning rate for actor network\n",
    "lr_critic = 0.001       # learning rate for critic network\n",
    "\n",
    "random_seed = 0         # set random seed if required (0 = no random seed)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "\n",
    "env = F110Env({})\n",
    "\n",
    "# state space dimension\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "# action space dimension\n",
    "if has_continuous_action_space:\n",
    "    action_dim = env.action_space.shape[0]\n",
    "else:\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "\n",
    "\n",
    "###################### logging ######################\n",
    "\n",
    "#### log files for multiple runs are NOT overwritten\n",
    "\n",
    "log_dir = \"PPO_logs\"\n",
    "if not os.path.exists(log_dir):\n",
    "      os.makedirs(log_dir)\n",
    "\n",
    "log_dir = log_dir + '/' + env_name + '/'\n",
    "if not os.path.exists(log_dir):\n",
    "      os.makedirs(log_dir)\n",
    "\n",
    "\n",
    "#### get number of log files in log directory\n",
    "run_num = 0\n",
    "current_num_files = next(os.walk(log_dir))[2]\n",
    "run_num = len(current_num_files)\n",
    "\n",
    "\n",
    "#### create new log file for each run \n",
    "log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
    "\n",
    "print(\"current logging run number for \" + env_name + \" : \", run_num)\n",
    "print(\"logging at : \" + log_f_name)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "################### checkpointing ###################\n",
    "\n",
    "run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
    "\n",
    "directory = \"PPO_preTrained\"\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "directory = directory + '/' + env_name + '/'\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "\n",
    "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
    "print(\"save checkpoint path : \" + checkpoint_path)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "############# print all hyperparameters #############\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"max training timesteps : \", max_training_timesteps)\n",
    "print(\"max timesteps per episode : \", max_ep_len)\n",
    "\n",
    "print(\"model saving frequency : \" + str(save_model_freq) + \" timesteps\")\n",
    "print(\"log frequency : \" + str(log_freq) + \" timesteps\")\n",
    "print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"state space dimension : \", state_dim)\n",
    "print(\"action space dimension : \", action_dim)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "if has_continuous_action_space:\n",
    "    print(\"Initializing a continuous action space policy\")\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"starting std of action distribution : \", action_std)\n",
    "    print(\"decay rate of std of action distribution : \", action_std_decay_rate)\n",
    "    print(\"minimum std of action distribution : \", min_action_std)\n",
    "    print(\"decay frequency of std of action distribution : \" + str(action_std_decay_freq) + \" timesteps\")\n",
    "\n",
    "else:\n",
    "    print(\"Initializing a discrete action space policy\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\") \n",
    "print(\"PPO K epochs : \", K_epochs)\n",
    "print(\"PPO epsilon clip : \", eps_clip)\n",
    "print(\"discount factor (gamma) : \", gamma)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"optimizer learning rate actor : \", lr_actor)\n",
    "print(\"optimizer learning rate critic : \", lr_critic)\n",
    "\n",
    "if random_seed:\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"setting random seed to \", random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "################# training procedure ################\n",
    "\n",
    "# initialize a PPO agent\n",
    "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "\n",
    "\n",
    "# track total training time\n",
    "start_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "# logging file\n",
    "log_f = open(log_f_name,\"w+\")\n",
    "log_f.write('episode,timestep,reward\\n')\n",
    "\n",
    "\n",
    "# printing and logging variables\n",
    "print_running_reward = 0\n",
    "print_running_episodes = 0\n",
    "\n",
    "log_running_reward = 0\n",
    "log_running_episodes = 0\n",
    "\n",
    "time_step = 0\n",
    "i_episode = 0\n",
    "\n",
    "\n",
    "# training loop\n",
    "while time_step <= max_training_timesteps:\n",
    "    \n",
    "    state = env.reset()\n",
    "    current_ep_reward = 0\n",
    "\n",
    "    for t in range(1, max_ep_len+1):\n",
    "        \n",
    "        # select action with policy\n",
    "        action = ppo_agent.select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # saving reward and is_terminals\n",
    "        ppo_agent.buffer.rewards.append(reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "        \n",
    "        time_step +=1\n",
    "        current_ep_reward += reward\n",
    "\n",
    "        # update PPO agent\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo_agent.update()\n",
    "\n",
    "        # if continuous action space; then decay action std of ouput action distribution\n",
    "        if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
    "            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "\n",
    "        # log in logging file\n",
    "        if time_step % log_freq == 0:\n",
    "\n",
    "            # log average reward till last episode\n",
    "            log_avg_reward = log_running_reward / log_running_episodes\n",
    "            log_avg_reward = round(log_avg_reward, 4)\n",
    "\n",
    "            log_f.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n",
    "            log_f.flush()\n",
    "\n",
    "            log_running_reward = 0\n",
    "            log_running_episodes = 0\n",
    "\n",
    "        # printing average reward\n",
    "        if time_step % print_freq == 0:\n",
    "\n",
    "            # print average reward till last episode\n",
    "            print_avg_reward = print_running_reward / print_running_episodes\n",
    "            print_avg_reward = round(print_avg_reward, 2)\n",
    "\n",
    "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
    "\n",
    "            print_running_reward = 0\n",
    "            print_running_episodes = 0\n",
    "            \n",
    "        # save model weights\n",
    "        if time_step % save_model_freq == 0:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"saving model at : \" + checkpoint_path)\n",
    "            ppo_agent.save(checkpoint_path)\n",
    "            print(\"model saved\")\n",
    "            print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            \n",
    "        # break; if the episode is over\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print_running_reward += current_ep_reward\n",
    "    print_running_episodes += 1\n",
    "\n",
    "    log_running_reward += current_ep_reward\n",
    "    log_running_episodes += 1\n",
    "\n",
    "    i_episode += 1\n",
    "\n",
    "\n",
    "log_f.close()\n",
    "env.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print total training time\n",
    "print(\"============================================================================================\")\n",
    "end_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "print(\"Finished training at (GMT) : \", end_time)\n",
    "print(\"Total training time  : \", end_time - start_time)\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d6b6d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
