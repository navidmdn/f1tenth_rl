{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54efaf43",
   "metadata": {},
   "source": [
    "# waypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9b9651f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.43225402646071\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('ss1-wp.np', 'rb') as f:\n",
    "    wps = np.load(f)\n",
    "    \n",
    "dist = np.linalg.norm(wps[0][:2] - wps[100][:2])\n",
    "print(dist)\n",
    "\n",
    "print(len(wps)//100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fb0cb8",
   "metadata": {},
   "source": [
    "# environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d388b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33133683, 0.33715568, 0.2789363 , 0.26730084, 0.27783558,\n",
       "       0.25131348, 0.18717651, 0.26272024, 0.20131201, 0.21702657,\n",
       "       0.22933016, 0.27686221, 0.20493082, 0.19033509, 0.21765053,\n",
       "       0.25655249, 0.26454268, 0.18620252, 0.22913007, 0.19603247,\n",
       "       0.27394254, 0.22005166, 0.24040377, 0.23238348, 0.17177792,\n",
       "       0.25728935, 0.21784019, 0.26235549, 0.25749418, 0.26047834,\n",
       "       0.2614054 , 0.27636353, 0.25049943, 0.21066345, 0.30938045,\n",
       "       0.3193564 , 0.31234092, 0.34076173, 0.32729207, 0.31842827,\n",
       "       0.46620134, 0.39337403, 0.43864189, 0.48565297, 0.53889876,\n",
       "       0.63525426, 0.70439087, 0.78542394, 1.02013366, 1.03592106,\n",
       "       1.0172587 , 1.01931304, 1.05088912, 0.9953354 , 1.00799235,\n",
       "       1.0053505 , 0.97386807, 1.01849488, 0.95585187, 0.82723608,\n",
       "       0.68907613, 0.63591835, 0.59702855, 0.56926326, 0.55046791,\n",
       "       0.43649147, 0.43212818, 0.41601932, 0.38496124, 0.37786633,\n",
       "       0.32285136, 0.34055656, 0.24469266, 0.30046297, 0.24067697,\n",
       "       0.25514711, 0.28392961, 0.27907553, 0.27734548, 0.21388443,\n",
       "       0.22234872, 0.24366084, 0.25912263, 0.23596161, 0.26282156,\n",
       "       0.24309529, 0.23585269, 0.23314502, 0.20973152, 0.23818064,\n",
       "       0.2077851 , 0.22507259, 0.23938981, 0.27476128, 0.25611348,\n",
       "       0.21727078, 0.22544419, 0.25134886, 0.25643892, 0.25689591,\n",
       "       0.26233871, 0.31050736, 0.28393903, 0.3081531 , 0.32386783,\n",
       "       0.28134303, 0.28877749, 0.31707471, 0.02673412, 0.02541579])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import yaml\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class F110RaceEnv(gym.Env):\n",
    "    def __init__(self, env_config,\n",
    "                 deterministic=False,\n",
    "                 test_map_name=None,\n",
    "                 scan_range=10.0,\n",
    "                 max_v=12.0,\n",
    "                 n_cps = 100,\n",
    "                 cp_reward = 0.1,\n",
    "                 min_cp_dist=2.0,\n",
    "                 train_map_name='SILVERSTONE_TRAIN'\n",
    "                ):\n",
    "        \n",
    "        self.deterministic = deterministic\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(110,), dtype=np.float32)\n",
    "        self.max_v = max_v\n",
    "        self.test_map_name = test_map_name\n",
    "        self.min_cp_dist = min_cp_dist\n",
    "        self.n_cps = n_cps\n",
    "        self.cp_reward = cp_reward\n",
    "        self.scan_range = scan_range\n",
    "        \n",
    "        self.train_map_name = train_map_name\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "    \n",
    "    def _render_callback(self, env_renderer):\n",
    "        # custom extra drawing function\n",
    "\n",
    "        e = env_renderer\n",
    "\n",
    "        # update camera to follow car\n",
    "        x = e.cars[0].vertices[::2]\n",
    "        y = e.cars[0].vertices[1::2]\n",
    "        top, bottom, left, right = max(y), min(y), min(x), max(x)\n",
    "        e.score_label.x = left\n",
    "        e.score_label.y = top - 700\n",
    "        e.left = left - 800\n",
    "        e.right = right + 800\n",
    "        e.top = top + 800\n",
    "        e.bottom = bottom - 800\n",
    "  \n",
    "    def load_train_setup(self):\n",
    "        no = np.random.choice([1, 2])\n",
    "        map_path = f\"./f1tenth_gym/examples/{self.train_map_name}{no}\"\n",
    "        with open(f'./ss{no}-wp.np', 'rb') as f:\n",
    "            wps = np.load(f)\n",
    "            \n",
    "        return map_path, wps\n",
    "    \n",
    "    def load_test_setup(self):\n",
    "        map_path = f\"./f1tenth_gym/examples/{self.test_map_name}\"\n",
    "        with open('./ss-wp.np', 'rb') as f:\n",
    "            wps = np.load(f)\n",
    "            \n",
    "        return map_path, wps\n",
    "\n",
    "    def build_checkpoints(self, wps, cp_dist):\n",
    "        \n",
    "        start = np.random.randint(0, len(wps)-1)\n",
    "        checkpoints = []\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(wps):\n",
    "            if i % cp_dist == 0:\n",
    "                checkpoints.append(wps[(start+i)%len(wps)])\n",
    "            i += 1\n",
    "        return checkpoints\n",
    "    \n",
    "    def reset(self):\n",
    "        if self.test_map_name is not None:\n",
    "            map_path, wps = self.load_test_setup()\n",
    "        else:\n",
    "            map_path, wps = self.load_train_setup()\n",
    "        \n",
    "#         print(map_path, len(wps))\n",
    "        \n",
    "        self.env = gym.make('f110_gym:f110-v0', map=map_path, map_ext='.png', num_agents=1)\n",
    "        self.env.add_render_callback(self._render_callback)\n",
    "        \n",
    "        cp_dist = len(wps)//self.n_cps\n",
    "        \n",
    "        self.checkpoints = self.build_checkpoints(wps, cp_dist)\n",
    "#         print(f\"number of checkpoints: {len(self.checkpoints)}\")\n",
    "\n",
    "        random_idx = np.random.randint(0, len(self.checkpoints)-1)\n",
    "        start_point = self.checkpoints[random_idx]\n",
    "    \n",
    "        obs, step_reward, done, info = self.env.reset(\n",
    "            np.array([\n",
    "                start_point\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        self.next_cp_idx = random_idx + 1\n",
    "        self.t = 0\n",
    "\n",
    "        return self.to_vector_state(obs)\n",
    "    \n",
    "    def to_vector_state(self, obs):\n",
    "        \n",
    "        scanner = np.clip(obs['scans'][0], 0, self.scan_range)\n",
    "        \n",
    "        buck = 10\n",
    "        size = 1080//buck\n",
    "        agg_scanner = np.zeros(size,)\n",
    "        for i in range(size):\n",
    "            agg_scanner[i] = np.min(scanner[i*buck: i*buck+buck])\n",
    "        \n",
    "        agg_scanner /= self.scan_range\n",
    "        state = np.concatenate([\n",
    "            agg_scanner,\n",
    "            np.array(obs['linear_vels_x'][:1])/self.max_v,\n",
    "            np.array(obs['ang_vels_z'][:1])/3.0,\n",
    "        ])\n",
    "        \n",
    "        noise = np.random.normal(loc=0.0, scale=0.03, size=state.shape)\n",
    "        state = state + noise\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def checkpoint(self, position):\n",
    "        dist = np.linalg.norm(position - self.checkpoints[self.next_cp_idx][:2])\n",
    "        reward = 0\n",
    "        if dist < self.min_cp_dist:\n",
    "            reward = self.cp_reward\n",
    "    \n",
    "            self.next_cp_idx = (self.next_cp_idx + 1)%len(self.checkpoints)\n",
    "        return reward\n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "\n",
    "        act_v = action[0]*(self.max_v/2)+(self.max_v/2)\n",
    "        th = action[1]*np.pi/6\n",
    "        act = np.array([[th, act_v]])\n",
    "\n",
    "        obs, step_reward, done, info = self.env.step(act)\n",
    "        pose_x = obs['poses_x'][0]\n",
    "        pose_y = obs['poses_y'][0]\n",
    "\n",
    "\n",
    "        position = np.array([pose_x, pose_y])\n",
    "\n",
    "        if obs['collisions'][0] == 1.0:\n",
    "            reward = -1\n",
    "            done = True\n",
    "            if self.test_map_name is not None:\n",
    "                print('CRASHED')\n",
    "            \n",
    "        cp_reward = self.checkpoint(position)\n",
    "        next_state = self.to_vector_state(obs)\n",
    "        reward += cp_reward\n",
    "        self.t += 1\n",
    "        \n",
    "        if done and self.test_map_name is not None:\n",
    "            print(obs['lap_times'])\n",
    "        \n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "e = F110RaceEnv({})\n",
    "e.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7aefd2",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a002a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1504b8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 17:46:57,977\tWARNING ppo.py:223 -- `train_batch_size` (100000) cannot be achieved with your other settings (num_workers=16 num_envs_per_worker=16 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 390.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=48715)\u001b[0m 2022-04-06 17:47:05,987\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=48738)\u001b[0m 2022-04-06 17:47:05,940\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=48701)\u001b[0m 2022-04-06 17:47:05,971\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=48729)\u001b[0m 2022-04-06 17:47:05,959\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=48714)\u001b[0m 2022-04-06 17:47:05,940\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=48737)\u001b[0m 2022-04-06 17:47:05,984\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=48719)\u001b[0m 2022-04-06 17:47:05,974\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=48736)\u001b[0m 2022-04-06 17:47:05,940\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=48696)\u001b[0m 2022-04-06 17:47:05,940\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=48722)\u001b[0m 2022-04-06 17:47:05,950\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=48693)\u001b[0m 2022-04-06 17:47:05,950\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=48692)\u001b[0m 2022-04-06 17:47:05,954\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=48691)\u001b[0m 2022-04-06 17:47:05,941\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=48702)\u001b[0m 2022-04-06 17:47:06,001\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=48735)\u001b[0m 2022-04-06 17:47:06,005\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=48694)\u001b[0m 2022-04-06 17:47:06,004\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "2022-04-06 17:47:11,067\tINFO trainable.py:125 -- Trainable.setup took 13.091 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-04-06 17:47:11,069\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_workers': 16, 'num_envs_per_worker': 16, 'create_env_on_driver': False, 'rollout_fragment_length': 390, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 0.0003, 'train_batch_size': 100000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'F110RaceEnv', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 16, 'num_envs_per_worker': 16, 'create_env_on_driver': False, 'rollout_fragment_length': 390, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 0.0003, 'train_batch_size': 100000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'F110RaceEnv', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1.0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.agents.ppo.ppo_torch_policy.PPOTorchPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 1.0, 'sgd_minibatch_size': 4096, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1.0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.agents.ppo.ppo_torch_policy.PPOTorchPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 1.0, 'sgd_minibatch_size': 4096, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 17:48:02,290\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward:-0.910286320254507\n",
      "checkpoint saved at ./checkpoints/v1.0/checkpoint_000001/checkpoint-1\n",
      "episode: 1 reward:-0.8899699097291878\n",
      "episode: 2 reward:-0.8301075268817206\n",
      "episode: 3 reward:-0.7534579439252337\n",
      "episode: 4 reward:-0.6063953488372092\n",
      "episode: 5 reward:-0.4241635687732342\n",
      "checkpoint saved at ./checkpoints/v1.0/checkpoint_000006/checkpoint-6\n",
      "episode: 6 reward:-0.29508928571428567\n",
      "episode: 7 reward:-0.1158730158730158\n",
      "episode: 8 reward:0.20441176470588257\n",
      "episode: 9 reward:0.6104838709677423\n",
      "episode: 10 reward:0.4284671532846717\n",
      "checkpoint saved at ./checkpoints/v1.0/checkpoint_000011/checkpoint-11\n",
      "episode: 11 reward:0.5104761904761906\n",
      "episode: 12 reward:0.6412280701754386\n",
      "episode: 13 reward:0.9219999999999999\n",
      "episode: 14 reward:0.981\n",
      "episode: 15 reward:1.296\n",
      "checkpoint saved at ./checkpoints/v1.0/checkpoint_000016/checkpoint-16\n",
      "episode: 16 reward:1.5739999999999998\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "# with half cheeta configs\n",
    "\n",
    "configs = {\n",
    "    'framework': 'torch',\n",
    "    'num_workers': 16,\n",
    "    'num_gpus': 1.0,\n",
    "    'kl_coeff': 1.0,\n",
    "    'clip_param': 0.2,\n",
    "    'num_envs_per_worker': 16,\n",
    "    'train_batch_size': 100000,\n",
    "    'sgd_minibatch_size': 4096,\n",
    "    'batch_mode': 'truncate_episodes',\n",
    "    'lr': .0003,\n",
    "}\n",
    "\n",
    "\n",
    "trainer = ppo.PPOTrainer(env=F110RaceEnv, config=configs)\n",
    "# print(\"FINE TUNING\")\n",
    "# trainer.restore('./checkpoints/race_vfh/checkpoint_000031/checkpoint-31')\n",
    "\n",
    "print(trainer.config)\n",
    "rewards = []\n",
    "best_reward = -100\n",
    "\n",
    "import pickle\n",
    "\n",
    "for i in range(300):\n",
    "    result = trainer.train()\n",
    "    episode_r = result['episode_reward_mean']\n",
    "    print(f\"episode: {i} reward:{episode_r}\")\n",
    "    rewards.append(episode_r)\n",
    "    with open('./checkpoints/race_v1.0_r', 'wb') as f:\n",
    "        pickle.dump(rewards, f)\n",
    "        \n",
    "    if (i%5 == 0 and episode_r > best_reward) or (i > 50 and episode_r > best_reward):\n",
    "        best_reward = episode_r\n",
    "        cp = trainer.save(\"./checkpoints/v1.0\")\n",
    "        print(\"checkpoint saved at\", cp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b36db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
